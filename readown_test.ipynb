{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import auxiliary as au\n",
    "import regression as reg\n",
    "import datetime as dt\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import io\n",
    "from scipy.interpolate import interp2d\n",
    "import os\n",
    "import sys\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "from scipy.interpolate import griddata\n",
    "import h5py\n",
    "\n",
    "\n",
    "def demread(file, lattar, lontar):\n",
    "    datatemp = io.loadmat(file)\n",
    "    demori = datatemp['DEM']\n",
    "    demori[np.isnan(demori)] = 0\n",
    "    info = datatemp['Info'][0][0]\n",
    "    latori = np.arange(info['yll'] + info['Ysize'] * info['nrows'] - info['Ysize'] / 2, info['yll'], -info['Ysize'])\n",
    "    lonori = np.arange(info['xll'] + info['Xsize'] / 2, info['xll'] + info['Xsize'] * info['ncols'], info['Xsize'])\n",
    "    f = interp2d(lonori, latori, demori, kind='linear')\n",
    "    demtar = f(lontar.flatten(), lattar.flatten())\n",
    "    demtar = np.flipud(demtar)\n",
    "    return demtar\n",
    "\n",
    "\n",
    "def neargrid(rowtar, coltar, rowori, colori, hwsize):\n",
    "    # inputs are 1D matrices\n",
    "    # tar is target area\n",
    "    # ori is original area\n",
    "    # hwsize is half window size (e.g., 4 means the space window width/length is 2*4+1)\n",
    "    # find a space window centering the target grid in the original area and calculate the weights\n",
    "    nrows = len(rowtar)\n",
    "    ncols = len(coltar)\n",
    "    rowse = np.zeros([nrows, ncols, 2]).astype(int)  # se: start/end\n",
    "    colse = np.zeros([nrows, ncols, 2]).astype(int)  # se: start/end\n",
    "    weight = np.nan * np.zeros([nrows, ncols, (hwsize * 2 + 1) ** 2])  # from left to right/from top to bottom weight\n",
    "\n",
    "    for rr in range(nrows):\n",
    "        rowloc = np.argmin(np.abs(rowori - rowtar[rr]))\n",
    "        rowse[rr, :, 0] = rowloc - hwsize\n",
    "        rowse[rr, :, 1] = rowloc + hwsize\n",
    "\n",
    "    for cc in range(ncols):\n",
    "        colloc = np.argmin(np.abs(colori - coltar[cc]))\n",
    "        colse[:, cc, 0] = colloc - hwsize\n",
    "        colse[:, cc, 1] = colloc + hwsize\n",
    "\n",
    "    rowse[rowse < 0] = 0\n",
    "    rowse[rowse > nrows] = nrows\n",
    "    colse[colse < 0] = 0\n",
    "    colse[colse > ncols] = nrows\n",
    "\n",
    "    maxdist = (hwsize + 0.5) * np.sqrt(2) + 0.5\n",
    "    for rr in range(nrows):\n",
    "        rowloc = np.argmin(np.abs(rowori - rowtar[rr]))\n",
    "        for cc in range(ncols):\n",
    "            colloc = np.argmin(np.abs(colori - coltar[cc]))\n",
    "\n",
    "            rowse_rc = rowse[rr, cc, :]\n",
    "            colse_rc = colse[rr, cc, :]\n",
    "            flag = 0\n",
    "            for i in range(rowse_rc[0], rowse_rc[1] + 1):\n",
    "                for j in range(colse_rc[0], colse_rc[1] + 1):\n",
    "                    dist = ((rowloc - i) ** 2 + (colloc - j) ** 2) ** 0.5\n",
    "                    weight[rr, cc, flag] = au.distanceweight(dist, maxdist, 3)\n",
    "                    flag = flag + 1\n",
    "\n",
    "            weight[rr, cc, :] = weight[rr, cc, :] / np.nansum(weight[rr, cc, :])\n",
    "\n",
    "    return rowse, colse, weight\n",
    "\n",
    "\n",
    "def readownscale(dataori, latori, lonori, demori, lattar, lontar, demtar, rowse, colse, weight, mask):\n",
    "    nrows = len(lattar)\n",
    "    ncols = len(lontar)\n",
    "    ntimes = np.shape(dataori)[2]\n",
    "    lonori, latori = np.meshgrid(lonori, latori)\n",
    "    datatar = np.nan * np.zeros([nrows, ncols, ntimes])\n",
    "\n",
    "    for rr in range(nrows):\n",
    "        for cc in range(ncols):\n",
    "            if mask[rr, cc] == 1:\n",
    "                rloc = rowse[rr, cc, :]\n",
    "                cloc = colse[rr, cc, :]\n",
    "                latnear = latori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "                lonnear = lonori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "                demnear = demori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "                nnum = np.size(latnear)\n",
    "                latnear = np.reshape(latnear, nnum)\n",
    "                lonnear = np.reshape(lonnear, nnum)\n",
    "                demnear = np.reshape(demnear, nnum)\n",
    "                weightnear = np.zeros([nnum, nnum])\n",
    "                for i in range(nnum):\n",
    "                    weightnear[i, i] = weight[rr, cc, i]\n",
    "\n",
    "                nearinfo = np.zeros([nnum, 4])\n",
    "                nearinfo[:, 0] = 1\n",
    "                nearinfo[:, 1] = latnear\n",
    "                nearinfo[:, 2] = lonnear\n",
    "                nearinfo[:, 3] = demnear\n",
    "\n",
    "                tarinfo = np.zeros(4)\n",
    "                tarinfo[0] = 1\n",
    "                tarinfo[1] = lattar[rr]\n",
    "                tarinfo[2] = lontar[cc]\n",
    "                tarinfo[3] = demtar[rr, cc]\n",
    "\n",
    "                tx_red = np.transpose(nearinfo)\n",
    "                twx_red = np.matmul(tx_red, weightnear)\n",
    "\n",
    "                for tt in range(ntimes):\n",
    "                    datanear = dataori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1, tt]\n",
    "                    datanear = np.reshape(datanear, nnum)\n",
    "\n",
    "                    # upper and lower boundary for the downscaled data\n",
    "                    # this is a conservative limitation\n",
    "                    lowbound = np.min(datanear)\n",
    "                    upbound = np.max(datanear)\n",
    "\n",
    "                    b = reg.least_squares(nearinfo, datanear, twx_red)\n",
    "                    datatemp = np.dot(tarinfo, b)\n",
    "                    if np.all(b == 0) or datatemp > upbound or datatemp < lowbound or np.isnan(datatemp):\n",
    "                        # use nearest neighbor interpolation\n",
    "                        weightnear = weight[rr, cc, 0:nnum]\n",
    "                        mloc = np.argmax(weightnear)\n",
    "                        datatar[rr, cc, tt] = datanear[mloc]\n",
    "                    else:\n",
    "                        datatar[rr, cc, tt] = datatemp\n",
    "    return datatar\n",
    "\n",
    "\n",
    "def readownscale_tostn(dataori, latori, lonori, demori, lattar, lontar, demtar, rowse, colse, weight, stn_row, stn_col,\n",
    "                       data0, method, stn_lle):\n",
    "    nstn = len(stn_row)\n",
    "    ntimes = np.shape(dataori)[2]\n",
    "    datatar = np.nan * np.zeros([nstn, ntimes])\n",
    "\n",
    "    if method == 'linear' or method == 'nearest':\n",
    "        xynew = stn_lle[:,[0, 1]]\n",
    "        for i in range(ntimes):\n",
    "            print('Time step:',i,ntimes)\n",
    "            rg = RegularGridInterpolator((latori, lonori), dataori[:, :, i], method=method)\n",
    "            datatar[:, i] = rg(xynew)\n",
    "\n",
    "    elif method == 'GWR':\n",
    "        lonori, latori = np.meshgrid(lonori, latori)\n",
    "        for gg in range(nstn):\n",
    "            if np.mod(gg, 5000) == 0:\n",
    "                print('station', gg, nstn)\n",
    "\n",
    "            if np.isnan(data0[gg]):\n",
    "                continue  # station does not have observations, thus does not need downscaling\n",
    "\n",
    "            rr = stn_row[gg]\n",
    "            cc = stn_col[gg]\n",
    "            rloc = rowse[rr, cc, :]\n",
    "            cloc = colse[rr, cc, :]\n",
    "            latnear = latori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "            lonnear = lonori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "            demnear = demori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "            nnum = np.size(latnear)\n",
    "            latnear = np.reshape(latnear, nnum)\n",
    "            lonnear = np.reshape(lonnear, nnum)\n",
    "            demnear = np.reshape(demnear, nnum)\n",
    "            weightnear = np.zeros([nnum, nnum])\n",
    "            for i in range(nnum):\n",
    "                weightnear[i, i] = weight[rr, cc, i]\n",
    "\n",
    "            nearinfo = np.zeros([nnum, 4])\n",
    "            nearinfo[:, 0] = 1\n",
    "            nearinfo[:, 1] = latnear\n",
    "            nearinfo[:, 2] = lonnear\n",
    "            nearinfo[:, 3] = demnear\n",
    "\n",
    "            tarinfo = np.zeros(4)\n",
    "            tarinfo[0] = 1\n",
    "            tarinfo[1] = stn_lle[gg, 0]\n",
    "            tarinfo[2] = stn_lle[gg, 1]\n",
    "            tarinfo[3] = stn_lle[gg, 2]\n",
    "\n",
    "            tx_red = np.transpose(nearinfo)\n",
    "            twx_red = np.matmul(tx_red, weightnear)\n",
    "\n",
    "            for tt in range(ntimes):\n",
    "                datanear = dataori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1, tt]\n",
    "                datanear = np.reshape(datanear, nnum)\n",
    "\n",
    "                # upper and lower boundary for the downscaled data\n",
    "                # this is a conservative limitation\n",
    "                lowbound = np.min(datanear)\n",
    "                upbound = np.max(datanear)\n",
    "\n",
    "                b = reg.least_squares(nearinfo, datanear, twx_red)\n",
    "                datatemp = np.dot(tarinfo, b)\n",
    "                if np.all(b == 0) or datatemp > upbound or datatemp < lowbound or np.isnan(datatemp):\n",
    "                    # use nearest neighbor interpolation\n",
    "                    weightnear = weight[rr, cc, 0:nnum]\n",
    "                    mloc = np.argmax(weightnear)\n",
    "                    datatar[gg, tt] = datanear[mloc]\n",
    "                else:\n",
    "                    datatar[gg, tt] = datatemp\n",
    "    else:\n",
    "        sys.exit('Unknown downscaling method')\n",
    "    return datatar\n",
    "\n",
    "\n",
    "def readstndata(inpath, stnID, ndays):\n",
    "    nstn = len(stnID)\n",
    "    prcp_stn = np.nan * np.zeros([nstn, ndays])\n",
    "    tmin_stn = np.nan * np.zeros([nstn, ndays])\n",
    "    tmax_stn = np.nan * np.zeros([nstn, ndays])\n",
    "\n",
    "    for i in range(nstn):\n",
    "        if np.mod(i, 1000) == 0:\n",
    "            print('station', i, nstn)\n",
    "        file = inpath + '/' + stnID[i] + '.nc'\n",
    "        fid = nc.Dataset(file)\n",
    "        varlist = fid.variables.keys()\n",
    "        if 'prcp' in varlist:\n",
    "            prcp_stn[i, :] = fid['prcp'][:].data\n",
    "        if 'tmin' in varlist:\n",
    "            tmin_stn[i, :] = fid['tmin'][:].data\n",
    "        if 'tmax' in varlist:\n",
    "            tmax_stn[i, :] = fid['tmax'][:].data\n",
    "        fid.close()\n",
    "\n",
    "    tmean_stn = (tmin_stn + tmax_stn) / 2\n",
    "    trange_stn = np.abs(tmax_stn - tmin_stn)\n",
    "\n",
    "    return prcp_stn, tmean_stn, trange_stn\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# time periods: inside or outside\n",
    "# outside\n",
    "# a = int(sys.argv[1])\n",
    "# b = int(sys.argv[2])\n",
    "# year = [a, b]\n",
    "# inside\n",
    "# year = [1979, 2018]\n",
    "# print('start/end year', year)\n",
    "########################################################################################################################\n",
    "\n",
    "# basic information: be set before running\n",
    "# mac\n",
    "filedem = './DEM/NA_DEM_010deg_trim.mat'\n",
    "# plato\n",
    "# filedem = '/datastore/GLOBALWATER/CommonData/EMDNA/DEM/NA_DEM_010deg_trim.mat'\n",
    "vars = ['prcp', 'tmin', 'tmax']\n",
    "lontar = np.arange(-180 + 0.05, -50, 0.1)\n",
    "lattar = np.arange(85 - 0.05, 5, -0.1)\n",
    "hwsize = 2  # use (2*2+1)**2 grids to perform regression\n",
    "downtostn_method = 'nearest' # method choices: \"GWR\", \"nearest\", \"linear\"\n",
    "\n",
    "\n",
    "# station information\n",
    "# mac\n",
    "gmet_stnfile = '/Users/localuser/GMET/pyGMET_NA/stnlist_whole.txt'\n",
    "gmet_stnpath = '/Users/localuser/GMET/StnInput_daily'\n",
    "gmet_stndatafile = '/Users/localuser/GMET/pyGMET_NA/stndata_whole.npz' # to be saved. only process when absent\n",
    "# plato\n",
    "# gmet_stnfile = '/home/gut428/GMET/eCAI_EMDNA/StnGridInfo/stnlist_whole.txt'\n",
    "# gmet_stnpath = '/home/gut428/GMET/StnInput_daily'\n",
    "# gmet_stndatafile = '/home/gut428/stndata_whole.npz'  # to be saved. only process when absent\n",
    "\n",
    "# reanalysis path: ERA-5\n",
    "# mac\n",
    "filedem_era = './DEM/ERA5_DEM2.mat'\n",
    "inpath = '/Users/localuser/Research/Test'\n",
    "outpath = '/Users/localuser/Research'\n",
    "# plato\n",
    "# filedem_era = '/datastore/GLOBALWATER/CommonData/EMDNA/DEM/ERA5_DEM2.mat'\n",
    "# inpath = '/datastore/GLOBALWATER/CommonData/EMDNA/ERA5_day_raw'  # downscale to 0.1 degree\n",
    "# outpath = '/home/gut428'\n",
    "file_readownstn = outpath + '/ERA5_downto_stn_' + downtostn_method + '.npz'  # downscale to station points (1979-2018)\n",
    "# filenear = '/datastore/GLOBALWATER/CommonData/EMDNA/ERA5_day_ds/weight_dem.npz'\n",
    "filenear = '/Users/localuser/Research/weight_dem.npz'\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# read some basic infomation\n",
    "datatemp = io.loadmat(filedem)\n",
    "demtar = datatemp['DEM']  # this is consistent with lontar lattar\n",
    "mask = demtar.copy()\n",
    "mask[~np.isnan(mask)] = 1\n",
    "\n",
    "stn_ID = np.genfromtxt(gmet_stnfile, dtype='str', skip_header=1, comments='#', delimiter=',', usecols=(0), unpack=False)\n",
    "stn_lle = np.loadtxt(gmet_stnfile, dtype=float, skiprows=1, comments='#', delimiter=',', usecols=(1, 2, 3),\n",
    "                     unpack=False)\n",
    "stn_row = ((85 - stn_lle[:, 0]) / 0.1).astype(int)\n",
    "stn_col = ((stn_lle[:, 1] + 180) / 0.1).astype(int)\n",
    "nstn = len(stn_ID)\n",
    "ndays = 14610  # days from 1979 to 2018\n",
    "\n",
    "# read all station data and save to facilitate analysis in the future\n",
    "if not os.path.isfile(gmet_stndatafile):\n",
    "    prcp_stn, tmean_stn, trange_stn = readstndata(gmet_stnpath, stn_ID, ndays)\n",
    "    prcp_stn = np.float32(prcp_stn)\n",
    "    tmean_stn = np.float32(tmean_stn)\n",
    "    trange_stn = np.float32(trange_stn)\n",
    "    np.savez_compressed(gmet_stndatafile, prcp_stn=prcp_stn, tmean_stn=tmean_stn, trange_stn=trange_stn,\n",
    "                        stn_ID=stn_ID, stn_lle=stn_lle, stn_row=stn_row, stn_col=stn_col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downscale to station points\n",
    "# original station data\n",
    "datatemp = np.load(gmet_stndatafile)\n",
    "prcp_stn0 = datatemp['prcp_stn'][:, 0]\n",
    "tmean_stn0 = datatemp['tmean_stn'][:, 0]\n",
    "del datatemp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downscale to station: year 1979\n",
      "Time step: 0 365\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The points in dimension 0 must be strictly ascending",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b79f3c922a60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         prcptar = readownscale_tostn(dataori, latori, lonori, demori, lattar, lontar, demtar, rowse, colse, weight,\n\u001b[0;32m---> 30\u001b[0;31m                                      stn_row, stn_col, prcp_stn0, downtostn_method, stn_lle)\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mprcptar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprcptar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-b118a4fd2473>\u001b[0m in \u001b[0;36mreadownscale_tostn\u001b[0;34m(dataori, latori, lonori, demori, lattar, lontar, demtar, rowse, colse, weight, stn_row, stn_col, data0, method, stn_lle)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mntimes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time step:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mntimes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegularGridInterpolator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatori\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlonori\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataori\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0mdatatar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxynew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/scipy/interpolate/interpolate.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, points, values, method, bounds_error, fill_value)\u001b[0m\n\u001b[1;32m   2442\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2443\u001b[0m                 raise ValueError(\"The points in dimension %d must be strictly \"\n\u001b[0;32m-> 2444\u001b[0;31m                                  \"ascending\" % i)\n\u001b[0m\u001b[1;32m   2445\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2446\u001b[0m                 raise ValueError(\"The points in dimension %d must be \"\n",
      "\u001b[0;31mValueError\u001b[0m: The points in dimension 0 must be strictly ascending"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(file_readownstn):\n",
    "    # ndays should minus 365 for MERRA2\n",
    "    prcp_readown = np.float32(np.nan * np.zeros([nstn, ndays]))\n",
    "    tmean_readown = np.float32(np.nan * np.zeros([nstn, ndays]))\n",
    "    trange_readown = np.float32(np.nan * np.zeros([nstn, ndays]))\n",
    "\n",
    "    # load nearby grid information\n",
    "    datatemp = io.loadmat(filenear)\n",
    "    rowse = datatemp['rowse']\n",
    "    colse = datatemp['colse']\n",
    "    weight = datatemp['weight']\n",
    "    demori = datatemp['demori']\n",
    "\n",
    "    flag = 0\n",
    "    for y in range(1979, 2019):\n",
    "        print('Downscale to station: year', y)\n",
    "        # prcp downscaling\n",
    "        infile = inpath + '/ERA5_prcp_' + str(y) + '.mat'\n",
    "        datatemp = {}\n",
    "        f = h5py.File(infile, 'r')\n",
    "        for k, v in f.items():\n",
    "            datatemp[k] = np.array(v)\n",
    "        latori = datatemp['latitude'][0]\n",
    "        lonori = datatemp['longitude'][0]\n",
    "        dataori = datatemp['data']\n",
    "        dataori = np.transpose(dataori, [2, 1, 0])\n",
    "        del datatemp\n",
    "        f.close()\n",
    "        prcptar = readownscale_tostn(dataori, latori, lonori, demori, lattar, lontar, demtar, rowse, colse, weight,\n",
    "                                     stn_row, stn_col, prcp_stn0, downtostn_method, stn_lle)\n",
    "        prcptar = np.float32(prcptar)\n",
    "\n",
    "        # tmin downscaling\n",
    "        infile = inpath + '/ERA5_tmin_' + str(y) + '.mat'\n",
    "        datatemp = {}\n",
    "        f = h5py.File(infile, 'r')\n",
    "        for k, v in f.items():\n",
    "            datatemp[k] = np.array(v)\n",
    "        latori = datatemp['latitude'][0]\n",
    "        lonori = datatemp['longitude'][0]\n",
    "        dataori = datatemp['data']\n",
    "        dataori = np.transpose(dataori, [2, 1, 0])\n",
    "        del datatemp\n",
    "        f.close()\n",
    "        tmintar = readownscale_tostn(dataori, latori, lonori, demori, lattar, lontar, demtar, rowse, colse, weight,\n",
    "                                     stn_row, stn_col, tmean_stn0, downtostn_method, stn_lle)\n",
    "        tmintar = np.float32(tmintar)\n",
    "\n",
    "        # tmax downscaling\n",
    "        infile = inpath + '/ERA5_tmax_' + str(y) + '.mat'\n",
    "        datatemp = {}\n",
    "        f = h5py.File(infile, 'r')\n",
    "        for k, v in f.items():\n",
    "            datatemp[k] = np.array(v)\n",
    "        latori = datatemp['latitude'][0]\n",
    "        lonori = datatemp['longitude'][0]\n",
    "        dataori = datatemp['data']\n",
    "        dataori = np.transpose(dataori, [2, 1, 0])\n",
    "        del datatemp\n",
    "        f.close()\n",
    "        tmaxtar = readownscale_tostn(dataori, latori, lonori, demori, lattar, lontar, demtar, rowse, colse, weight,\n",
    "                                     stn_row, stn_col, tmean_stn0, downtostn_method, stn_lle)\n",
    "        tmaxtar = np.float32(tmaxtar)\n",
    "\n",
    "        # merge\n",
    "        dayy = np.shape(prcptar)[1]\n",
    "        prcp_readown[:, flag:flag + dayy] = prcptar\n",
    "        tmean_readown[:, flag:flag + dayy] = (tmintar + tmaxtar) / 2\n",
    "        trange_readown[:, flag:flag + dayy] = np.abs(tmaxtar - tmintar)\n",
    "        flag = flag + dayy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009803259745240211"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def threshold_for_occurrence(dref, dtar):\n",
    "    # dref is station prcp and >0 means positive precipitation\n",
    "    # find the threshold for dtar so that dref and dtar have the same number of precipitation events\n",
    "    indnan = (np.isnan(dref)) | (np.isnan(dtar))\n",
    "    if np.sum(indnan)>0:\n",
    "        dref = dref[~indnan]\n",
    "        dtar = dtar[~indnan]\n",
    "\n",
    "    num1 = np.sum(dref > 0)\n",
    "    if num1 == 0:\n",
    "        threshold = 0\n",
    "    else:\n",
    "        indnan = (dtar == 0) | (np.isnan(dtar))\n",
    "        dtar = dtar[~indnan]\n",
    "        if len(dtar)<=num1:\n",
    "            threshold = 0\n",
    "        else:\n",
    "            dtars = np.flip(np.sort(dtar))\n",
    "            threshold = (dtars[num1] + dtars[num1-1]) / 2\n",
    "    return threshold\n",
    "\n",
    "threshold_for_occurrence(dataori[100,100,:], dataori[100,300,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 365)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.tile(dataori[100,100,:],(10,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
