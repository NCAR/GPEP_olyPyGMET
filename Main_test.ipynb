{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read study area basic information\n",
      "Read study area basic information\n",
      "Read station precipitation and temperature data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:134: RuntimeWarning: invalid value encountered in greater\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:135: RuntimeWarning: invalid value encountered in greater\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:138: RuntimeWarning: invalid value encountered in greater\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:138: RuntimeWarning: invalid value encountered in less\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:169: RuntimeWarning: invalid value encountered in greater\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:170: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate correlation (auto_cc and t_p_cc)\n",
      "Tmean lag-1 autocorrelation:  0.7750418057075311\n",
      "Trange-prcp correlation:  -0.23047390605643742\n",
      "calculate station weights for each grid cell\n",
      "calculate station weights for each station\n",
      "Estimate regression error at station points\n",
      "Locally weighted regression of precipitation and temperature\n",
      "Regression time step:  1 ---Total time steps:  31\n",
      "Regression time step:  2 ---Total time steps:  31\n",
      "Regression time step:  3 ---Total time steps:  31\n",
      "Regression time step:  4 ---Total time steps:  31\n",
      "Regression time step:  5 ---Total time steps:  31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:503: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression time step:  6 ---Total time steps:  31\n",
      "Regression time step:  7 ---Total time steps:  31\n",
      "Regression time step:  8 ---Total time steps:  31\n",
      "Regression time step:  9 ---Total time steps:  31\n",
      "Regression time step:  10 ---Total time steps:  31\n",
      "Regression time step:  11 ---Total time steps:  31\n",
      "Regression time step:  12 ---Total time steps:  31\n",
      "Regression time step:  13 ---Total time steps:  31\n",
      "Regression time step:  14 ---Total time steps:  31\n",
      "Regression time step:  15 ---Total time steps:  31\n",
      "Regression time step:  16 ---Total time steps:  31\n",
      "Regression time step:  17 ---Total time steps:  31\n",
      "Regression time step:  18 ---Total time steps:  31\n",
      "Regression time step:  19 ---Total time steps:  31\n",
      "Regression time step:  20 ---Total time steps:  31\n",
      "Regression time step:  21 ---Total time steps:  31\n",
      "Regression time step:  22 ---Total time steps:  31\n",
      "Regression time step:  23 ---Total time steps:  31\n",
      "Regression time step:  24 ---Total time steps:  31\n",
      "Regression time step:  25 ---Total time steps:  31\n",
      "Regression time step:  26 ---Total time steps:  31\n",
      "Regression time step:  27 ---Total time steps:  31\n",
      "Regression time step:  28 ---Total time steps:  31\n",
      "Regression time step:  29 ---Total time steps:  31\n",
      "Regression time step:  30 ---Total time steps:  31\n",
      "Regression time step:  31 ---Total time steps:  31\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'auxiliary' has no attribute 'metric_stn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-ea2fe85d4078>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0mest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mau\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobst\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpcp_err_stn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0mkge_stn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mau\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkge2012\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m     \u001b[0mmetric_stn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mau\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric_stn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprcp_stn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'auxiliary' has no attribute 'metric_stn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import auxiliary as au\n",
    "import regression as reg\n",
    "import datetime as dt\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import io\n",
    "import os\n",
    "import sys\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "########################################################################################################################\n",
    "# 0. read/define configuration information\n",
    "# setting: file and path names of inputs\n",
    "# FileStnInfo = 'stnlist_slope.w_subset.txt'  # station basic information (lists)\n",
    "# FileGridInfo = 'gridinfo.0625.w_subset.nc'  # study area information\n",
    "# PathStn = 'stndata'  # original station data (prcp ...)\n",
    "FileStnInfo = '/Users/localuser/GMET/Example_tgq/inputs/stnlist_example.txt'  # station basic information (lists)\n",
    "FileGridInfo = '/Users/localuser/GMET/Example_tgq/inputs/gridinfo_example.nc'  # study area information\n",
    "# PathStn = '/Users/localuser/GMET/Example_tgq/StnDaily_train'  # original station data (prcp ...)\n",
    "PathStn = '/Users/localuser/GMET/StnInput_daily'\n",
    "\n",
    "# setting: start and end date\n",
    "# calculation start/end date:\n",
    "date_cal_start = 20180101  # yyyymmdd: start date\n",
    "date_cal_end = 20180131  # yyyymmdd: end date\n",
    "# station data (in PathStn) start/end date:\n",
    "date_stn_start = 19790101  # yyyymmdd: start date\n",
    "date_stn_end = 20181231  # yyyymmdd: end date\n",
    "\n",
    "# setting: paramters for lag correlation of tmean_stn, and cross-correlation between prcp and trange_stn\n",
    "windows = 31  # parameters for auto-cc t-p-cc calculation\n",
    "lag = 1\n",
    "\n",
    "# setting: searching nearby stations\n",
    "nearstn_min = 10  # nearby stations: minimum number\n",
    "nearstn_max = 30  # nearby stations: maximum number\n",
    "search_radius = 1000  # km. only search stations within this radius even nearstn_max cannot be reached\n",
    "max_dist = 100  # max_distance in distance-based weight calculation\n",
    "overwrite_weight = 1  # 1: overwrite FileWeight; other values, do not overwrite FileWeight\n",
    "FileWeight = '/Users/localuser/GMET/pyGMET_res/weight_nearstn.npz'\n",
    "\n",
    "# setting: parameters for transforming temp to approximate normal distribution\n",
    "trans_exp = 4\n",
    "trans_mode = 'box-cox'  # box-cox or power-law or none\n",
    "\n",
    "# setting: regression outputs\n",
    "overwrite_regress = 1  # 1: overwrite  regression output files; other values, do not overwrite\n",
    "FileRegError = '/Users/localuser/GMET/pyGMET_res/regress_error.npz'  # regression error at station points\n",
    "FileRegression = '/Users/localuser/GMET/pyGMET_res/regress_daily.npz'\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# check file status\n",
    "# this part should be activated in operational application\n",
    "# if os.path.isfile(FileRegression) and overwrite_regress != 1:\n",
    "#     print('Condition-1:', FileRegression, 'exists')\n",
    "#     print('Condition-2: overwrite_regress != 1')\n",
    "#     sys.exit('Output files have been generated. Exit the program')\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# 1. basic information\n",
    "print('Read study area basic information')\n",
    "\n",
    "# station location and attribute information\n",
    "# stninfo: [ stations, 1/lat/lon/elev/slope_ns/slope_we ]\n",
    "stnID, stninfo = au.readstnlist(FileStnInfo)\n",
    "nstn = len(stnID)\n",
    "\n",
    "# time information\n",
    "if date_cal_start < date_stn_start:\n",
    "    sys.exit('The calculation period is earlier than the station period')\n",
    "if date_cal_end > date_stn_end:\n",
    "    sys.exit('The calculation period is later than the station period')\n",
    "\n",
    "date_cal_start2 = dt.datetime.strptime(str(date_cal_start), '%Y%m%d')\n",
    "date_cal_end2 = dt.datetime.strptime(str(date_cal_end), '%Y%m%d')\n",
    "ntimes = (date_cal_end2 - date_cal_start2).days + 1  # time steps to be processed\n",
    "\n",
    "date_stn_start2 = dt.datetime.strptime(str(date_stn_start), '%Y%m%d')\n",
    "loc_start = (date_cal_start2 - date_stn_start2).days  # start location in the netcdf file\n",
    "loc_end = loc_start + ntimes\n",
    "\n",
    "# seconds since 1970-1-1 0:0:0\n",
    "daydiff = (date_cal_start2 - dt.datetime(1970, 1, 1)).days\n",
    "seconds = (np.arange(ntimes) + daydiff) * 86400\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# 2. read study area basic information\n",
    "print('Read study area basic information')\n",
    "ncfid = nc.Dataset(FileGridInfo)\n",
    "gridlat = ncfid.variables['latitude'][:].data\n",
    "gridlon = ncfid.variables['longitude'][:].data\n",
    "gridele = ncfid.variables['elev'][:].data\n",
    "gridgns = ncfid.variables['gradient_n_s'][:].data\n",
    "gridgwe = ncfid.variables['gradient_w_e'][:].data\n",
    "mask = ncfid.variables['mask'][:].data  # 1: grids to be considered; the other values: invalid grids\n",
    "ncfid.close()\n",
    "\n",
    "nrows, ncols = np.shape(gridlat)\n",
    "gridinfo = np.zeros([nrows, ncols, 6])\n",
    "gridinfo[:, :, 0] = 1\n",
    "gridinfo[:, :, 1] = gridlat\n",
    "gridinfo[:, :, 2] = gridlon\n",
    "gridinfo[:, :, 3] = gridele\n",
    "gridinfo[:, :, 4] = gridgns\n",
    "gridinfo[:, :, 5] = gridgwe\n",
    "del gridlat, gridlon, gridele, gridgns, gridgwe\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# 3. read data (prcp, tmin, tmax) from station files\n",
    "print('Read station precipitation and temperature data')\n",
    "prcp_stn = np.nan * np.zeros([nstn, ntimes])\n",
    "tmin_stn = np.nan * np.zeros([nstn, ntimes])\n",
    "tmax_stn = np.nan * np.zeros([nstn, ntimes])\n",
    "for i in range(nstn):\n",
    "    filei = PathStn + '/' + stnID[i] + '.nc'\n",
    "    try:\n",
    "        ncfid = nc.Dataset(filei)\n",
    "        varlist = ncfid.variables.keys()\n",
    "        if 'prcp' in varlist:\n",
    "            prcp_stn[i, :] = ncfid.variables['prcp'][loc_start:loc_end].data\n",
    "        if 'tmin' in varlist:\n",
    "            tmin_stn[i, :] = ncfid.variables['tmin'][loc_start:loc_end].data\n",
    "        if 'tmax' in varlist:\n",
    "            tmax_stn[i, :] = ncfid.variables['tmax'][loc_start:loc_end].data\n",
    "        ncfid.close()\n",
    "    except:\n",
    "        print('fail to read station:', filei)\n",
    "\n",
    "tmin_stn[np.abs(tmin_stn) > 100] = np.nan\n",
    "tmax_stn[np.abs(tmax_stn) > 100] = np.nan\n",
    "tmean_stn = (tmin_stn + tmax_stn) / 2\n",
    "trange_stn = np.abs(tmax_stn - tmin_stn)\n",
    "prcp_stn[(prcp_stn > 0) & (prcp_stn < 0.05)] = 0  # what is the best threshold?\n",
    "del tmin_stn, tmax_stn\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# 4. calculate auto_corr and t_p_corr\n",
    "print('Calculate correlation (auto_cc and t_p_cc)')\n",
    "local_window = np.min([windows, int(ntimes * 0.1)])\n",
    "if lag > ntimes - 2:  # at least two values are needed to calculate cc\n",
    "    print('The time lag for auto_cc calculation is too large.')\n",
    "    lag = 1\n",
    "    if lag > ntimes - 2:\n",
    "        lag = 0\n",
    "\n",
    "auto_corr = np.zeros(nstn)\n",
    "t_p_corr = np.zeros(nstn)\n",
    "\n",
    "for i in range(nstn):\n",
    "    tmeani = tmean_stn[i, :]\n",
    "    trangei = trange_stn[i, :]\n",
    "    prcpi = prcp_stn[i, :]\n",
    "    # smooth tmean_stn/trange_stn: moving average\n",
    "    tmean_smoothi = np.convolve(tmeani, np.ones((local_window,)) / local_window, mode='same')\n",
    "    trange_smoothi = np.convolve(trangei, np.ones((local_window,)) / local_window, mode='same')\n",
    "    # lag auto_corr of tmean_stn\n",
    "    cci = np.corrcoef(tmean_smoothi[0:-lag], tmean_smoothi[lag:])\n",
    "    auto_corr[i] = cci[0, 1]\n",
    "    # t_p_corr\n",
    "    cci = np.corrcoef(trange_smoothi, prcpi)\n",
    "    t_p_corr[i] = cci[0, 1]\n",
    "\n",
    "auto_corr[abs(auto_corr) > 1] = np.nan\n",
    "t_p_corr[abs(t_p_corr) > 1] = np.nan\n",
    "mean_autocorr = np.nanmean(auto_corr)\n",
    "mean_tp_corr = np.nanmean(t_p_corr)\n",
    "print('Tmean lag-1 autocorrelation: ', mean_autocorr)\n",
    "print('Trange-prcp correlation: ', mean_tp_corr)\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# 5. find neighboring stations and calculate distance-based weights\n",
    "if os.path.isfile(FileWeight) and overwrite_weight != 1:\n",
    "    print('FileWeight exists. loading ...')\n",
    "    with np.load(FileWeight) as datatemp:\n",
    "        near_grid_prcpLoc = datatemp['near_grid_prcpLoc']\n",
    "        near_grid_prcpWeight = datatemp['near_grid_prcpWeight']\n",
    "        near_grid_tempLoc = datatemp['near_grid_tempLoc']\n",
    "        near_grid_tempWeight = datatemp['near_grid_tempWeight']\n",
    "        near_stn_prcpLoc = datatemp['near_stn_prcpLoc']\n",
    "        near_stn_prcpWeight = datatemp['near_stn_prcpWeight']\n",
    "        near_stn_tempLoc = datatemp['near_stn_tempLoc']\n",
    "        near_stn_tempWeight = datatemp['near_stn_tempWeight']\n",
    "    del datatemp\n",
    "else:\n",
    "    # 5.1 for each grid cell, find its neighboring stations (near_grid_*)\n",
    "    print('calculate station weights for each grid cell')\n",
    "    # address stations that don't have prcp_stn or tmean_stn data\n",
    "    latlon_prcp = np.zeros([nstn, 3])\n",
    "    latlon_tmean = np.zeros([nstn, 3])\n",
    "    latlon_prcp[:, 0:2] = stninfo[:, 1:3]\n",
    "    latlon_tmean[:, 0:2] = stninfo[:, 1:3]\n",
    "    for i in range(nstn):\n",
    "        # this is for serially complete station data. if stations have missing values, this should be modified\n",
    "        if np.isnan(prcp_stn[i, 0]):\n",
    "            latlon_prcp[i, 0:2] = np.array([np.nan, np.nan])\n",
    "        if np.isnan(tmean_stn[i, 0]):\n",
    "            latlon_tmean[i, 0:2] = np.array([np.nan, np.nan])\n",
    "    latlon_prcp[:, 2] = np.arange(nstn)  # station ID numbers/oders\n",
    "    latlon_tmean[:, 2] = np.arange(nstn)  # station ID numbers/oders\n",
    "\n",
    "    # simple distance threshold\n",
    "    try_radius = search_radius / 100  # try within this degree (assume 1 degree ~= 100 km). if failed, expanding to all stations.\n",
    "\n",
    "    # initialization\n",
    "    near_grid_prcpLoc = -999 * np.ones([nrows, ncols, nearstn_max], dtype=int)\n",
    "    near_grid_prcpDist = -999 * np.ones([nrows, ncols, nearstn_max], dtype=float)\n",
    "    near_grid_prcpWeight = -999 * np.ones([nrows, ncols, nearstn_max], dtype=float)\n",
    "    near_grid_tempLoc = -999 * np.ones([nrows, ncols, nearstn_max], dtype=int)\n",
    "    near_grid_tempDist = -999 * np.ones([nrows, ncols, nearstn_max], dtype=float)\n",
    "    near_grid_tempWeight = -999 * np.ones([nrows, ncols, nearstn_max], dtype=float)\n",
    "\n",
    "    for rr in range(nrows):\n",
    "        for cc in range(ncols):\n",
    "            if mask[rr, cc] == 1:\n",
    "                latlon_gridrc = gridinfo[rr, cc, 1:3]\n",
    "\n",
    "                # prcp_stn\n",
    "                near_gridLocrc, near_gridDistrc, near_gridWeightrc = \\\n",
    "                    au.find_nearstn(latlon_gridrc, latlon_prcp, try_radius, search_radius, max_dist, nearstn_min,\n",
    "                                    nearstn_max)\n",
    "                near_grid_prcpLoc[rr, cc, :] = near_gridLocrc\n",
    "                near_grid_prcpDist[rr, cc, :] = near_gridDistrc\n",
    "                near_grid_prcpWeight[rr, cc, :] = near_gridWeightrc\n",
    "\n",
    "                # tmean_stn and trange_stn\n",
    "                near_gridLocrc, near_gridDistrc, near_gridWeightrc = \\\n",
    "                    au.find_nearstn(latlon_gridrc, latlon_tmean, try_radius, search_radius, max_dist, nearstn_min,\n",
    "                                    nearstn_max)\n",
    "                near_grid_tempLoc[rr, cc, :] = near_gridLocrc\n",
    "                near_grid_tempDist[rr, cc, :] = near_gridDistrc\n",
    "                near_grid_tempWeight[rr, cc, :] = near_gridWeightrc\n",
    "\n",
    "    # 5.2 for station, find its neighboring stations (near_stn_*)\n",
    "    print('calculate station weights for each station')\n",
    "    # initialization\n",
    "    near_stn_prcpLoc = -999 * np.ones([nstn, nearstn_max], dtype=int)\n",
    "    near_stn_prcpDist = -999 * np.ones([nstn, nearstn_max], dtype=float)\n",
    "    near_stn_prcpWeight = -999 * np.ones([nstn, nearstn_max], dtype=float)\n",
    "    near_stn_tempLoc = -999 * np.ones([nstn, nearstn_max], dtype=int)\n",
    "    near_stn_tempDist = -999 * np.ones([nstn, nearstn_max], dtype=float)\n",
    "    near_stn_tempWeight = -999 * np.ones([nstn, nearstn_max], dtype=float)\n",
    "    for i in range(nstn):\n",
    "        # prcp_stn\n",
    "        if not np.isnan(latlon_prcp[i, 0]):\n",
    "            latlon_target = latlon_prcp[i, 0:2]\n",
    "            latlon_prcpi = latlon_prcp.copy()\n",
    "            latlon_prcpi[i, 0:2] = np.array([np.nan, np.nan])\n",
    "            near_stnLocrc, near_stnDistrc, near_stnWeightrc = \\\n",
    "                au.find_nearstn(latlon_target, latlon_prcpi, try_radius, search_radius, max_dist, nearstn_min,\n",
    "                                nearstn_max)\n",
    "            near_stn_prcpLoc[i, :] = near_stnLocrc\n",
    "            near_stn_prcpDist[i, :] = near_stnDistrc\n",
    "            near_stn_prcpWeight[i, :] = near_stnWeightrc\n",
    "\n",
    "        # tmean_stn and trange_stn\n",
    "        if not np.isnan(latlon_tmean[i, 0]):\n",
    "            latlon_target = latlon_tmean[i, 0:2]\n",
    "            latlon_tmeani = latlon_tmean.copy()\n",
    "            latlon_tmeani[i, 0:2] = np.array([np.nan, np.nan])\n",
    "            near_stnLocrc, near_stnDistrc, near_stnWeightrc = \\\n",
    "                au.find_nearstn(latlon_target, latlon_tmeani, try_radius, search_radius, max_dist, nearstn_min,\n",
    "                                nearstn_max)\n",
    "            near_stn_tempLoc[i, :] = near_stnLocrc\n",
    "            near_stn_tempDist[i, :] = near_stnDistrc\n",
    "            near_stn_tempWeight[i, :] = near_stnWeightrc\n",
    "\n",
    "    # save data\n",
    "    np.savez_compressed(FileWeight, near_grid_prcpLoc=near_grid_prcpLoc, near_grid_prcpDist=near_grid_prcpDist,\n",
    "                        near_grid_prcpWeight=near_grid_prcpWeight, near_grid_tempLoc=near_grid_tempLoc,\n",
    "                        near_grid_tempDist=near_grid_tempDist, near_grid_tempWeight=near_grid_tempWeight,\n",
    "                        near_stn_prcpLoc=near_stn_prcpLoc, near_stn_prcpDist=near_stn_prcpDist,\n",
    "                        near_stn_prcpWeight=near_stn_prcpWeight, near_stn_tempLoc=near_stn_tempLoc,\n",
    "                        near_stn_tempDist=near_stn_tempDist, near_stn_tempWeight=near_stn_tempWeight)\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# 6. start spatial regression\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# 6.1 estimate regression error at station points\n",
    "if os.path.isfile(FileRegError) and overwrite_regress != 1:\n",
    "    print('FileRegError exists. loading ...')\n",
    "    with np.load(FileRegError) as datatemp:\n",
    "        pcp_err_stn = datatemp['pcp_err_stn']\n",
    "        tmean_err_stn = datatemp['tmean_err_stn']\n",
    "        trange_err_stn = datatemp['trange_err_stn']\n",
    "    del datatemp\n",
    "else:\n",
    "    print('Estimate regression error at station points')\n",
    "    pcp_err_stn = -999 * np.ones([nstn, ntimes])\n",
    "    tmean_err_stn = -999 * np.ones([nstn, ntimes])\n",
    "    trange_err_stn = -999 * np.ones([nstn, ntimes])\n",
    "\n",
    "    for t in range(ntimes):\n",
    "        # assign vectors of station alues for prcp_stn, temp, for current time step\n",
    "        # transform prcp_stn to approximate normal distribution\n",
    "        y_prcp = au.transform(prcp_stn[:, t], trans_exp, trans_mode)\n",
    "        y_tmean = tmean_stn[:, t]\n",
    "        y_trange = trange_stn[:, t]\n",
    "\n",
    "        for gg in range(nstn):\n",
    "            if prcp_stn[gg, t] > -1:\n",
    "                # reduced matrices for precip\n",
    "                nstn_prcp = int(np.sum(near_stn_prcpLoc[gg, :] > -1))\n",
    "                if nstn_prcp >= nearstn_min:\n",
    "                    w_pcp_red = np.zeros([nstn_prcp, nstn_prcp])\n",
    "                    for i in range(nstn_prcp):\n",
    "                        w_pcp_red[i, i] = near_stn_prcpWeight[gg, i]  # eye matrix: stn weight in one-one line\n",
    "                    w_pcp_1d = near_stn_prcpWeight[gg, 0:nstn_prcp]  # stn weight\n",
    "                    w_pcp_1d_loc = near_stn_prcpLoc[gg, 0:nstn_prcp]  # stn ID number/location\n",
    "                    y_prcp_red = y_prcp[w_pcp_1d_loc]  # transformed prcp_stn\n",
    "                    x_red = stninfo[w_pcp_1d_loc, :]  # station lat/lon/ele/slope_ns/slope_we\n",
    "\n",
    "                    yp_red = np.zeros(nstn_prcp)  # pop: 0/1\n",
    "                    yp_red[prcp_stn[w_pcp_1d_loc, t] > 0] = 1\n",
    "                    ndata = np.sum(yp_red == 1)  # number of prcp_stn>0\n",
    "                else:\n",
    "                    # there are not enough nearby stations\n",
    "                    x_red = 0  # not really necessary. just to stop warming from Pycharm\n",
    "                    w_pcp_red = 0  # not really necessary. just to stop warming from Pycharm\n",
    "                    yp_red = 0  # not really necessary. just to stop warming from Pycharm\n",
    "                    y_prcp_red = 0  # not really necessary. just to stop warming from Pycharm\n",
    "                    ndata = 0\n",
    "\n",
    "                # prcp processing\n",
    "                if ndata == 0:\n",
    "                    # nearby stations do not have positive prcp data\n",
    "                    pcp_err_stn[gg, t] = 0\n",
    "                else:\n",
    "                    # tmp needs to be matmul(TX, X) where TX = TWX_red and X = X_red\n",
    "                    mat_test = np.matmul(np.transpose(x_red), w_pcp_red)\n",
    "                    tmp = np.matmul(mat_test, x_red)\n",
    "                    vv = np.max(np.abs(tmp), axis=1)\n",
    "\n",
    "                    # decide if slope is to be used in regression\n",
    "                    if np.any(vv) == 0 or (abs(stninfo[gg, 4]) < 3.6 or abs(stninfo[gg, 5]) < 3.6):\n",
    "                        slope_flag_pcp = 0\n",
    "                        x_red_use = x_red[:, 0:4]  # do not use slope in regression\n",
    "                        stninfo_use = stninfo[gg, 0:4]\n",
    "                    else:\n",
    "                        slope_flag_pcp = 1\n",
    "                        x_red_use = x_red\n",
    "                        stninfo_use = stninfo[gg, :]\n",
    "\n",
    "                    tx_red = np.transpose(x_red_use)\n",
    "                    twx_red = np.matmul(tx_red, w_pcp_red)\n",
    "\n",
    "                    # calculate pcp\n",
    "                    b = reg.least_squares(x_red_use, y_prcp_red, twx_red)\n",
    "                    pcpgg = np.dot(stninfo_use, b)\n",
    "                    pcp_err_stn[gg, t] = pcpgg - y_prcp[gg]\n",
    "\n",
    "            # tmean/trange processing\n",
    "            if y_tmean[gg] > -100:\n",
    "                # reduced matrices for tmean_stn/trange_stn\n",
    "                nstn_temp = int(np.sum(near_stn_tempLoc[gg, :] > -1))\n",
    "                if nstn_temp >= nearstn_min:\n",
    "                    w_temp_red = np.zeros([nstn_temp, nstn_temp])\n",
    "                    for i in range(nstn_temp):\n",
    "                        w_temp_red[i, i] = near_stn_tempWeight[gg, i]  # eye matrix: stn weight in one-one lien\n",
    "                    w_temp_1d = near_stn_tempWeight[gg, 0:nstn_temp]  # stn weight\n",
    "                    w_temp_1d_loc = near_stn_tempLoc[gg, 0:nstn_temp]  # stn ID number/location\n",
    "                    y_tmean_red = y_tmean[w_temp_1d_loc]  # transformed temp\n",
    "                    y_trange_red = y_trange[w_temp_1d_loc]  # transformed temp\n",
    "                    x_red_t = stninfo[w_temp_1d_loc, :]  # station lat/lon/ele/slope_ns/slope_we\n",
    "\n",
    "                    ndata_t = np.sum(y_tmean_red > -100)\n",
    "                    nodata_t = nstn_temp - ndata_t  # invalid temperature\n",
    "                else:\n",
    "                    x_red_t = 0  # not really necessary. just to stop warming from Pycharm\n",
    "                    w_temp_red = 0  # not really necessary. just to stop warming from Pycharm\n",
    "                    y_tmean_red = 0  # not really necessary. just to stop warming from Pycharm\n",
    "                    y_trange_red = 0  # not really necessary. just to stop warming from Pycharm\n",
    "                    ndata_t = 0\n",
    "                    nodata_t = 0\n",
    "\n",
    "                if ndata_t > 0:\n",
    "                    stninfo_use = stninfo[gg, 0:4]\n",
    "                    x_red_use = x_red_t[:, 0:4]  # do not use slope for temperature\n",
    "                    tx_red = np.transpose(x_red_use)\n",
    "                    twx_red = np.matmul(tx_red, w_temp_red)\n",
    "\n",
    "                    b = reg.least_squares(x_red_use, y_tmean_red, tx_red)\n",
    "                    tmeangg = np.dot(stninfo_use, b)\n",
    "                    tmean_err_stn[gg, t] = tmeangg - y_tmean[gg]\n",
    "\n",
    "                    b = reg.least_squares(x_red_use, y_trange_red, tx_red)\n",
    "                    trangegg = np.dot(stninfo_use, b)\n",
    "                    trange_err_stn[gg, t] = trangegg - y_trange[gg]\n",
    "\n",
    "    np.savez_compressed(FileRegError, pcp_err_stn=pcp_err_stn, tmean_err_stn=tmean_err_stn,\n",
    "                        trange_err_stn=trange_err_stn, stninfo=stninfo)\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# regression for each grid cell\n",
    "if os.path.isfile(FileRegression) and overwrite_regress != 1:\n",
    "    print('FileRegression exists. loading ...')\n",
    "    with np.load(FileRegression) as datatemp:\n",
    "        pop = datatemp['pop']\n",
    "        pcp = datatemp['pcp']\n",
    "        tmean = datatemp['tmean']\n",
    "        trange = datatemp['trange']\n",
    "        pcp_err = datatemp['pcp_err']\n",
    "        tmean_err = datatemp['tmean_err']\n",
    "        trange_err = datatemp['trange_err']\n",
    "        y_max = datatemp['y_max']\n",
    "    del datatemp\n",
    "else:\n",
    "    pass\n",
    "\n",
    "    # 6.2 initialization\n",
    "    print('Locally weighted regression of precipitation and temperature')\n",
    "    tmp_weight_arr = np.eye(nearstn_max)\n",
    "    y_max = -3 * np.ones([nrows, ncols, ntimes])\n",
    "    pcp = -3 * np.ones([nrows, ncols, ntimes])\n",
    "    pop = np.zeros([nrows, ncols, ntimes])\n",
    "    pcp_err = np.zeros([nrows, ncols, ntimes])\n",
    "    tmean = np.zeros([nrows, ncols, ntimes])\n",
    "    trange = np.zeros([nrows, ncols, ntimes])\n",
    "    tmean_err = np.zeros([nrows, ncols, ntimes])\n",
    "    trange_err = np.zeros([nrows, ncols, ntimes])\n",
    "\n",
    "    # start regression ...\n",
    "    # loop through time steps\n",
    "    for t in range(ntimes):\n",
    "        print('Regression time step: ', t + 1, '---Total time steps: ', ntimes)\n",
    "        # assign vectors of station alues for prcp_stn, temp, for current time step\n",
    "        # transform prcp_stn to approximate normal distribution\n",
    "        y_prcp = au.transform(prcp_stn[:, t], trans_exp, trans_mode)\n",
    "        y_tmean = tmean_stn[:, t]\n",
    "        y_trange = trange_stn[:, t]\n",
    "\n",
    "        # loop through grids (row, col)\n",
    "        for rr in range(nrows):\n",
    "            for cc in range(ncols):\n",
    "                if mask[rr, cc] != 1:\n",
    "                    # the grid is outside mask extent\n",
    "                    continue\n",
    "\n",
    "                ############################################################################################################\n",
    "\n",
    "                # 6.3 Precipitation estimation (pop and pcp)\n",
    "                nstn_prcp = int(np.sum(near_grid_prcpLoc[rr, cc, :] > -1))\n",
    "                if nstn_prcp < nearstn_min:\n",
    "                    print('Precipitation regression: current time step, row, and col are', t, rr, cc)\n",
    "                    sys.exit('Cannot find enough input stations for this grid cell')\n",
    "                else:\n",
    "                    # 6.3.1 reduced matrices for precipitation\n",
    "                    w_pcp_red = np.zeros([nstn_prcp, nstn_prcp])\n",
    "                    for i in range(nstn_prcp):\n",
    "                        w_pcp_red[i, i] = near_grid_prcpWeight[rr, cc, i]  # eye matrix: stn weight in one-one lien\n",
    "                    w_pcp_1d = near_grid_prcpWeight[rr, cc, 0:nstn_prcp]  # stn weight\n",
    "                    w_pcp_1d_loc = near_grid_prcpLoc[rr, cc, 0:nstn_prcp]  # stn ID number/location\n",
    "                    y_prcp_red = y_prcp[w_pcp_1d_loc]  # transformed prcp_stn\n",
    "                    x_red = stninfo[w_pcp_1d_loc, :]  # station lat/lon/ele/slope_ns/slope_we\n",
    "\n",
    "                    yp_red = np.zeros(nstn_prcp)  # pop: 0/1\n",
    "                    yp_red[prcp_stn[w_pcp_1d_loc, t] > 0] = 1\n",
    "                    ndata = np.sum(yp_red == 1)  # number of prcp_stn>0\n",
    "                    nodata = np.sum(yp_red == 0)\n",
    "                    y_max[rr, cc, t] = max(y_prcp_red)\n",
    "\n",
    "                    # 6.3.2 estimate pop, pcp, pcp_err\n",
    "                    # note: pcp_err is based on results from 6.1 and independent with gridded pop and pcp regression\n",
    "                    if ndata == 0:\n",
    "                        # nearby stations do not have positive prcp data (i.e., zero or missing)\n",
    "                        pop[rr, cc, t] = 0\n",
    "                        pcp[rr, cc, t] = y_prcp_red[0]  # corresponding to zero precipitation\n",
    "                        pcp_err[rr, cc, t] = 0\n",
    "                    else:\n",
    "                        # decide if slope is to be used in regression\n",
    "                        # tmp needs to be matmul(TX, X) where TX = TWX_red and X = X_red\n",
    "                        mat_test = np.matmul(np.transpose(x_red), w_pcp_red)\n",
    "                        tmp = np.matmul(mat_test, x_red)\n",
    "                        vv = np.max(np.abs(tmp), axis=1)\n",
    "                        if np.any(vv) == 0 or (abs(gridinfo[rr, cc, 4]) < 3.6 or abs(gridinfo[rr, cc, 5]) < 3.6):\n",
    "                            slope_flag_pcp = 0\n",
    "                            x_red_use = x_red[:, 0:4]  # do not use slope in regression\n",
    "                            gridinfo_use = gridinfo[rr, cc, 0:4]\n",
    "                        else:\n",
    "                            slope_flag_pcp = 1\n",
    "                            x_red_use = x_red\n",
    "                            gridinfo_use = gridinfo[rr, cc, :]\n",
    "\n",
    "                        tx_red = np.transpose(x_red_use)\n",
    "                        twx_red = np.matmul(tx_red, w_pcp_red)\n",
    "\n",
    "                        # calculate pop\n",
    "                        if nodata == 0:\n",
    "                            pop[rr, cc, t] = 1\n",
    "                        else:\n",
    "                            b = reg.logistic_regression(x_red_use, tx_red, yp_red)\n",
    "                            zb = - np.dot(gridinfo_use, b)\n",
    "                            pop[rr, cc, t] = 1 / (1 + np.exp(zb))\n",
    "\n",
    "                        # calculate pcp\n",
    "                        b = reg.least_squares(x_red_use, y_prcp_red, twx_red)\n",
    "                        pcp[rr, cc, t] = np.dot(gridinfo_use, b)\n",
    "\n",
    "                        # 6.4.3 estimate pcp error\n",
    "                        err0 = pcp_err_stn[w_pcp_1d_loc, t]\n",
    "                        pcp_err[rr, cc, t] = (np.sum((err0 ** 2) * w_pcp_1d) / np.sum(w_pcp_1d)) ** 0.5\n",
    "\n",
    "                ############################################################################################################\n",
    "\n",
    "                # 6.5 Temperature estimation (tmean and trange)\n",
    "                # reduced matrices for tmean_stn/trange_stn\n",
    "                nstn_temp = int(np.sum(near_grid_tempLoc[rr, cc, :] > -1))\n",
    "                if nstn_temp < nearstn_min:\n",
    "                    print('Temperature regression: current time step, row, and col are', t, rr, cc)\n",
    "                    sys.exit('Cannot find enough input stations for this grid cell')\n",
    "                else:\n",
    "                    # 6.5.1 reduced matrices for precipitation\n",
    "                    w_temp_red = np.zeros([nstn_temp, nstn_temp])\n",
    "                    for i in range(nstn_temp):\n",
    "                        w_temp_red[i, i] = near_grid_tempWeight[rr, cc, i]  # eye matrix: stn weight in one-one lien\n",
    "                    w_temp_1d = near_grid_tempWeight[rr, cc, 0:nstn_temp]  # stn weight\n",
    "                    w_temp_1d_loc = near_grid_tempLoc[rr, cc, 0:nstn_temp]  # stn ID number/location\n",
    "                    y_tmean_red = y_tmean[w_temp_1d_loc]  # transformed temp\n",
    "                    y_trange_red = y_trange[w_temp_1d_loc]  # transformed temp\n",
    "                    x_red_t = stninfo[w_temp_1d_loc, :]  # station lat/lon/ele/slope_ns/slope_we\n",
    "\n",
    "                    ndata_t = np.sum(y_tmean_red > -100)\n",
    "\n",
    "                    if ndata_t == 0:\n",
    "                        # This is not a problem for serially complete dataset (scd).\n",
    "                        # But even if inputs have missing data, the way in Fortran-based GMET (simple filling) is not\n",
    "                        # a good way. This problem should be solved when finding near stations.\n",
    "                        print('Temperature regression: current time step, row, and col are', t, rr, cc)\n",
    "                        sys.exit('Nearby stations do not have any valid temperature data')\n",
    "                    else:\n",
    "                        # 6.5.1 estimate tmean and its error\n",
    "                        gridinfo_use = gridinfo[rr, cc, 0:4]\n",
    "                        x_red_use = x_red_t[:, 0:4]  # do not use slope for temperature\n",
    "                        tx_red = np.transpose(x_red_use)\n",
    "                        twx_red = np.matmul(tx_red, w_temp_red)\n",
    "                        b = reg.least_squares(x_red_use, y_tmean_red, tx_red)\n",
    "                        tmean[rr, cc, t] = np.dot(gridinfo_use, b)\n",
    "\n",
    "                        # error estimation\n",
    "                        err0 = tmean_err_stn[w_temp_1d_loc, t]\n",
    "                        tmean_err[rr, cc, t] = (np.sum((err0 ** 2) * w_pcp_1d) / np.sum(w_temp_1d)) ** 0.5\n",
    "\n",
    "                        # 6.5.2 estimate trange and its error\n",
    "                        b = reg.least_squares(x_red_use, y_trange_red, tx_red)\n",
    "                        trange[rr, cc, t] = np.dot(gridinfo_use, b)\n",
    "\n",
    "                        # error estimation\n",
    "                        err0 = trange_err_stn[w_temp_1d_loc, t]\n",
    "                        trange_err[rr, cc, t] = (np.sum((err0 ** 2) * w_pcp_1d) / np.sum(w_temp_1d)) ** 0.5\n",
    "\n",
    "    np.savez_compressed(FileRegression, pop=pop, pcp=pcp, tmean=tmean, trange=trange,\n",
    "                        pcp_err=pcp_err, tmean_err=tmean_err, trange_err=trange_err, y_max=y_max)\n",
    "    # io.savemat('outputs.mat',{'pop': pop, 'tmean': tmean, 'trange': trange, 'pcp': pcp, 'tmean_err': tmean_err,\n",
    "    #                           'trange_err': trange_err, 'pcp_err': pcp_err, 'pcp_stn': prcp_stn,\n",
    "    #                           'tmean_stn': tmean_stn, 'trange_stn': trange_stn, 'stninfo': stninfo})\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# 7. save outputs\n",
    "# variables to be saved\n",
    "# pcp, pop, pcp_error, tmean, tmean_error, trange, trange_error\n",
    "# nx, ny, grdlat, grdlon, grdalt, times, mean_autocorr, mean_tp_corr, y_max\n",
    "# if (not os.path.isfile(FileRegression)) or overwrite_regress == 1:\n",
    "#     ncfid = nc.Dataset(FileRegression, 'w', format='NETCDF4')\n",
    "#\n",
    "#     ncfid.createDimension('y', nrows)\n",
    "#     ncfid.createDimension('x', ncols)\n",
    "#     ncfid.createDimension('time', ntimes)\n",
    "#     ncfid.createDimension('const', 1)\n",
    "#\n",
    "#     varin = ncfid.createVariable('time', 'f4', ('time'), zlib=True, complevel=9)\n",
    "#     varin[:] = seconds\n",
    "#     varin.description = 'seconds since 1970-1-1 0:0:0'\n",
    "#\n",
    "#     varin = ncfid.createVariable('auto_corr', 'f4', ('const'), zlib=True, complevel=9)\n",
    "#     varin[:] = mean_autocorr\n",
    "#\n",
    "#     varin = ncfid.createVariable('tp_corr', 'f4', ('const'), zlib=True, complevel=9)\n",
    "#     varin[:] = mean_tp_corr\n",
    "#\n",
    "#     varin = ncfid.createVariable('latitude', 'f4', ('y', 'x'), zlib=True, complevel=9)\n",
    "#     varin[:] = gridinfo[:, :, 1]\n",
    "#\n",
    "#     varin = ncfid.createVariable('longitude', 'f4', ('y', 'x'), zlib=True, complevel=9)\n",
    "#     varin[:] = gridinfo[:, :, 2]\n",
    "#\n",
    "#     varin = ncfid.createVariable('altitude', 'f4', ('y', 'x'), zlib=True, complevel=9)\n",
    "#     varin[:] = gridinfo[:, :, 3]\n",
    "#\n",
    "#     varin = ncfid.createVariable('pcp', 'f4', ('time', 'y', 'x'), zlib=True, complevel=9)\n",
    "#     dw = np.transpose(pcp, [2, 0, 1])\n",
    "#     varin[:] = dw\n",
    "#\n",
    "#     varin = ncfid.createVariable('pop', 'f4', ('time', 'y', 'x'), zlib=True, complevel=9)\n",
    "#     dw = np.transpose(pop, [2, 0, 1])\n",
    "#     varin[:] = dw\n",
    "#\n",
    "#     varin = ncfid.createVariable('pcp_error', 'f4', ('time', 'y', 'x'), zlib=True, complevel=9)\n",
    "#     dw = np.transpose(pcp_err, [2, 0, 1])\n",
    "#     varin[:] = dw\n",
    "#\n",
    "#     varin = ncfid.createVariable('tmean', 'f4', ('time', 'y', 'x'), zlib=True, complevel=9)\n",
    "#     dw = np.transpose(tmean, [2, 0, 1])\n",
    "#     varin[:] = dw\n",
    "#\n",
    "#     varin = ncfid.createVariable('tmean_error', 'f4', ('time', 'y', 'x'), zlib=True, complevel=9)\n",
    "#     dw = np.transpose(tmean_err, [2, 0, 1])\n",
    "#     varin[:] = dw\n",
    "#\n",
    "#     varin = ncfid.createVariable('trange', 'f4', ('time', 'y', 'x'), zlib=True, complevel=9)\n",
    "#     dw = np.transpose(trange, [2, 0, 1])\n",
    "#     varin[:] = dw\n",
    "#\n",
    "#     varin = ncfid.createVariable('trange_error', 'f4', ('time', 'y', 'x'), zlib=True, complevel=9)\n",
    "#     dw = np.transpose(trange_err, [2, 0, 1])\n",
    "#     varin[:] = dw\n",
    "#\n",
    "#     varin = ncfid.createVariable('ymax', 'f4', ('time', 'y', 'x'), zlib=True, complevel=9)\n",
    "#     dw = np.transpose(y_max, [2, 0, 1])\n",
    "#     varin[:] = dw\n",
    "#\n",
    "#     ncfid.close()\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# 8. Evaluate regression results\n",
    "# this part should be moved for the operational version\n",
    "# 8.1 evaluate regression for each station\n",
    "kge_stn = [0] * 3\n",
    "metric_stn = [0]*3\n",
    "kge_pcp_t = [0] * 3 # transformed pcp\n",
    "kge_pcp_t[0] = np.zeros([nstn, 4])\n",
    "for i in range(3):\n",
    "    kge_stn[i] = np.zeros([nstn, 4])\n",
    "    metric_stn[i] = np.zeros([nstn, 4])\n",
    "\n",
    "for i in range(nstn):\n",
    "    obs = prcp_stn[i, :].copy()\n",
    "    obst = au.transform(obs, trans_exp, trans_mode)\n",
    "    est = au.retransform(obst + pcp_err_stn[i, :], trans_exp, trans_mode)\n",
    "    kge_stn[0][i, :] = au.kge2012(obs, est)\n",
    "    metric_stn[0][i, :] = au.metric_stn(obs, est)\n",
    "\n",
    "    obs = prcp_stn[i, :].copy()\n",
    "    obst = au.transform(obs, trans_exp, trans_mode)\n",
    "    est = obst + pcp_err_stn[i, :]\n",
    "    kge_pcp_t[0][i, :] = au.kge2012(obst, est)\n",
    "\n",
    "\n",
    "    obs = tmean_stn[i, :]\n",
    "    est = obs + tmean_err_stn[i, :]\n",
    "    kge_stn[1][i, :] = au.kge2012(obs, est)\n",
    "    metric_stn[1][i, :] = au.metric_stn(obs, est)\n",
    "\n",
    "    obs = trange_stn[i, :]\n",
    "    est = obs + trange_err_stn[i, :]\n",
    "    kge_stn[2][i, :] = au.kge2012(obs, est)\n",
    "    metric_stn[2][i, :] = au.metric_stn(obs, est)\n",
    "\n",
    "# 8.2 evaluate regression of gridded estimates\n",
    "kge_grid = [0] * 3\n",
    "for i in range(3):\n",
    "    kge_grid[i] = np.zeros([nstn, 4])\n",
    "kge_pcp_t[1] = np.zeros([nstn, 4])\n",
    "\n",
    "gridlat = gridinfo[:, 1, 1]\n",
    "gridlon = gridinfo[1, :, 2]\n",
    "for i in range(nstn):\n",
    "    stnlat = stninfo[i, 1]\n",
    "    stnlon = stninfo[i, 2]\n",
    "    row = np.argmin(np.abs(stnlat - gridlat))\n",
    "    col = np.argmin(np.abs(stnlon - gridlon))\n",
    "\n",
    "    obs = prcp_stn[i, :].copy()\n",
    "    est = pcp[row, col, :].copy()\n",
    "    est = au.retransform(est, trans_exp, trans_mode)\n",
    "    kge_grid[0][i, :] = au.kge2012(obs, est)\n",
    "\n",
    "    obs = prcp_stn[i, :].copy()\n",
    "    obs = au.transform(obs, trans_exp, trans_mode)\n",
    "    est = pcp[row, col, :].copy()\n",
    "    kge_pcp_t[1][i, :] = au.kge2012(obs, est)\n",
    "\n",
    "    obs = tmean_stn[i, :]\n",
    "    est = tmean[row, col, :]\n",
    "    kge_grid[1][i, :] = au.kge2012(obs, est)\n",
    "\n",
    "    obs = trange_stn[i, :]\n",
    "    est = trange[row, col, :]\n",
    "    kge_grid[2][i, :] = au.kge2012(obs, est)\n",
    "\n",
    "# 8.3 evaluate using independent stations\n",
    "pathtest = '/Users/localuser/GMET/Example_tgq/StnDaily_test'\n",
    "dirtest = os.listdir(pathtest)\n",
    "ntest = len(dirtest)\n",
    "\n",
    "kge_ind = [0] * 3\n",
    "for i in range(3):\n",
    "    kge_ind[i] = np.zeros([ntest, 4])\n",
    "kge_pcp_t[2] = np.zeros([ntest, 4])\n",
    "\n",
    "gridlat = gridinfo[:, 1, 1]\n",
    "gridlon = gridinfo[1, :, 2]\n",
    "for i in range(ntest):\n",
    "    filei = pathtest + '/' + dirtest[i]\n",
    "    ncfid = nc.Dataset(filei)\n",
    "    stnlat = ncfid.variables['latitude'][:]\n",
    "    stnlon = ncfid.variables['longitude'][:]\n",
    "\n",
    "    varlist = ncfid.variables.keys()\n",
    "    if 'prcp' in varlist:\n",
    "        pcpobs = ncfid.variables['prcp'][:].data\n",
    "    else:\n",
    "        pcpobs = np.nan * np.ones(ntimes)\n",
    "    if 'tmin' in varlist:\n",
    "        tminobs = ncfid.variables['tmin'][:].data\n",
    "    else:\n",
    "        tminobs = np.nan * np.ones(ntimes)\n",
    "    if 'tmax' in varlist:\n",
    "        tmaxobs = ncfid.variables['tmax'][:].data\n",
    "    else:\n",
    "        tmaxobs = np.nan * np.ones(ntimes)\n",
    "    ncfid.close()\n",
    "\n",
    "    row = np.argmin(np.abs(stnlat - gridlat))\n",
    "    col = np.argmin(np.abs(stnlon - gridlon))\n",
    "    tmeanobs = (tminobs + tmaxobs) / 2\n",
    "    trangeobs = (tmaxobs - tminobs)\n",
    "\n",
    "    est = pcp[row, col, :]\n",
    "    est = au.retransform(est, trans_exp, trans_mode)\n",
    "    kge_ind[0][i, :] = au.kge2012(pcpobs, est)\n",
    "\n",
    "    pcpobs = au.transform(pcpobs, trans_exp, trans_mode)\n",
    "    est = pcp[row, col, :]\n",
    "    kge_pcp_t[2][i, :] = au.kge2012(pcpobs, est)\n",
    "\n",
    "    est = tmean[row, col, :]\n",
    "    kge_ind[1][i, :] = au.kge2012(tmeanobs, est)\n",
    "\n",
    "    est = trange[row, col, :]\n",
    "    kge_ind[2][i, :] = au.kge2012(trangeobs, est)\n",
    "\n",
    "\n",
    "print('median kge')\n",
    "print('-------------------------------------')\n",
    "print('pcp after transformation')\n",
    "print('station, grid_train, grid_test' )\n",
    "print('%.3f  %.3f   %.3f' % (np.nanmedian(kge_pcp_t[0][:,0]), np.nanmedian(kge_pcp_t[1][:,0]), np.nanmedian(kge_pcp_t[2][:,0])))\n",
    "print('-------------------------------------')\n",
    "print('pcp before transformation')\n",
    "print('station, grid_train, grid_test' )\n",
    "print('%.3f  %.3f   %.3f' % (np.nanmedian(kge_stn[0][:,0]), np.nanmedian(kge_grid[0][:,0]), np.nanmedian(kge_ind[0][:,0])))\n",
    "print('-------------------------------------')\n",
    "print('tmean')\n",
    "print('station, gridtrain, gridtest' )\n",
    "print('%.3f  %.3f   %.3f' % (np.nanmedian(kge_stn[1][:,0]), np.nanmedian(kge_grid[1][:,0]), np.nanmedian(kge_ind[1][:,0])))\n",
    "print('-------------------------------------')\n",
    "print('trange')\n",
    "print('tation, grid_train, grid_test' )\n",
    "print('%.3f  %.3f   %.3f' % (np.nanmedian(kge_stn[2][:,0]), np.nanmedian(kge_grid[2][:,0]), np.nanmedian(kge_ind[2][:,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py:390: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/usr/local/lib/python3.7/site-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "/usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py:2526: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar)\n",
      "/usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py:2455: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "/usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py:2455: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:11: RuntimeWarning: Mean of empty slice\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: Mean of empty slice\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  del sys.path[0]\n",
      "/Users/localuser/Github/GMET_py/auxiliary.py:90: RuntimeWarning: invalid value encountered in less\n",
      "  data[data < -3] = -3\n",
      "/Users/localuser/Github/GMET_py/auxiliary.py:169: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  np.sum((obs - obs_mean) ** 2, dtype=np.float64))\n",
      "/Users/localuser/Github/GMET_py/auxiliary.py:173: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  (np.std(obs, dtype=np.float64) / obs_mean)\n",
      "/Users/localuser/Github/GMET_py/auxiliary.py:176: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  np.mean(obs, axis=0, dtype=np.float64)\n",
      "/Users/localuser/Github/GMET_py/auxiliary.py:173: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  (np.std(obs, dtype=np.float64) / obs_mean)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median kge\n",
      "-------------------------------------\n",
      "pcp after transformation\n",
      "station, grid_train, grid_test\n",
      "0.643  0.705   -0.101\n",
      "-------------------------------------\n",
      "pcp before transformation\n",
      "station, grid_train, grid_test\n",
      "0.364  0.469   -0.224\n",
      "-------------------------------------\n",
      "tmean\n",
      "station, gridtrain, gridtest\n",
      "0.244  0.263   -19.260\n",
      "-------------------------------------\n",
      "trange\n",
      "tation, grid_train, grid_test\n",
      "0.531  0.562   -0.264\n"
     ]
    }
   ],
   "source": [
    "def metric(obs, pre, preprocess=True):\n",
    "    if preprocess:\n",
    "        # delete the nan values\n",
    "        ind_nan = np.isnan(obs) | np.isnan(pre)\n",
    "        obs = obs[~ind_nan]\n",
    "        pre = pre[~ind_nan]\n",
    "\n",
    "    metout = np.zeros(4)\n",
    "    temp = np.corrcoef(obs, pre)\n",
    "    metout[0] = temp[0][1] # CC\n",
    "    metout[1] = np.nanmean(pre - obs) # ME\n",
    "    metout[2] = np.nanmean( np.abs(pre - obs)) # MAE\n",
    "    metout[3] = np.sqrt(np.sum( np.square(obs - pre) ) / len(obs)) # RMSE\n",
    "    return metout\n",
    "\n",
    "kge_stn = [0] * 3\n",
    "metric_stn = [0]*3\n",
    "kge_pcp_t = [0] * 3 # transformed pcp\n",
    "kge_pcp_t[0] = np.zeros([nstn, 4])\n",
    "for i in range(3):\n",
    "    kge_stn[i] = np.zeros([nstn, 4])\n",
    "    metric_stn[i] = np.zeros([nstn, 4])\n",
    "\n",
    "for i in range(nstn):\n",
    "    obs = prcp_stn[i, :].copy()\n",
    "    obst = au.transform(obs, trans_exp, trans_mode)\n",
    "    est = au.retransform(obst + pcp_err_stn[i, :], trans_exp, trans_mode)\n",
    "    kge_stn[0][i, :] = au.kge2012(obs, est)\n",
    "    metric_stn[0][i, :] = metric(obs, est)\n",
    "\n",
    "    obs = prcp_stn[i, :].copy()\n",
    "    obst = au.transform(obs, trans_exp, trans_mode)\n",
    "    est = obst + pcp_err_stn[i, :]\n",
    "    kge_pcp_t[0][i, :] = au.kge2012(obst, est)\n",
    "\n",
    "\n",
    "    obs = tmean_stn[i, :]\n",
    "    est = obs + tmean_err_stn[i, :]\n",
    "    kge_stn[1][i, :] = au.kge2012(obs, est)\n",
    "    metric_stn[1][i, :] = metric(obs, est)\n",
    "\n",
    "    obs = trange_stn[i, :]\n",
    "    est = obs + trange_err_stn[i, :]\n",
    "    kge_stn[2][i, :] = au.kge2012(obs, est)\n",
    "    metric_stn[2][i, :] = metric(obs, est)\n",
    "\n",
    "# 8.2 evaluate regression of gridded estimates\n",
    "kge_grid = [0] * 3\n",
    "for i in range(3):\n",
    "    kge_grid[i] = np.zeros([nstn, 4])\n",
    "kge_pcp_t[1] = np.zeros([nstn, 4])\n",
    "\n",
    "gridlat = gridinfo[:, 1, 1]\n",
    "gridlon = gridinfo[1, :, 2]\n",
    "for i in range(nstn):\n",
    "    stnlat = stninfo[i, 1]\n",
    "    stnlon = stninfo[i, 2]\n",
    "    row = np.argmin(np.abs(stnlat - gridlat))\n",
    "    col = np.argmin(np.abs(stnlon - gridlon))\n",
    "\n",
    "    obs = prcp_stn[i, :].copy()\n",
    "    est = pcp[row, col, :].copy()\n",
    "    est = au.retransform(est, trans_exp, trans_mode)\n",
    "    kge_grid[0][i, :] = au.kge2012(obs, est)\n",
    "\n",
    "    obs = prcp_stn[i, :].copy()\n",
    "    obs = au.transform(obs, trans_exp, trans_mode)\n",
    "    est = pcp[row, col, :].copy()\n",
    "    kge_pcp_t[1][i, :] = au.kge2012(obs, est)\n",
    "\n",
    "    obs = tmean_stn[i, :]\n",
    "    est = tmean[row, col, :]\n",
    "    kge_grid[1][i, :] = au.kge2012(obs, est)\n",
    "\n",
    "    obs = trange_stn[i, :]\n",
    "    est = trange[row, col, :]\n",
    "    kge_grid[2][i, :] = au.kge2012(obs, est)\n",
    "\n",
    "# 8.3 evaluate using independent stations\n",
    "pathtest = '/Users/localuser/GMET/Example_tgq/StnDaily_test'\n",
    "dirtest = os.listdir(pathtest)\n",
    "ntest = len(dirtest)\n",
    "\n",
    "kge_ind = [0] * 3\n",
    "for i in range(3):\n",
    "    kge_ind[i] = np.zeros([ntest, 4])\n",
    "kge_pcp_t[2] = np.zeros([ntest, 4])\n",
    "\n",
    "gridlat = gridinfo[:, 1, 1]\n",
    "gridlon = gridinfo[1, :, 2]\n",
    "for i in range(ntest):\n",
    "    filei = pathtest + '/' + dirtest[i]\n",
    "    ncfid = nc.Dataset(filei)\n",
    "    stnlat = ncfid.variables['latitude'][:]\n",
    "    stnlon = ncfid.variables['longitude'][:]\n",
    "\n",
    "    varlist = ncfid.variables.keys()\n",
    "    if 'prcp' in varlist:\n",
    "        pcpobs = ncfid.variables['prcp'][:].data\n",
    "    else:\n",
    "        pcpobs = np.nan * np.ones(ntimes)\n",
    "    if 'tmin' in varlist:\n",
    "        tminobs = ncfid.variables['tmin'][:].data\n",
    "    else:\n",
    "        tminobs = np.nan * np.ones(ntimes)\n",
    "    if 'tmax' in varlist:\n",
    "        tmaxobs = ncfid.variables['tmax'][:].data\n",
    "    else:\n",
    "        tmaxobs = np.nan * np.ones(ntimes)\n",
    "    ncfid.close()\n",
    "\n",
    "    row = np.argmin(np.abs(stnlat - gridlat))\n",
    "    col = np.argmin(np.abs(stnlon - gridlon))\n",
    "    tmeanobs = (tminobs + tmaxobs) / 2\n",
    "    trangeobs = (tmaxobs - tminobs)\n",
    "\n",
    "    est = pcp[row, col, :]\n",
    "    est = au.retransform(est, trans_exp, trans_mode)\n",
    "    kge_ind[0][i, :] = au.kge2012(pcpobs, est)\n",
    "\n",
    "    pcpobs = au.transform(pcpobs, trans_exp, trans_mode)\n",
    "    est = pcp[row, col, :]\n",
    "    kge_pcp_t[2][i, :] = au.kge2012(pcpobs, est)\n",
    "\n",
    "    est = tmean[row, col, :]\n",
    "    kge_ind[1][i, :] = au.kge2012(tmeanobs, est)\n",
    "\n",
    "    est = trange[row, col, :]\n",
    "    kge_ind[2][i, :] = au.kge2012(trangeobs, est)\n",
    "\n",
    "\n",
    "print('median kge')\n",
    "print('-------------------------------------')\n",
    "print('pcp after transformation')\n",
    "print('station, grid_train, grid_test' )\n",
    "print('%.3f  %.3f   %.3f' % (np.nanmedian(kge_pcp_t[0][:,0]), np.nanmedian(kge_pcp_t[1][:,0]), np.nanmedian(kge_pcp_t[2][:,0])))\n",
    "print('-------------------------------------')\n",
    "print('pcp before transformation')\n",
    "print('station, grid_train, grid_test' )\n",
    "print('%.3f  %.3f   %.3f' % (np.nanmedian(kge_stn[0][:,0]), np.nanmedian(kge_grid[0][:,0]), np.nanmedian(kge_ind[0][:,0])))\n",
    "print('-------------------------------------')\n",
    "print('tmean')\n",
    "print('station, gridtrain, gridtest' )\n",
    "print('%.3f  %.3f   %.3f' % (np.nanmedian(kge_stn[1][:,0]), np.nanmedian(kge_grid[1][:,0]), np.nanmedian(kge_ind[1][:,0])))\n",
    "print('-------------------------------------')\n",
    "print('trange')\n",
    "print('tation, grid_train, grid_test' )\n",
    "print('%.3f  %.3f   %.3f' % (np.nanmedian(kge_stn[2][:,0]), np.nanmedian(kge_grid[2][:,0]), np.nanmedian(kge_ind[2][:,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.83730499, -0.11184221,  0.26552814,  0.88510567])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90943228, 0.03539273, 1.60096407, 1.9271506 ])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmedian(metric_stn[1],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.68919171, -0.03754733,  2.65115211,  3.31455501])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmedian(metric_stn[2],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
