{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/localuser/Github/PyGMET/auxiliary.py:13: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  data = np.genfromtxt(FileStnInfo, delimiter=',', dtype=None, skip_header=2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import io\n",
    "import os\n",
    "import sys\n",
    "from optimal_interpolation import OImerge\n",
    "import calendar\n",
    "import auxiliary as au\n",
    "from auxiliary_merge import *\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# time periods and methods\n",
    "# vars = sys.argv[1]\n",
    "# vars = [vars]\n",
    "# month = int(sys.argv[2])\n",
    "\n",
    "vars = 'prcp'\n",
    "vars = [vars]\n",
    "month = 2\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# basic settings\n",
    "weightmode = 'BMA' # method used to merge different reanalysis products\n",
    "# vars = ['prcp', 'tmean', 'trange']\n",
    "hwsize = 2  # 5X5 space window used to support estimation at the center grid\n",
    "lontar = np.arange(-180 + 0.05, -50, 0.1)\n",
    "lattar = np.arange(85 - 0.05, 5, -0.1)\n",
    "\n",
    "# \"Gaussian\": prcp will be transformed into normal distributions; \"Actual\": actual space\n",
    "# \"Gaussian\" is not a good choice because station prcp regression using box-cox has large underestimation\n",
    "prcp_space = 'Actual'\n",
    "\n",
    "### Local Mac settings\n",
    "# input files/paths\n",
    "path_bac = '/Users/localuser/Research/EMDNA/merge' # data that will be used as background\n",
    "path_obs = '/Users/localuser/Research/EMDNA/regression' # data that will be used as observation\n",
    "near_file_GMET = '/Users/localuser/Research/EMDNA/regression/weight_nearstn.npz' # near station of stations/grids\n",
    "file_mask = './DEM/NA_DEM_010deg_trim.mat'\n",
    "FileStnInfo = '/Users/localuser/GMET/pyGMET_NA/stnlist_whole.txt'\n",
    "gmet_stndatafile = '/Users/localuser/Research/EMDNA/stndata_whole.npz'\n",
    "\n",
    "# output files/paths (can also be used as inputs once generated)\n",
    "path_oimerge = '/Users/localuser/Research/EMDNA/oimerge'\n",
    "\n",
    "### Local Mac settings\n",
    "\n",
    "\n",
    "# ### Plato settings\n",
    "# # input files/paths\n",
    "# path_bac = '/datastore/GLOBALWATER/CommonData/EMDNA/ReanalysisCorrMerge/Reanalysis_merge'\n",
    "# path_obs = '/datastore/GLOBALWATER/CommonData/EMDNA/PyGMETout'\n",
    "# near_file_GMET = '/datastore/GLOBALWATER/CommonData/EMDNA/PyGMETout/weight.npz'\n",
    "# file_mask = '/datastore/GLOBALWATER/CommonData/EMDNA/DEM/NA_DEM_010deg_trim.mat'\n",
    "# FileStnInfo = '/home/gut428/GMET/eCAI_EMDNA/StnGridInfo/stnlist_whole.txt'\n",
    "# gmet_stndatafile = '/datastore/GLOBALWATER/CommonData/EMDNA/stndata_whole.npz'\n",
    "#\n",
    "# # output files/paths (can also be used as inputs once generated)\n",
    "# path_oimerge = '/home/gut428/OImerge'\n",
    "# ### Plato settings\n",
    "\n",
    "file_regression_stn = path_obs + '/daily_regression_stn.npz'\n",
    "file_corrmerge_stn = [''] * len(vars)\n",
    "for i in range(len(vars)):\n",
    "    if vars[i] == 'pop':\n",
    "        file_corrmerge_stn[i] = path_bac + '/merge_stn_' + vars[i] + '_GWR_' + weightmode + '.npz'\n",
    "    else:\n",
    "        file_corrmerge_stn[i] = path_bac + '/mergecorr_stn_' + vars[i] + '_GWRQM_' + weightmode + '.npz'\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# basic processing\n",
    "mask = io.loadmat(file_mask)\n",
    "mask = mask['DEM']\n",
    "mask[~np.isnan(mask)] = 1  # 1: valid pixels\n",
    "nrows, ncols = np.shape(mask)\n",
    "\n",
    "# date\n",
    "date_list, date_number = m_DateList(1979, 2018, 'ByYear')\n",
    "\n",
    "# stninfo\n",
    "stnID, stninfo = au.readstnlist(FileStnInfo)\n",
    "nstn = len(stnID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OI merge at grids: prcp\n",
      "month 2\n",
      "load gridded merged data\n",
      "calculate OI merging weights\n"
     ]
    }
   ],
   "source": [
    "for v in range(len(vars)):\n",
    "    print('OI merge at grids:', vars[v])\n",
    "\n",
    "    # load station original observations\n",
    "    datatemp = np.load(gmet_stndatafile)\n",
    "    if vars[v] == 'pop':\n",
    "        observation_stn = datatemp['prcp_stn']\n",
    "        observation_stn[observation_stn > 0] = 1\n",
    "    else:\n",
    "        observation_stn = datatemp[vars[v] + '_stn']\n",
    "    del datatemp\n",
    "\n",
    "    # load station regression estimates (obs)\n",
    "    datatemp = np.load(file_regression_stn)\n",
    "    regression_stn = datatemp[vars[v]]\n",
    "    del datatemp\n",
    "\n",
    "    # load corrected/merged reanalysis data at all station points (those are totally independent with station observations)\n",
    "    datatemp = np.load(file_corrmerge_stn[v])\n",
    "    reafinal_stn = datatemp['reamerge_stn']\n",
    "    nstn, ntimes = np.shape(reafinal_stn)\n",
    "    del datatemp\n",
    "\n",
    "    # load near station information\n",
    "    datatemp = np.load(near_file_GMET)\n",
    "    if vars[v] == 'prcp' or vars[v] == 'pop':\n",
    "        near_loc = datatemp['near_grid_prcpLoc']\n",
    "        near_weight = datatemp['near_grid_prcpWeight']\n",
    "        near_dist = datatemp['near_grid_prcpDist']\n",
    "    else:\n",
    "        near_loc = datatemp['near_grid_tempLoc']\n",
    "        near_weight = datatemp['near_grid_tempWeight']\n",
    "        near_dist = datatemp['near_grid_tempDist']\n",
    "    near_loc = np.flipud(near_loc)\n",
    "    near_weight = np.flipud(near_weight)\n",
    "    near_dist = np.flipud(near_dist)\n",
    "    del datatemp\n",
    "\n",
    "    # load OI merged data at station points\n",
    "    filemerge_stn = path_oimerge + '/OImerge_stn_GWRQMBMA_' + vars[v] + '.npz'\n",
    "    datatemp = np.load(filemerge_stn)\n",
    "    oimerge_stn = datatemp['oimerge_stn']\n",
    "    del datatemp\n",
    "\n",
    "    # start OI merging\n",
    "    for m in range(month-1, month):\n",
    "        print('month', m + 1)\n",
    "        indm = (date_number['mm'] == m + 1)\n",
    "        nday = sum(indm)\n",
    "        datem = date_number['yyyy'][indm]\n",
    "\n",
    "        # load gridded merged reanalysis data for all years\n",
    "        print('load gridded merged data')\n",
    "        reagrid_value = np.nan * np.zeros([nrows, ncols, nday], dtype=np.float32)\n",
    "        reagrid_error = np.nan * np.zeros([nrows, ncols, nday], dtype=np.float32)\n",
    "        for y in range(1979, 2019):\n",
    "            indym = datem == y\n",
    "            filey = path_bac + '/bmamerge_' + vars[v] + '_' + str(y*100+m+1) + '.npz'\n",
    "            datatemp = np.load(filey)\n",
    "            reagrid_value[:, :, indym] = datatemp['bma_data']\n",
    "            reagrid_error[:, :, indym] = datatemp['bma_error']\n",
    "            del datatemp\n",
    "\n",
    "        # calculate OI-merging weights for every grids\n",
    "        print('calculate OI merging weights')\n",
    "        file_oiweight = path_oimerge + '/oiweight_' + vars[v] + '_month_' + str(m+1) + '.npz'\n",
    "        if os.path.isfile(file_oiweight):\n",
    "            datatemp = np.load(file_oiweight)\n",
    "            oiweight = datatemp['oiweight']\n",
    "            del datatemp\n",
    "        else:\n",
    "            oiweight = np.nan * np.zeros([nrows, ncols, np.shape(near_loc)[2]], dtype=np.float32)\n",
    "            for r in range(nrows):\n",
    "                if np.mod(r,10)==0:\n",
    "                    print(r)\n",
    "                for c in range(ncols):\n",
    "                    if np.isnan(mask[r, c]):\n",
    "                        continue\n",
    "                    near_loci = near_loc[r, c, :]\n",
    "                    near_loci = near_loci[near_loci > -1]\n",
    "\n",
    "                    b_near = reafinal_stn[near_loci, :][:, indm]\n",
    "                    o_near = regression_stn[near_loci, :][:, indm]\n",
    "                    # this error is from weighted mean. if using nearest neighbor to obtain gridded error, this weight will be more similar to stn-OI\n",
    "                    tar_err_b = reagrid_error[r, c, :]\n",
    "                    near_err_b = b_near - observation_stn[near_loci, :][:, indm]\n",
    "                    near_err_o = o_near - observation_stn[near_loci, :][:, indm]\n",
    "\n",
    "                    # delete possible nan values\n",
    "                    induse = ~np.isnan(tar_err_b + np.sum(near_err_b, axis=0) + np.sum(near_err_o, axis=0))\n",
    "                    weight = OImerge(tar_err_b[induse], near_err_b[:, induse], near_err_o[:, induse], eye_o=0)\n",
    "\n",
    "                    oiweight[r, c, 0:len(weight)] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            oiweight = np.nan * np.zeros([nrows, ncols, np.shape(near_loc)[2]], dtype=np.float32)\n",
    "            for r in range(nrows):\n",
    "                if np.mod(r,10)==0:\n",
    "                    print(r)\n",
    "                for c in range(ncols):\n",
    "                    if np.isnan(mask[r, c]):\n",
    "                        continue\n",
    "                    near_loci = near_loc[r, c, :]\n",
    "                    near_loci = near_loci[near_loci > -1]\n",
    "\n",
    "                    b_near = reafinal_stn[near_loci, :][:, indm]\n",
    "                    o_near = regression_stn[near_loci, :][:, indm]\n",
    "                    # this error is from weighted mean. if using nearest neighbor to obtain gridded error, this weight will be more similar to stn-OI\n",
    "                    tar_err_b = reagrid_error[r, c, :]\n",
    "                    near_err_b = b_near - observation_stn[near_loci, :][:, indm]\n",
    "                    near_err_o = o_near - observation_stn[near_loci, :][:, indm]\n",
    "\n",
    "                    # delete possible nan values\n",
    "                    induse = ~np.isnan(tar_err_b + np.sum(near_err_b, axis=0) + np.sum(near_err_o, axis=0))\n",
    "                    weight = OImerge(tar_err_b[induse], near_err_b[:, induse], near_err_o[:, induse], eye_o=0)\n",
    "\n",
    "                    oiweight[r, c, 0:len(weight)] = weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
