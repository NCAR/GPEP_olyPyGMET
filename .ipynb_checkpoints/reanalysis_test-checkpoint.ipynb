{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var is  prcp\n",
      "weightmode is  BMA\n",
      "years are  2000 2000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import auxiliary as au\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import io\n",
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "from bma_merge import bma\n",
    "from auxiliary_merge import *\n",
    "\n",
    "def empirical_cdf(data, probtar):\n",
    "    # data: vector of data\n",
    "    data2 = data[~np.isnan(data)]\n",
    "    if len(data2) > 0:\n",
    "        ds = np.sort(data2)\n",
    "        probreal = np.arange(len(data2)) / (len(data2)+1)\n",
    "        ecdf_out = np.interp(probtar, probreal, ds)\n",
    "    else:\n",
    "        ecdf_out = np.nan * np.zeros(len(probtar))\n",
    "    return ecdf_out\n",
    "\n",
    "def cdf_correction(cdf_ref, value_ref, cdf_raw, value_raw, value_tar):\n",
    "    prob_tar = np.interp(value_tar, value_raw, cdf_raw)\n",
    "    value_out = np.interp(prob_tar, cdf_ref, value_ref)\n",
    "    return value_out\n",
    "\n",
    "\n",
    "\n",
    "def calculate_anomaly(datatar, dataref, hwsize, amode, upbound=5, lowbound=0.2):\n",
    "    # datatar, dataref: 2D [nstn, ntime]\n",
    "    # amode: anomaly mode ('ratio' or 'diff')\n",
    "    # hwsize: define time window (2*hwsize+1) used to calculate ratio (as ratio for a specific day is too variable)\n",
    "    # upbound/lowbound: upper and lower limitation of ratio/difference\n",
    "    if np.ndim(datatar) == 1:  # only one time step\n",
    "        datatar = datatar[:, np.newaxis]\n",
    "        dataref = dataref[:, np.newaxis]\n",
    "\n",
    "    nstn, ntime = np.shape(datatar)\n",
    "    if ntime < hwsize * 2 + 1:\n",
    "        print('The window size is larger than time steps when calculating ratio between tar and ref datasets')\n",
    "        print('Please set a smaller hwsize')\n",
    "        sys.exit()\n",
    "\n",
    "    anom = np.ones([nstn, ntime])\n",
    "\n",
    "    for i in range(ntime):\n",
    "        if i < hwsize:\n",
    "            windex = np.arange(hwsize * 2 + 1)\n",
    "        elif i >= ntime - hwsize:\n",
    "            windex = np.arange(ntime - hwsize * 2 - 1, ntime)\n",
    "        else:\n",
    "            windex = np.arange(i - hwsize, i + hwsize + 1)\n",
    "        dtari = np.nanmean(datatar[:, windex], axis=1)\n",
    "        drefi = np.nanmean(dataref[:, windex], axis=1)\n",
    "\n",
    "        if amode == 'ratio':\n",
    "            temp = drefi / dtari\n",
    "            temp[(dtari == 0) & (drefi == 0)] = 1\n",
    "            anom[:, i] = temp\n",
    "        elif amode == 'diff':\n",
    "            anom[:, i] = drefi - dtari\n",
    "        else:\n",
    "            sys.exit('Unknow amode. Please use either ratio or diff')\n",
    "\n",
    "    anom[anom > upbound] = upbound\n",
    "    anom[anom < lowbound] = lowbound\n",
    "    return anom\n",
    "\n",
    "def findnearstn(stnlat, stnlon, tarlat, tarlon, nearnum, noself):\n",
    "    # only use lat/lon to find near stations without considering distance in km\n",
    "    # stnlat/stnlon: 1D\n",
    "    # tarlat/tarlon: 1D or 2D\n",
    "    # noself: 1--stnlat and tarlat have overlapped stations, which should be excluded from stnlat\n",
    "\n",
    "    stnll = np.zeros([len(stnlat), 2])\n",
    "    stnll[:, 0] = stnlat\n",
    "    stnll[:, 1] = stnlon\n",
    "\n",
    "    if len(np.shape(tarlat)) == 1:\n",
    "        num = len(tarlat)\n",
    "        nearstn_loc = -1 * np.ones([num, nearnum], dtype=int)\n",
    "        nearstn_dist = -1 * np.ones([num, nearnum], dtype=float)\n",
    "        for i in range(num):\n",
    "            if np.isnan(tarlat[i]) or np.isnan(tarlon[i]):\n",
    "                continue\n",
    "            tari = np.array([tarlat[i], tarlon[i]])\n",
    "            dist = au.distance(tari, stnll)\n",
    "            dist[np.isnan(dist)] = 1000000000\n",
    "            if noself == 1:\n",
    "                dist[dist == 0] = np.inf  # may not be perfect, but work for SCDNA\n",
    "            indi = np.argsort(dist)\n",
    "            nearstn_loc[i, :] = indi[0:nearnum]\n",
    "            nearstn_dist[i, :] = dist[nearstn_loc[i, :]]\n",
    "    elif len(np.shape(tarlat)) == 2:\n",
    "        nrows, ncols = np.shape(tarlat)\n",
    "        nearstn_loc = -1 * np.ones([nrows, ncols, nearnum], dtype=int)\n",
    "        nearstn_dist = -1 * np.ones([nrows, ncols, nearnum], dtype=float)\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if np.isnan(tarlat[r, c]) or np.isnan(tarlon[r, c]):\n",
    "                    continue\n",
    "                tari = np.array([tarlat[r, c], tarlon[r, c]])\n",
    "                dist = au.distance(tari, stnll)\n",
    "                dist[np.isnan(dist)] = 1000000000\n",
    "                indi = np.argsort(dist)\n",
    "                nearstn_loc[r, c, :] = indi[0:nearnum]\n",
    "                nearstn_dist[r, c, :] = dist[nearstn_loc[r, c, :]]\n",
    "    else:\n",
    "        print('The dimensions of tarlat or tarlon are larger than 2')\n",
    "        sys.exit()\n",
    "\n",
    "    return nearstn_loc, nearstn_dist\n",
    "\n",
    "\n",
    "def error_correction_new(corrmode, stndata_i2_near, nearstn_weighti2, readata_stn_i2, readata_i2_near, ecdf_prob):\n",
    "    # corrmode: QM, Mul_Climo, Mul_Daily, Add_Climo, Add_Climo\n",
    "    nearstn_numi2, ntimes = np.shape(stndata_i2_near)\n",
    "    corrdata_out = np.zeros(ntimes)\n",
    "    if corrmode == 'QM':\n",
    "        cdf_rea = empirical_cdf(readata_stn_i2, ecdf_prob)\n",
    "        for j in range(nearstn_numi2):\n",
    "            cdf_ref = empirical_cdf(stndata_i2_near[j, :], ecdf_prob)\n",
    "            qmdata_rj = cdf_correction(ecdf_prob, cdf_ref, ecdf_prob, cdf_rea, readata_stn_i2)\n",
    "            corrdata_out = corrdata_out + qmdata_rj * nearstn_weighti2[j]\n",
    "        corrdata_out = corrdata_out / np.sum(nearstn_weighti2)\n",
    "    elif corrmode[0:3] == 'Mul' or corrmode[0:3] == 'Add':\n",
    "        # multplicative correction or additive correction\n",
    "        if corrmode[4:] == 'Daily':\n",
    "            dtar = readata_i2_near\n",
    "            dref = stndata_i2_near\n",
    "        elif corrmode[4:] == 'Climo':\n",
    "            dtar = np.nanmean(readata_i2_near, axis=1)\n",
    "            dtar = dtar[:, np.newaxis]\n",
    "            dref = np.nanmean(stndata_i2_near, axis=1)\n",
    "            dref = dref[:, np.newaxis]\n",
    "        else:\n",
    "            sys.exit('Unknown corrmode')\n",
    "        if corrmode[0:3] == 'Mul':\n",
    "            corrfactor_i2_near = calculate_anomaly(dtar, dref, 0, 'ratio', 10, 0)  # 10 is default max limit\n",
    "        else:\n",
    "            corrfactor_i2_near = calculate_anomaly(dtar, dref, 0, 'diff', 9999, -9999)\n",
    "        weight_use = np.tile(nearstn_weighti2, (np.shape(corrfactor_i2_near)[1], 1)).T\n",
    "        weight_use[np.isnan(corrfactor_i2_near)] = np.nan\n",
    "        corrfactor_i2 = np.nansum(corrfactor_i2_near * weight_use, axis=0) / np.nansum(weight_use)\n",
    "        if corrmode[0:3] == 'Mul':\n",
    "            corrdata_out = readata_stn_i2 * corrfactor_i2\n",
    "        else:\n",
    "            corrdata_out = readata_stn_i2 + corrfactor_i2\n",
    "    else:\n",
    "        sys.exit('Unknown corrmode')\n",
    "\n",
    "    return corrdata_out\n",
    "\n",
    "def error_correction(dataori, anomaly, mode='ratio'):\n",
    "    # default: time is the last dimension\n",
    "    if mode == 'ratio':\n",
    "        datacorr = dataori * anomaly\n",
    "    elif mode == 'diff':\n",
    "        datacorr = dataori + anomaly\n",
    "    else:\n",
    "        sys.exit('Wrong error correction mode')\n",
    "    return datacorr\n",
    "\n",
    "\n",
    "def calweight(obs, rea, mode, preprocess=True):\n",
    "    ntimes, reanum = np.shape(rea)\n",
    "    if preprocess:\n",
    "        # delete the nan values\n",
    "        ind_nan = np.isnan(obs + np.sum(rea, axis=1))\n",
    "        obs = obs[~ind_nan]\n",
    "        rea = rea[~ind_nan, :]\n",
    "\n",
    "    if len(obs) > 2:\n",
    "        if mode == 'BMA':\n",
    "            weight, sigma, sigma_s = bma(rea, obs)\n",
    "        else:\n",
    "            met = np.zeros(reanum)\n",
    "            if mode == 'RMSE':\n",
    "                for i in range(reanum):\n",
    "                    met[i] = np.sqrt(np.sum(np.square(obs - rea[:, i])) / len(obs))  # RMSE\n",
    "                weight = 1 / (met ** 2)\n",
    "            elif mode == 'CC':\n",
    "                for i in range(reanum):\n",
    "                    met[i] = np.corrcoef(obs, rea[:, i])[0][1]\n",
    "                weight = (met ** 2)\n",
    "    else:\n",
    "        weight = np.ones(reanum) / reanum\n",
    "    if np.any(np.isnan(weight)):\n",
    "        weight = np.ones(reanum) / reanum\n",
    "    return weight\n",
    "\n",
    "def weightmerge(data, weight):\n",
    "    if np.ndim(data) == 2:\n",
    "        weight2 = weight.copy()\n",
    "        weight2[np.isnan(data)] = np.nan\n",
    "        dataout = np.nansum(data * weight2, axis=1) / np.nansum(weight2, axis=1)\n",
    "    elif np.ndim(data) == 3:\n",
    "        weight2 = weight.copy()\n",
    "        weight2[np.isnan(data)] = np.nan\n",
    "        dataout = np.nansum(data * weight2, axis=2) / np.nansum(weight2, axis=2)\n",
    "        dataout[np.isnan(data[:,:,0])]=np.nan\n",
    "    return dataout\n",
    "\n",
    "\n",
    "def merge_correction_stnerror(stndata, stnecdf_prob, readata_stn, nearstn_loc, nearstn_dist, var, corrmode, weightmode):\n",
    "    # corrmode = 'QM'  # QM, Mul_Climo, Mul_Daily, Add_Climo, Add_Climo\n",
    "    # use 2-layer cross-validation to estimate the weight and independent data of merge/correction data\n",
    "    reanum, nstn, ntimes = np.shape(readata_stn)\n",
    "    nprob = len(ecdf_prob)\n",
    "\n",
    "    # initialization\n",
    "    reacorr_stn = np.nan * np.zeros([reanum, nstn, ntimes], dtype=np.float32)  # corrected reanalysis data\n",
    "    reamerge_weight_stn = np.nan * np.zeros([nstn, reanum], dtype=np.float32)  # weight used to obtain reamerge_stn\n",
    "    reamerge_stn = np.nan * np.zeros([nstn, ntimes], dtype=np.float32)  # merged reanalysis at station points\n",
    "\n",
    "    for i1 in range(nstn): # layer-1\n",
    "        if np.mod(i1, 1000) == 0:\n",
    "            print(i1)\n",
    "        if np.isnan(stndata[i1, 0]):\n",
    "            continue\n",
    "        nearstn_loci1 = nearstn_loc[i1, :]\n",
    "        nearstn_disti1 = nearstn_dist[i1, :]\n",
    "        induse = nearstn_loci1 > -1\n",
    "        nearstn_loci1 = nearstn_loci1[induse]\n",
    "        nearstn_disti1 = nearstn_disti1[induse]\n",
    "        nearstn_numi1 = len(nearstn_loci1)\n",
    "        if nearstn_numi1 == 0:\n",
    "            sys.exit('No near station for the target station (layer-1)')\n",
    "\n",
    "        # start layer-2\n",
    "        reamerge_weight_i2 = np.zeros([nearstn_numi1, reanum])\n",
    "        for i2 in range(nearstn_numi1): # layer-2\n",
    "            nearstn_loci2 = nearstn_loc[nearstn_loci1[i2], :]\n",
    "            nearstn_disti2 = nearstn_dist[nearstn_loci1[i2], :]\n",
    "            induse = (nearstn_loci2 > -1) & (nearstn_loci2 != i1) # i1 should be independent\n",
    "            nearstn_loci2 = nearstn_loci2[induse]\n",
    "            nearstn_disti2 = nearstn_disti2[induse]\n",
    "            maxd = np.max([np.max(nearstn_disti2)+1, 100])\n",
    "            nearstn_weighti2 = au.distanceweight(nearstn_disti2, maxd, 3)\n",
    "            nearstn_weighti2 = nearstn_weighti2 / np.sum(nearstn_weighti2)\n",
    "\n",
    "            nearstn_numi2 = len(nearstn_loci2)\n",
    "            if nearstn_numi2 == 0:\n",
    "                sys.exit('No near station for the target station (layer-1)')\n",
    "\n",
    "            # data at i2 station\n",
    "            stndata_i2 = stndata[nearstn_loci1[i2], :]\n",
    "            stndata_i2_near = stndata[nearstn_loci2, :]\n",
    "            readata_stn_i2 = readata_stn[:, nearstn_loci1[i2], :]\n",
    "            readata_i2_near = readata_stn[:, nearstn_loci2, :]\n",
    "\n",
    "            # error correction for each reanalysis dataset using different modes\n",
    "            corrdata_i2 = np.zeros([ntimes, reanum])\n",
    "            for r in range(reanum):\n",
    "                corrdata_i2[:, r] = error_correction_new(corrmode, stndata_i2_near, nearstn_weighti2,\n",
    "                                                         readata_stn_i2[r,:,:], readata_i2_near[r,:,:], ecdf_prob)\n",
    "\n",
    "            # calculate merging weight for i2\n",
    "            if weightmode == 'BMA' and var == 'prcp':\n",
    "                # exclude zero precipitation and carry out box-cox transformation\n",
    "                datatemp = np.zeros([ntimes, reanum + 1])\n",
    "                datatemp[:, 0] = stndata_i2\n",
    "                datatemp[:, 1:] = corrdata_i2\n",
    "                ind0 = np.sum(datatemp >= 0.01, axis=1) == (reanum + 1)  # positive hit events\n",
    "                dobs = box_cox_transform(stndata_i2[ind0])\n",
    "                drea = box_cox_transform(corrdata_i2[ind0, :])\n",
    "            else:\n",
    "                dobs = stndata_i2\n",
    "                drea = corrdata_i2\n",
    "            reamerge_weight_i2[i2, :] = calweight(dobs, drea, weightmode)\n",
    "        # end layer-2\n",
    "\n",
    "        stndata_i1 = stndata[i1, :]\n",
    "        stndata_i1_near = stndata[nearstn_loci1, :]\n",
    "        readata_stn_i1 = readata_stn[:, i1, :]\n",
    "        readata_i1_near = readata_stn[:, nearstn_loci1, :]\n",
    "        maxd = np.max([np.max(nearstn_disti1) + 1, 100])\n",
    "        nearstn_weighti1 = au.distanceweight(nearstn_disti1, maxd, 3)\n",
    "        nearstn_weighti1 = nearstn_weighti1 / np.sum(nearstn_weighti1)\n",
    "\n",
    "        # get corrected data at i1\n",
    "        corrdata_i1 = np.zeros([ntimes, reanum])\n",
    "        for r in range(reanum):\n",
    "            corrdata_i1[:, r] = error_correction_new(corrmode, stndata_i1_near, nearstn_weighti1,\n",
    "                                                     readata_stn_i1[r, :, :], readata_i1_near[r, :, :], ecdf_prob)\n",
    "        reacorr_stn[:, i1, :] = corrdata_i1.T\n",
    "\n",
    "        # get merging weight at i1 and merge reanalysis\n",
    "        # note: this weight is just for independent merging so we can estimate the error of merged reanalysis\n",
    "        # the real weight will be estimated using just one-layer cross-validation\n",
    "        weight_use = np.tile(nearstn_weighti1, (reanum, 1)).T\n",
    "        weight_i1 = np.sum(weight_use * reamerge_weight_i2, axis=0) / np.sum(weight_use)\n",
    "\n",
    "        weight_use = np.tile(weight_i1, (ntimes, 1)).T\n",
    "        if weightmode == 'BMA' and var == 'prcp':\n",
    "            reamerge_stni1 = np.sum(weight_use * box_cox_transform(corrdata_i1), axis=1)\n",
    "            reamerge_stni1 = box_cox_recover(reamerge_stni1)\n",
    "        else:\n",
    "            reamerge_stni1 = np.sum(weight_use * corrdata_i1, axis=1)\n",
    "        reamerge_stn[i1, :] = reamerge_stni1\n",
    "\n",
    "        # get the final merging weight\n",
    "        if weightmode == 'BMA' and var == 'prcp':\n",
    "            # exclude zero precipitation and carry out box-cox transformation\n",
    "            datatemp = np.zeros([ntimes, reanum + 1])\n",
    "            datatemp[:, 0] = stndata_i1\n",
    "            datatemp[:, 1:] = corrdata_i1\n",
    "            ind0 = np.sum(datatemp >= 0.01, axis=1) == (reanum + 1)  # positive hit events\n",
    "            dobs = box_cox_transform(stndata_i1[ind0])\n",
    "            drea = box_cox_transform(corrdata_i1[ind0, :])\n",
    "        else:\n",
    "            dobs = stndata_i1\n",
    "            drea = corrdata_i1\n",
    "        reamerge_weight_stn[i1, :] = calweight(dobs, drea, weightmode)\n",
    "\n",
    "    # note: reamerge_weight_stn is the final merging weight, and reacorr_stn is the final corrected data\n",
    "    # but reamerge_stn is just independent merging estimates which is calculated from 2-layer cross validation\n",
    "    return reamerge_stn, reamerge_weight_stn, reacorr_stn\n",
    "\n",
    "\n",
    "def correction_comparison(stndata, ecdf_prob, readata_stn, nearstn_loc, nearstn_dist):\n",
    "    # compare the performance of daily-scale multiplicative correction and QM\n",
    "    reanum, nstn, ntimes = np.shape(readata_stn)\n",
    "    nprob = len(ecdf_prob)\n",
    "\n",
    "    # initialization\n",
    "    reacorr_qm = np.nan * np.zeros([reanum, nstn, ntimes], dtype=np.float32)  # corrected reanalysis data\n",
    "    reacorr_mulclimo = np.nan * np.zeros([reanum, nstn, ntimes], dtype=np.float32)  # corrected reanalysis data\n",
    "\n",
    "    for i1 in range(1, nstn): # layer-1\n",
    "        # if np.mod(i1, 1000) == 0:\n",
    "        #     print(i1)\n",
    "        if np.isnan(stndata[i1, 0]):\n",
    "            continue\n",
    "\n",
    "        nearstn_loci1 = nearstn_loc[i1, :]\n",
    "        nearstn_disti1 = nearstn_dist[i1, :]\n",
    "        induse = nearstn_loci1 > -1\n",
    "        nearstn_loci1 = nearstn_loci1[induse]\n",
    "        nearstn_disti1 = nearstn_disti1[induse]\n",
    "        nearstn_numi1 = len(nearstn_loci1)\n",
    "        if nearstn_numi1 == 0:\n",
    "            sys.exit('No near station for the target station (layer-1)')\n",
    "\n",
    "        stndata_i1_near = stndata[nearstn_loci1, :]\n",
    "        readata_stn_i1 = readata_stn[:, i1, :]\n",
    "        readata_i1_near = readata_stn[:, nearstn_loci1, :]\n",
    "        maxd = np.max([np.max(nearstn_disti1) + 1, 100])\n",
    "        nearstn_weighti1 = au.distanceweight(nearstn_disti1, maxd, 3)\n",
    "        nearstn_weighti1 = nearstn_weighti1 / np.sum(nearstn_weighti1)\n",
    "\n",
    "        # get corrected data at i1\n",
    "        corrmode = 'QM'\n",
    "        corrdata_i1 = np.zeros([ntimes, reanum])\n",
    "        for r in range(reanum):\n",
    "            corrdata_i1[:, r] = error_correction_new(corrmode, stndata_i1_near, nearstn_weighti1,\n",
    "                                                     readata_stn_i1[r, :], readata_i1_near[r, :, :], ecdf_prob)\n",
    "        reacorr_qm[:, i1, :] = corrdata_i1.T\n",
    "\n",
    "        corrmode = 'Mul_Climo'\n",
    "        corrdata_i1 = np.zeros([ntimes, reanum])\n",
    "        for r in range(reanum):\n",
    "            corrdata_i1[:, r] = error_correction_new(corrmode, stndata_i1_near, nearstn_weighti1,\n",
    "                                                     readata_stn_i1[r, :], readata_i1_near[r, :, :], ecdf_prob)\n",
    "        reacorr_mulclimo[:, i1, :] = corrdata_i1.T\n",
    "\n",
    "    return reacorr_qm, reacorr_mulclimo\n",
    "\n",
    "def correct_merge(stndata, readata_raw, readata_stn, reacorr_stn, reamerge_stn, reamerge_weight_stn, neargrid_loc,\n",
    "         neargrid_dist, merge_choice, mask, hwsize, corrmode, anombound, var, weightmode):\n",
    "\n",
    "    nrows, ncols, nearnum = np.shape(neargrid_loc)\n",
    "    reanum, nstn, nday = np.shape(readata_stn)\n",
    "\n",
    "    neargrid_loc = neargrid_loc.copy()\n",
    "    neargrid_dist = neargrid_dist.copy()\n",
    "    mask2 = np.tile(mask[:,:,np.newaxis], (1,1,nearnum))\n",
    "    neargrid_loc[mask2 != 1] = -1\n",
    "    neargrid_dist[mask2 != 1] = np.nan\n",
    "    del mask2\n",
    "\n",
    "\n",
    "    # correct raw gridded reanalysis data using all stations\n",
    "    corr_data = np.nan * np.zeros([reanum, nrows, ncols, nday], dtype=np.float32)\n",
    "    for rr in range(reanum):\n",
    "        # calculate correction ratio at all station point\n",
    "        anom_ori = calculate_anomaly(readata_stn[rr, :, :], stndata[:, :],\n",
    "                                     hwsize, corrmode, upbound=anombound[1], lowbound=anombound[0])\n",
    "        anom_ext = extrapolation(anom_ori, neargrid_loc, neargrid_dist)\n",
    "        corr_data[rr, :, :, :] = error_correction(readata_raw[rr,:,:,:], anom_ext, mode=corrmode)\n",
    "\n",
    "    # first error estimation\n",
    "    corr_error = np.nan * np.zeros([reanum, nrows, ncols, nday], dtype=np.float32)\n",
    "    for rr in range(reanum):\n",
    "        corr_error[rr, :, :, :] = extrapolation(reacorr_stn[rr, :, :] - stndata, neargrid_loc, neargrid_dist)\n",
    "    merge_error0 = extrapolation(reamerge_stn - stndata, neargrid_loc, neargrid_dist)\n",
    "\n",
    "    if var == 'prcp' and weightmode == 'BMA':\n",
    "        corr_data = box_cox_transform(corr_data)\n",
    "        stndata = box_cox_transform(stndata)\n",
    "        reamerge_stn = box_cox_transform(reamerge_stn)\n",
    "        reacorr_stn = box_cox_transform(reacorr_stn)\n",
    "\n",
    "        # correction error in normal space (box-cox)\n",
    "        corr_error_bc = np.nan * np.zeros([reanum, nrows, ncols, nday], dtype=np.float32)\n",
    "        for rr in range(reanum):\n",
    "            corr_error_bc[rr, :, :, :] = extrapolation(reacorr_stn[rr, :, :] - stndata, neargrid_loc, neargrid_dist)\n",
    "        merge_error0_bc = extrapolation(reamerge_stn - stndata, neargrid_loc, neargrid_dist)\n",
    "\n",
    "    # merge reanalysis data\n",
    "    merge_data = np.nan * np.zeros([nrows, ncols, nday], dtype=np.float32)\n",
    "    weight_grid = extrapolation(reamerge_weight_stn, neargrid_loc, neargrid_dist)\n",
    "    for i in range(nday):\n",
    "        datain = np.zeros([nrows, ncols, reanum])\n",
    "        for rr in range(reanum):\n",
    "            datain[:, :, rr] = corr_data[rr, :, :, i]\n",
    "        merge_data[:, :, i] = weightmerge(datain, weight_grid)\n",
    "\n",
    "    # calculate the error of merged data (this is actually independent with merged data estimation)\n",
    "    merge_error = np.nan * np.zeros([nrows, ncols, nday], dtype=np.float32)\n",
    "    for r in range(nrows):\n",
    "        for c in range(ncols):\n",
    "            if not np.isnan(mask[r, c]):\n",
    "                chi = merge_choice[r, c]\n",
    "                if chi > 0:\n",
    "                    merge_error[r, c, :] = corr_error[chi - 1, r, c, :]\n",
    "                else:\n",
    "                    merge_error[r, c, :] = merge_error0[r, c, :]\n",
    "\n",
    "    if var == 'prcp' and weightmode == 'BMA':\n",
    "        merge_error_bc = np.nan * np.zeros([nrows, ncols, nday], dtype=np.float32)\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if not np.isnan(mask[r, c]):\n",
    "                    chi = merge_choice[r, c]\n",
    "                    if chi > 0:\n",
    "                        merge_error_bc[r, c, :] = corr_error_bc[chi - 1, r, c, :]\n",
    "                    else:\n",
    "                        merge_error_bc[r, c, :] = merge_error0_bc[r, c, :]\n",
    "\n",
    "    if var == 'prcp' and weightmode == 'BMA':\n",
    "        merge_error_out = [''] * 2\n",
    "        merge_error_out[0] = merge_error\n",
    "        merge_error_out[1] = merge_error_bc\n",
    "        merge_data = box_cox_recover(merge_data)\n",
    "        corr_data = box_cox_recover(corr_data)\n",
    "    else:\n",
    "        merge_error_out = ['']\n",
    "        merge_error_out[0] = merge_error\n",
    "    return corr_data, corr_error, merge_data, merge_error_out\n",
    "\n",
    "\n",
    "def mse_error(stndata, reacorr_stn, reamerge_stn, neargrid_loc, neargrid_dist, merge_choice, mask, var, pcptrans = False):\n",
    "\n",
    "    nrows, ncols, nearnum = np.shape(neargrid_loc)\n",
    "    reanum, nstn, nday = np.shape(reacorr_stn)\n",
    "\n",
    "    neargrid_loc = neargrid_loc.copy()\n",
    "    neargrid_dist = neargrid_dist.copy()\n",
    "    mask2 = np.tile(mask[:,:,np.newaxis], (1,1,nearnum))\n",
    "    neargrid_loc[mask2 != 1] = -1\n",
    "    neargrid_dist[mask2 != 1] = np.nan\n",
    "    del mask2\n",
    "\n",
    "    if var == 'prcp' and pcptrans == True:\n",
    "        stndata = box_cox_transform(stndata)\n",
    "        reamerge_stn = box_cox_transform(reamerge_stn)\n",
    "        reacorr_stn = box_cox_transform(reacorr_stn)\n",
    "\n",
    "    corr_error = np.nan * np.zeros([reanum, nrows, ncols, nday], dtype=np.float32)\n",
    "    for rr in range(reanum):\n",
    "        corr_error[rr, :, :, :] = extrapolation((reacorr_stn[rr, :, :] - stndata) ** 2, neargrid_loc, neargrid_dist)\n",
    "    corr_error = corr_error ** 0.5\n",
    "    merge_error0 = extrapolation((reamerge_stn - stndata) ** 2, neargrid_loc, neargrid_dist)\n",
    "    merge_error0 = merge_error0 ** 0.5\n",
    "\n",
    "    mse_error = np.nan * np.zeros([nrows, ncols, nday], dtype=np.float32)\n",
    "    for r in range(nrows):\n",
    "        for c in range(ncols):\n",
    "            if not np.isnan(mask[r, c]):\n",
    "                chi = merge_choice[r, c]\n",
    "                if chi > 0:\n",
    "                    mse_error[r, c, :] = corr_error[chi - 1, r, c, :]\n",
    "                else:\n",
    "                    mse_error[r, c, :] = merge_error0[r, c, :]\n",
    "\n",
    "    return mse_error\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "var='prcp'\n",
    "weightmode = 'BMA'\n",
    "y1=2000\n",
    "y2=2000\n",
    "year = [y1,y2]\n",
    "print('var is ',var)\n",
    "print('weightmode is ',weightmode)\n",
    "print('years are ',y1,y2)\n",
    "\n",
    "########################################################################################################################\n",
    "# var = 'prcp'  # ['prcp', 'tmean', 'trange']: this should be input from sbtach script\n",
    "# weightmode = 'RMSE'  # (CC, RMSE, BMA). Weight = CC**2, or Weight = 1/RMSE**2, or Weight = BMA\n",
    "# year = [1979, 2018]  # year range for merging. note weight is calculated using all data not limited by year\n",
    "# basic settings\n",
    "lontar = np.arange(-180 + 0.05, -50, 0.1)\n",
    "lattar = np.arange(85 - 0.05, 5, -0.1)\n",
    "hwsize = 0  # define time window (2*hwsize+1) used to calculate ratio (as ratio for a specific day is too variable)\n",
    "nearnum = 8  # the number of nearby stations used to extrapolate points to grids (for correction and merging)\n",
    "dividenum = 10  # divide the datasets into X parts, e.g. 10-fold cross-validation\n",
    "anombound = [0.2, 5]  # upper and lower bound when calculating the anomaly for correction\n",
    "binprob = 500\n",
    "\n",
    "# input files\n",
    "# station list and data\n",
    "# gmet_stnfile = '/home/gut428/GMET/eCAI_EMDNA/StnGridInfo/stnlist_whole.txt'\n",
    "# gmet_stndatafile = '/home/gut428/stndata_whole.npz'\n",
    "gmet_stnfile = '/Users/localuser/GMET/pyGMET_NA/stnlist_whole.txt'\n",
    "gmet_stndatafile = '/Users/localuser/GMET/pyGMET_NA/stndata_whole.npz'  # to be saved. only process when absent\n",
    "\n",
    "# mask file\n",
    "# file_mask = '/datastore/GLOBALWATER/CommonData/EMDNA/DEM/NA_DEM_010deg_trim.mat'\n",
    "file_mask = './DEM/NA_DEM_010deg_trim.mat'\n",
    "\n",
    "# downscaled reanalysis: gridded data\n",
    "prefix = ['ERA5_', 'MERRA2_', 'JRA55_']\n",
    "# downscaled reanalysis data at station points\n",
    "# path_readowngrid = ['/datastore/GLOBALWATER/CommonData/EMDNA/ERA5_day_ds',\n",
    "#                    '/datastore/GLOBALWATER/CommonData/EMDNA/MERRA2_day_ds',\n",
    "#                    '/datastore/GLOBALWATER/CommonData/EMDNA/JRA55_day_ds']\n",
    "# file_readownstn = ['/datastore/GLOBALWATER/CommonData/EMDNA/ERA5_day_ds/ERA5_downto_stn.npz',\n",
    "#                    '/datastore/GLOBALWATER/CommonData/EMDNA/MERRA2_day_ds/MERRA2_downto_stn.npz',\n",
    "#                    '/datastore/GLOBALWATER/CommonData/EMDNA/JRA55_day_ds/JRA55_downto_stn.npz']\n",
    "path_readowngrid = ['/Users/localuser/Research/Test',\n",
    "                   '/Users/localuser/Research/Test',\n",
    "                   '/Users/localuser/Research/Test']\n",
    "file_readownstn = ['/Users/localuser/Research/Test/ERA5_downto_stn_nearest.npz',\n",
    "                   '/Users/localuser/Research/Test/MERRA2_downto_stn_nearest.npz',\n",
    "                   '/Users/localuser/Research/Test/JRA55_downto_stn_nearest.npz']\n",
    "\n",
    "# output files\n",
    "\n",
    "# near stations\n",
    "near_path = '/Users/localuser/Research/Test'\n",
    "near_file_GMET = '/Users/localuser/GMET/pyGMET_NA/weight_nearstn.npz'\n",
    "# near_path = '/home/gut428/ReanalysisCorrMerge'\n",
    "# near_file_GMET = '/datastore/GLOBALWATER/CommonData/EMDNA/PyGMETout/weight.npz'\n",
    "useGMET = True\n",
    "\n",
    "# error and merging at station level\n",
    "path_reastn_cv = '/Users/localuser/Research/Test'\n",
    "# path_reastn_cv = '/datastore/GLOBALWATER/CommonData/EMDNA/ReanalysisCorrMerge/CrossValidate_2layer'\n",
    "# path_reastn_cv = '/home/gut428/ReanalysisCorrMerge/CrossValidate_2layer'\n",
    "file_corrmerge_stn = path_reastn_cv + '/mergecorr_' + var + '_' + weightmode + '-new.npz'\n",
    "\n",
    "# output corrected and merged data\n",
    "path_reacorr = '/Users/localuser/Research/Test'\n",
    "path_merge = '/Users/localuser/Research/Test'\n",
    "# path_reacorr = '/home/gut428/ReanalysisCorrMerge/Reanalysis_corr'\n",
    "# path_merge = '/home/gut428/ReanalysisCorrMerge/Reanalysis_merge'\n",
    "file_mergechoice = path_merge + '/mergechoice_' + var + '_' +  weightmode + '.npz'\n",
    "\n",
    "# file_ecdf = '/home/gut428/ReanalysisCorrMerge/stn_ecdf_' + var + '.npz'\n",
    "file_ecdf = '/Users/localuser/Research/Test/stn_ecdf_' + var + '.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start basic processing\n"
     ]
    }
   ],
   "source": [
    "# basic processing\n",
    "print('start basic processing')\n",
    "# decide correction mode according to variables\n",
    "# corrmode: QM, Mul_Climo, Mul_Daily, Add_Climo, Add_Climo\n",
    "# Mul_ has default upper limit of 10\n",
    "if var == 'prcp':\n",
    "    corrmode = 'Mul_Climo'\n",
    "elif var == 'tmean' or var == 'trange':\n",
    "    corrmode = 'Mul_Climo'\n",
    "else:\n",
    "    sys.exit('Unknown correction mode')\n",
    "\n",
    "# mask\n",
    "mask = io.loadmat(file_mask)\n",
    "mask = mask['DEM']\n",
    "mask[~np.isnan(mask)] = 1  # 1: valid pixels\n",
    "# attributes\n",
    "reanum = len(file_readownstn)\n",
    "nrows, ncols = np.shape(mask)\n",
    "lontarm, lattarm = np.meshgrid(lontar, lattar)\n",
    "lontarm[np.isnan(mask)] = np.nan\n",
    "lattarm[np.isnan(mask)] = np.nan\n",
    "\n",
    "# date\n",
    "date_list, date_number = m_DateList(1979, 2018, 'ByYear')\n",
    "\n",
    "# load observations for all stations\n",
    "datatemp = np.load(gmet_stndatafile)\n",
    "stndata = datatemp[var + '_stn']\n",
    "stnlle = datatemp['stn_lle']\n",
    "nstn, ntimes = np.shape(stndata)\n",
    "del datatemp\n",
    "\n",
    "# probability bins for QM\n",
    "binprob = 500\n",
    "prob_stn = np.arange(0, 1 + 1 / binprob, 1 / binprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load near station information for points\n"
     ]
    }
   ],
   "source": [
    "# find near stations for all grids and station\n",
    "if var == 'trange':\n",
    "    vari = 'tmean'  # trange and tmean have the same index\n",
    "else:\n",
    "    vari = var\n",
    "\n",
    "near_stnfile = near_path + '/near_stn_' + vari + '.npz'\n",
    "near_gridfile = near_path + '/near_grid_' + vari + '.npz'\n",
    "\n",
    "if os.path.isfile(near_stnfile):\n",
    "    print('load near station information for points')\n",
    "    with np.load(near_stnfile) as datatemp:\n",
    "        nearstn_loc = datatemp['nearstn_loc']\n",
    "        nearstn_dist = datatemp['nearstn_dist']\n",
    "    del datatemp\n",
    "else:\n",
    "    print('find near stations for station points')\n",
    "    stnllein = stnlle.copy()\n",
    "    stnllein[np.isnan(stndata[:, 0]),:] = np.nan\n",
    "    nearstn_loc, nearstn_dist = findnearstn\\\n",
    "        (stnllein[:, 0], stnllein[:, 1], stnllein[:, 0], stnllein[:, 1], nearnum, 1)\n",
    "    np.savez_compressed(near_stnfile, nearstn_loc=nearstn_loc, nearstn_dist=nearstn_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load downscaled reanalysis data at station points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in less\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "# load downscaled reanalysis for all stations\n",
    "print('load downscaled reanalysis data at station points')\n",
    "readata_stn = np.nan * np.zeros([reanum, nstn, ntimes], dtype=np.float32)\n",
    "for rr in range(reanum):\n",
    "    dr = np.load(file_readownstn[rr])\n",
    "    temp = dr[var + '_readown']\n",
    "#     if prefix[rr] == 'MERRA2_':  # unify the time length of all data as MERRA2 lacks 1979\n",
    "#         add = np.nan * np.zeros([nstn, 365])\n",
    "#         temp = np.concatenate((add, temp), axis=1)\n",
    "    readata_stn[rr, :, :] = temp\n",
    "    del dr, temp\n",
    "if var == 'prcp':\n",
    "    readata_stn[readata_stn<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year 1979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:135: RuntimeWarning: Mean of empty slice\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:56: RuntimeWarning: Mean of empty slice\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:68: RuntimeWarning: invalid value encountered in greater\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:69: RuntimeWarning: invalid value encountered in less\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:147: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year 1980\n",
      "year 1981\n",
      "year 1982\n",
      "year 1983\n",
      "year 1984\n",
      "year 1985\n",
      "year 1986\n",
      "year 1987\n",
      "year 1988\n",
      "year 1989\n",
      "year 1990\n",
      "year 1991\n",
      "year 1992\n",
      "year 1993\n",
      "year 1994\n",
      "year 1995\n",
      "year 1996\n",
      "year 1997\n",
      "year 1998\n",
      "year 1999\n",
      "year 2000\n",
      "year 2001\n",
      "year 2002\n",
      "year 2003\n",
      "year 2004\n",
      "year 2005\n",
      "year 2006\n",
      "year 2007\n",
      "year 2008\n",
      "year 2009\n",
      "year 2010\n",
      "year 2011\n",
      "year 2012\n",
      "year 2013\n",
      "year 2014\n",
      "year 2015\n",
      "year 2016\n",
      "year 2017\n",
      "year 2018\n"
     ]
    }
   ],
   "source": [
    "def correction_comparison(stndata, ecdf_prob, readata_stn, nearstn_loc, nearstn_dist):\n",
    "    # compare the performance of daily-scale multiplicative correction and QM\n",
    "    reanum, nstn, ntimes = np.shape(readata_stn)\n",
    "    nprob = len(ecdf_prob)\n",
    "\n",
    "    # initialization\n",
    "    reacorr_qm = np.nan * np.zeros([reanum, nstn, ntimes], dtype=np.float32)  # corrected reanalysis data\n",
    "    reacorr_mulclimo = np.nan * np.zeros([reanum, nstn, ntimes], dtype=np.float32)  # corrected reanalysis data\n",
    "\n",
    "    for i1 in range(0, 5): # layer-1\n",
    "#         if np.mod(i1, 5000) == 0:\n",
    "#             continue\n",
    "#             print(i1)\n",
    "        \n",
    "        if np.isnan(stndata[i1, 0]):\n",
    "            continue\n",
    "\n",
    "        nearstn_loci1 = nearstn_loc[i1, :]\n",
    "        nearstn_disti1 = nearstn_dist[i1, :]\n",
    "        induse = nearstn_loci1 > -1\n",
    "        nearstn_loci1 = nearstn_loci1[induse]\n",
    "        nearstn_disti1 = nearstn_disti1[induse]\n",
    "        nearstn_numi1 = len(nearstn_loci1)\n",
    "        if nearstn_numi1 == 0:\n",
    "            sys.exit('No near station for the target station (layer-1)')\n",
    "\n",
    "        stndata_i1_near = stndata[nearstn_loci1, :]\n",
    "        readata_stn_i1 = readata_stn[:, i1, :]\n",
    "        readata_i1_near = readata_stn[:, nearstn_loci1, :]\n",
    "        maxd = np.max([np.max(nearstn_disti1) + 1, 100])\n",
    "        nearstn_weighti1 = au.distanceweight(nearstn_disti1, maxd, 3)\n",
    "        nearstn_weighti1 = nearstn_weighti1 / np.sum(nearstn_weighti1)\n",
    "\n",
    "        # get corrected data at i1\n",
    "#         corrmode = 'QM'\n",
    "#         corrdata_i1 = np.zeros([ntimes, reanum])\n",
    "#         for r in range(reanum):\n",
    "#             corrdata_i1[:, r] = error_correction_new(corrmode, stndata_i1_near, nearstn_weighti1,\n",
    "#                                                      readata_stn_i1[r, :], readata_i1_near[r, :, :], ecdf_prob)\n",
    "#         reacorr_qm[:, i1, :] = corrdata_i1.T\n",
    "\n",
    "        corrmode = 'Mul_Climo'\n",
    "        corrdata_i1 = np.zeros([ntimes, reanum])\n",
    "        for r in range(reanum):\n",
    "            corrdata_i1[:, r] = error_correction_new(corrmode, stndata_i1_near, nearstn_weighti1,\n",
    "                                                     readata_stn_i1[r, :], readata_i1_near[r, :, :], ecdf_prob)\n",
    "        reacorr_mulclimo[:, i1, :] = corrdata_i1.T\n",
    "\n",
    "    return reacorr_qm, reacorr_mulclimo\n",
    "\n",
    "\n",
    "reacorr_qm = np.nan * np.zeros([reanum, nstn, ntimes], dtype=np.float32)\n",
    "reacorr_mul = np.nan * np.zeros([reanum, nstn, ntimes], dtype=np.float32)\n",
    "# for y in range(1979,2019):\n",
    "#     print('year', y)\n",
    "#     for m in range(12):\n",
    "# #         print('year / month', y, m+1)\n",
    "# #         indm = (date_number['mm'] == (m + 1) )\n",
    "#         indm = (date_number['mm'] == (m + 1) ) & (date_number['yyyy'] == y)\n",
    "#         qmcorr, mulcorr = correction_comparison(stndata[:, indm], prob_stn, readata_stn[:,:,indm], nearstn_loc, nearstn_dist)\n",
    "#         reacorr_qm[:,:,indm] = qmcorr\n",
    "#         reacorr_mul[:,:,indm] = mulcorr\n",
    "\n",
    "\n",
    "for m in range(12):\n",
    "    print('month', m+1)\n",
    "    indm = (date_number['mm'] == (m + 1) )\n",
    "    qmcorr, mulcorr = correction_comparison(stndata[:, indm], prob_stn, readata_stn[:,:,indm], nearstn_loc, nearstn_dist)\n",
    "    reacorr_qm[:,:,indm] = qmcorr\n",
    "    reacorr_mul[:,:,indm] = mulcorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.51385966 0.39922881 2.66656375 5.86798717]\n",
      "[nan nan nan nan]\n",
      "[ 0.55384149 -0.08389875  2.40637255  5.29945942]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py:390: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/usr/local/lib/python3.7/site-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "/usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py:2526: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar)\n",
      "/usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py:2455: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "/usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py:2455: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n",
      "/Users/localuser/Github/PyGMET/auxiliary.py:150: RuntimeWarning: Mean of empty slice\n",
      "  metout[1] = np.nanmean(pre - obs)  # ME\n",
      "/Users/localuser/Github/PyGMET/auxiliary.py:151: RuntimeWarning: Mean of empty slice\n",
      "  metout[2] = np.nanmean(np.abs(pre - obs))  # MAE\n",
      "/Users/localuser/Github/PyGMET/auxiliary.py:152: RuntimeWarning: invalid value encountered in true_divide\n",
      "  metout[3] = np.sqrt(np.sum(np.square(obs - pre)) / len(obs))  # RMSE\n",
      "/usr/local/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1115: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input=overwrite_input)\n"
     ]
    }
   ],
   "source": [
    "met = np.nan * np.zeros([3, nstn, 4])\n",
    "for i in range(5):\n",
    "#     if not np.mod(i,1000) ==0:\n",
    "#         continue\n",
    "    met[0,i,:]=au.metric(stndata[i,:],readata_stn[0,i,:])\n",
    "    met[1,i,:]=au.metric(stndata[i,:],reacorr_qm[0,i,:])\n",
    "    met[2,i,:]=au.metric(stndata[i,:],reacorr_mul[0,i,:])\n",
    "met = met[:,~np.isnan(met[0,:,0]),:]\n",
    "for i in range(3):\n",
    "    print(np.nanmedian(met[i,:,:],axis=0))\n",
    "#     print('###############################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.50331099 0.57778102 2.65009689 5.84378128]\n",
      "[nan nan nan nan]\n",
      "[0.54755018 0.20999882 2.38430238 5.27478951]\n",
      "$$$\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(met[i,3,:])\n",
    "print('$$$')\n",
    "# for i in range(3):\n",
    "#     print(met_old[i,1230,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.54724364 0.23155205 2.44621015 6.12922632]\n",
      "[ 5.49905805e-01 -6.10473612e-03  2.31197810e+00  6.48442594e+00]\n",
      "[ 0.55582552 -0.01320286  2.26363158  5.83260256]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(np.nanmedian(met_old[i,:,:],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47636569 0.72937173 2.77754593 5.86798717]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d088460489fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmet2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "datatemp = np.load('metcorr.npz')\n",
    "met2 = datatemp['arr_0']\n",
    "for i in range(5):\n",
    "    print(met[0,i,:])\n",
    "    print(met2[0,i,:])\n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(met2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
