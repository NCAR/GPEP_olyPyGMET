{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start basic processing\n",
      "load near station information for points\n",
      "load near station information for grids\n",
      "load downscaled reanalysis data at station points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:741: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load independent merged/corrected data at station points\n",
      "load merge choice file\n",
      "Correction and Merge: year 2000\n",
      "Correction and Merge: month 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:94: RuntimeWarning: Mean of empty slice\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:95: RuntimeWarning: Mean of empty slice\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:98: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:98: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:106: RuntimeWarning: invalid value encountered in greater\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:107: RuntimeWarning: invalid value encountered in less\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:284: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correction and Merge: month 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-190-5c23dbf0745e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0mcorr_datam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorr_errorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_datam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_errorm\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m             correct_merge(stndata[:, indym], readata_raw[:,:,:,indm], readata_stn[:, :, indym], reacorr_stn[:, :, indym],\n\u001b[0m\u001b[1;32m    876\u001b[0m                           \u001b[0mreamerge_stn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindym\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreamerge_weight_stn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneargrid_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneargrid_dist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m                           merge_choice[m, :, :], mask, hwsize, corrmode, anombound, var, weightmode)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import auxiliary as au\n",
    "import regression as reg\n",
    "import datetime as dt\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import io\n",
    "from scipy.interpolate import interp2d\n",
    "import os\n",
    "import sys\n",
    "from scipy.interpolate import griddata\n",
    "import h5py\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "from bma_merge import bma\n",
    "\n",
    "\n",
    "def divide_train_test(data, dividenum, randseed=-1):\n",
    "    if randseed == -1:\n",
    "        random.seed(time.time())\n",
    "    num = len(data)\n",
    "    subnum = int(num / dividenum)\n",
    "    data_train = np.zeros([dividenum, num - subnum], dtype=int)\n",
    "    data_test = np.zeros([dividenum, subnum], dtype=int)\n",
    "    randindex = random.sample(range(num), num)\n",
    "    for i in range(dividenum):\n",
    "        data_test[i, :] = np.sort(data[randindex[i * subnum:(i + 1) * subnum]])\n",
    "        data_train[i, :] = np.setdiff1d(data, data_test[i])\n",
    "    return data_train, data_test\n",
    "\n",
    "\n",
    "def double_cvindex(gmet_stndatafile, dividenum, rndseed=123):\n",
    "    # index for double cross-validation\n",
    "    datatemp = np.load(gmet_stndatafile)\n",
    "    prcp_stn0 = datatemp['prcp_stn'][:, 0]\n",
    "    tmean_stn0 = datatemp['tmean_stn'][:, 0]\n",
    "    prcp_stnindex = np.argwhere(~np.isnan(prcp_stn0))\n",
    "    prcp_stnindex = prcp_stnindex.flatten()\n",
    "    tmean_stnindex = np.argwhere(~np.isnan(tmean_stn0))\n",
    "    tmean_stnindex = tmean_stnindex.flatten()\n",
    "\n",
    "    subnum1 = int(len(prcp_stnindex) / dividenum)\n",
    "    subnum2 = int((len(prcp_stnindex) - subnum1) / dividenum)\n",
    "    # prcp_testindex1 = np.zeros([dividenum,subnum1])\n",
    "    # prcp_trainindex1 = np.zeros([dividenum,len(prcp_stnindex) - subnum1])\n",
    "    prcp_trainindex1, prcp_testindex1 = divide_train_test(prcp_stnindex, dividenum, randseed=rndseed)\n",
    "    prcp_testindex2 = np.zeros([dividenum, dividenum, subnum2], dtype=int)\n",
    "    prcp_trainindex2 = np.zeros([dividenum, dividenum, len(prcp_stnindex) - subnum1 - subnum2], dtype=int)\n",
    "    for i in range(dividenum):\n",
    "        traini, testi = divide_train_test(prcp_trainindex1[i, :], dividenum, randseed=rndseed)\n",
    "        prcp_trainindex2[i, :, :] = traini\n",
    "        prcp_testindex2[i, :, :] = testi\n",
    "\n",
    "    subnum1 = int(len(tmean_stnindex) / dividenum)\n",
    "    subnum2 = int((len(tmean_stnindex) - subnum1) / dividenum)\n",
    "    # tmean_testindex1 = np.zeros([dividenum,subnum1])\n",
    "    # tmean_trainindex1 = np.zeros([dividenum,len(tmean_stnindex) - subnum1])\n",
    "    tmean_trainindex1, tmean_testindex1 = divide_train_test(tmean_stnindex, dividenum, randseed=rndseed)\n",
    "    tmean_testindex2 = np.zeros([dividenum, dividenum, subnum2], dtype=int)\n",
    "    tmean_trainindex2 = np.zeros([dividenum, dividenum, len(tmean_stnindex) - subnum1 - subnum2], dtype=int)\n",
    "    for i in range(dividenum):\n",
    "        traini, testi = divide_train_test(tmean_trainindex1[i, :], dividenum, randseed=rndseed)\n",
    "        tmean_trainindex2[i, :, :] = traini\n",
    "        tmean_testindex2[i, :, :] = testi\n",
    "    return prcp_trainindex1, prcp_testindex1, prcp_trainindex2, prcp_testindex2, \\\n",
    "           tmean_trainindex1, tmean_testindex1, tmean_trainindex2, tmean_testindex2\n",
    "\n",
    "\n",
    "def calculate_anomaly(datatar, dataref, hwsize, amode, upbound=5, lowbound=0.2):\n",
    "    # datatar, dataref: 2D [nstn, ntime]\n",
    "    # amode: anomaly mode ('ratio' or 'diff')\n",
    "    # hwsize: define time window (2*hwsize+1) used to calculate ratio (as ratio for a specific day is too variable)\n",
    "    # upbound/lowbound: upper and lower limitation of ratio/difference\n",
    "    if np.ndim(datatar) == 1:  # only one time step\n",
    "        datatar = datatar[:, np.newaxis]\n",
    "        dataref = dataref[:, np.newaxis]\n",
    "\n",
    "    nstn, ntime = np.shape(datatar)\n",
    "    if ntime < hwsize * 2 + 1:\n",
    "        print('The window size is larger than time steps when calculating ratio between tar and ref datasets')\n",
    "        print('Please set a smaller hwsize')\n",
    "        sys.exit()\n",
    "\n",
    "    anom = np.ones([nstn, ntime])\n",
    "\n",
    "    for i in range(ntime):\n",
    "        if i < hwsize:\n",
    "            windex = np.arange(hwsize * 2 + 1)\n",
    "        elif i >= ntime - hwsize:\n",
    "            windex = np.arange(ntime - hwsize * 2 - 1, ntime)\n",
    "        else:\n",
    "            windex = np.arange(i - hwsize, i + hwsize + 1)\n",
    "        dtari = np.nanmean(datatar[:, windex], axis=1)\n",
    "        drefi = np.nanmean(dataref[:, windex], axis=1)\n",
    "\n",
    "        if amode == 'ratio':\n",
    "            temp = drefi / dtari\n",
    "            temp[(dtari == 0) & (drefi == 0)] = 1\n",
    "            anom[:, i] = temp\n",
    "        elif amode == 'diff':\n",
    "            anom[:, i] = drefi - dtari\n",
    "        else:\n",
    "            sys.exit('Unknow amode. Please use either ratio or diff')\n",
    "\n",
    "    anom[anom > upbound] = upbound\n",
    "    anom[anom < lowbound] = lowbound\n",
    "    return anom\n",
    "\n",
    "\n",
    "def extrapolation(datain, nearstn_loc, nearstn_dist):\n",
    "    # datain: one or multiple time steps\n",
    "    wexp = 3\n",
    "    if np.ndim(datain) == 1:  # add time axis\n",
    "        datain = datain[:, np.newaxis]\n",
    "\n",
    "    if np.ndim(nearstn_loc) == 2:  # extrapolate to station points\n",
    "        num = np.shape(nearstn_loc)[0]\n",
    "        ntimes = np.shape(datain)[1]\n",
    "        dataout = np.nan * np.zeros([num, ntimes], dtype=np.float32)\n",
    "        for i in range(num):\n",
    "            if not nearstn_loc[i, 0]>=0:\n",
    "                continue\n",
    "            nearloci = nearstn_loc[i, :]\n",
    "            indloci = nearloci > -1\n",
    "            dataini = datain[nearloci[indloci], :]\n",
    "            disti = nearstn_dist[i, indloci]\n",
    "            weighti = au.distanceweight(disti, np.max(disti) + 1, wexp)\n",
    "            weighti[np.isnan(dataini[:,0])] = np.nan\n",
    "            weighti = weighti / np.nansum(weighti)\n",
    "            weighti2 = np.tile(weighti,[ntimes,1]).T\n",
    "            dataout[i, :] = np.nansum(dataini * weighti2, axis=0)\n",
    "    elif np.ndim(nearstn_loc) == 3:  # extrapolate to gridds\n",
    "        nrows, ncols, nearnum = np.shape(nearstn_loc)\n",
    "        nstn, ntimes = np.shape(datain)\n",
    "        dataout = np.nan * np.zeros([nrows, ncols, ntimes], dtype=np.float32)\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if not nearstn_loc[r, c, 0]>=0:\n",
    "                    continue\n",
    "                nearloci = nearstn_loc[r, c, :]\n",
    "                indloci = nearloci > -1\n",
    "                dataini = datain[nearloci[indloci], :]\n",
    "                disti = nearstn_dist[r, c, indloci]\n",
    "                weighti = au.distanceweight(disti, np.max(disti) + 1, wexp)\n",
    "                weighti[np.isnan(dataini[:,0])]=np.nan\n",
    "                weighti = weighti / np.nansum(weighti)\n",
    "                weighti2 = np.tile(weighti, [ntimes, 1]).T\n",
    "                dataout[r, c, :] = np.nansum(dataini * weighti2, axis=0)\n",
    "    else:\n",
    "        print('The dimensions of tarlat or tarlon are larger than 2')\n",
    "        sys.exit()\n",
    "    if ntimes == 1:\n",
    "        dataout = np.squeeze(dataout)\n",
    "    return dataout\n",
    "\n",
    "\n",
    "def findnearstn(stnlat, stnlon, tarlat, tarlon, nearnum, noself):\n",
    "    # only use lat/lon to find near stations without considering distance in km\n",
    "    # stnlat/stnlon: 1D\n",
    "    # tarlat/tarlon: 1D or 2D\n",
    "    # noself: 1--stnlat and tarlat have overlapped stations, which should be excluded from stnlat\n",
    "\n",
    "    stnll = np.zeros([len(stnlat), 2])\n",
    "    stnll[:, 0] = stnlat\n",
    "    stnll[:, 1] = stnlon\n",
    "\n",
    "    if len(np.shape(tarlat)) == 1:\n",
    "        num = len(tarlat)\n",
    "        nearstn_loc = -1 * np.ones([num, nearnum], dtype=int)\n",
    "        nearstn_dist = -1 * np.ones([num, nearnum], dtype=float)\n",
    "        for i in range(num):\n",
    "            if np.isnan(tarlat[i]) or np.isnan(tarlon[i]):\n",
    "                continue\n",
    "            tari = np.array([tarlat[i], tarlon[i]])\n",
    "            dist = au.distance(tari, stnll)\n",
    "            dist[np.isnan(dist)] = 1000000000\n",
    "            if noself == 1:\n",
    "                dist[dist == 0] = np.inf  # may not be perfect, but work for SCDNA\n",
    "            indi = np.argsort(dist)\n",
    "            nearstn_loc[i, :] = indi[0:nearnum]\n",
    "            nearstn_dist[i, :] = dist[nearstn_loc[i, :]]\n",
    "    elif len(np.shape(tarlat)) == 2:\n",
    "        nrows, ncols = np.shape(tarlat)\n",
    "        nearstn_loc = -1 * np.ones([nrows, ncols, nearnum], dtype=int)\n",
    "        nearstn_dist = -1 * np.ones([nrows, ncols, nearnum], dtype=float)\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if np.isnan(tarlat[r, c]) or np.isnan(tarlon[r, c]):\n",
    "                    continue\n",
    "                tari = np.array([tarlat[r, c], tarlon[r, c]])\n",
    "                dist = au.distance(tari, stnll)\n",
    "                dist[np.isnan(dist)] = 1000000000\n",
    "                indi = np.argsort(dist)\n",
    "                nearstn_loc[r, c, :] = indi[0:nearnum]\n",
    "                nearstn_dist[r, c, :] = dist[nearstn_loc[r, c, :]]\n",
    "    else:\n",
    "        print('The dimensions of tarlat or tarlon are larger than 2')\n",
    "        sys.exit()\n",
    "\n",
    "    return nearstn_loc, nearstn_dist\n",
    "\n",
    "\n",
    "def error_correction(dataori, anomaly, mode='ratio'):\n",
    "    # default: time is the last dimension\n",
    "    if mode == 'ratio':\n",
    "        datacorr = dataori * anomaly\n",
    "    elif mode == 'diff':\n",
    "        datacorr = dataori + anomaly\n",
    "    else:\n",
    "        sys.exit('Wrong error correction mode')\n",
    "    return datacorr\n",
    "\n",
    "\n",
    "def calweight(obsall, preall, mode='RMSE', preprocess=True):\n",
    "    nstn, ntime = np.shape(obsall)\n",
    "    met = np.nan * np.zeros(nstn)\n",
    "    for i in range(nstn):\n",
    "        obs = obsall[i, :]\n",
    "        pre = preall[i, :]\n",
    "        if preprocess:\n",
    "            # delete the nan values\n",
    "            ind_nan = np.isnan(obs) | np.isnan(pre)\n",
    "            obs = obs[~ind_nan]\n",
    "            pre = pre[~ind_nan]\n",
    "\n",
    "        if len(obs) < 3:\n",
    "            continue\n",
    "\n",
    "        if mode == 'RMSE':\n",
    "            met[i] = np.sqrt(np.sum(np.square(obs - pre)) / len(obs))  # RMSE\n",
    "        elif mode == 'CC':\n",
    "            temp = np.corrcoef(obs, pre)\n",
    "            met[i] = temp[0][1]  # CC\n",
    "        else:\n",
    "            sys.exit('Unknown inputs for calmetric')\n",
    "\n",
    "    if mode == 'RMSE':\n",
    "        met[met==0] = 0.01\n",
    "        weight = 1 / (met ** 2)\n",
    "    elif mode == 'CC':\n",
    "        met[met < 0] = 0\n",
    "        weight = met ** 2\n",
    "    else:\n",
    "        sys.exit('Unknown inputs for calmetric')\n",
    "\n",
    "    return weight\n",
    "\n",
    "\n",
    "def calmetric(dtar, dref, metname='RMSE'):\n",
    "    if np.ndim(dtar) == 1:\n",
    "        dtar = dtar[np.newaxis, :]\n",
    "        dref = dref[np.newaxis, :]\n",
    "    nstn, ntimes = np.shape(dtar)\n",
    "    metout = np.nan * np.zeros(nstn, dtype=np.float32)\n",
    "    if metname == 'RMSE':\n",
    "        for i in range(nstn):\n",
    "            metout[i] = np.sqrt(np.nansum(np.square(dtar[i, :] - dref[i, :])) / ntimes)  # RMSE\n",
    "    elif metname == 'CC':\n",
    "        for i in range(nstn):\n",
    "            temp = np.corrcoef(dtar[i, :], dref[i, :])\n",
    "            metout[i] = temp[0, 1]\n",
    "    else:\n",
    "        sys.exit('Unkown metric name')\n",
    "    return metout\n",
    "\n",
    "\n",
    "def ismember(a, b):\n",
    "    # tf = np.in1d(a,b) # for newer versions of numpy\n",
    "    tf = np.array([i in b for i in a])\n",
    "    u = np.unique(a[tf])\n",
    "    index = np.array([(np.where(b == i))[0][-1] if t else 0 for i, t in zip(a, tf)])\n",
    "    return tf, index\n",
    "\n",
    "\n",
    "def weightmerge(data, weight):\n",
    "    if np.ndim(data) == 2:\n",
    "        weight2 = weight.copy()\n",
    "        weight2[np.isnan(data)] = np.nan\n",
    "        dataout = np.nansum(data * weight2, axis=1) / np.nansum(weight2, axis=1)\n",
    "    elif np.ndim(data) == 3:\n",
    "        weight2 = weight.copy()\n",
    "        weight2[np.isnan(data)] = np.nan\n",
    "        dataout = np.nansum(data * weight2, axis=2) / np.nansum(weight2, axis=2)\n",
    "        dataout[np.isnan(data[:,:,0])]=np.nan\n",
    "    return dataout\n",
    "\n",
    "\n",
    "def m_DateList(year_start, year_end, mode):\n",
    "    # generate a date list (yyyymmdd) between start year and end year\n",
    "    # mode: 'ByDay', 'ByMonth', 'ByYear': time scales of input files\n",
    "    date_start = datetime.date(year_start, 1, 1)\n",
    "    date_end = datetime.date(year_end, 12, 31)\n",
    "    daynum = (date_end - date_start).days + 1\n",
    "\n",
    "    # generate date in format: yyyymmdd\n",
    "    date_ymd = np.zeros(daynum, dtype=int)\n",
    "    dated = date_start\n",
    "    for d in range(daynum):\n",
    "        if d > 0:\n",
    "            dated = dated + datetime.timedelta(days=1)\n",
    "        date_ymd[d] = int(dated.strftime(\"%Y%m%d\"))\n",
    "    date_number = {'yyyymmdd': date_ymd,\n",
    "                   'yyyymm': np.floor(date_ymd / 100).astype(int),\n",
    "                   'yyyy': np.floor(date_ymd / 10000).astype(int),\n",
    "                   'mm': np.floor(np.mod(date_ymd, 10000) / 100).astype(int),\n",
    "                   'dd': np.mod(date_ymd, 100).astype(int)}\n",
    "\n",
    "    # generate file list\n",
    "    if mode == 'ByDay':\n",
    "        datemode = date_number['yyyymmdd']\n",
    "    else:\n",
    "        if mode == 'ByMonth':\n",
    "            datemode = date_number['yyyymm']\n",
    "        elif mode == 'ByYear':\n",
    "            datemode = date_number['yyyy']\n",
    "        datemode = np.unique(datemode)\n",
    "\n",
    "    date_list = [' '] * len(datemode)\n",
    "    for i in range(len(datemode)):\n",
    "        date_list[i] = str(datemode[i])\n",
    "\n",
    "    return date_list, date_number\n",
    "\n",
    "def box_cox_transform(data, exp=0.25):\n",
    "    return (data ** exp - 1) / exp\n",
    "\n",
    "def box_cox_recover(data, exp=0.25):\n",
    "    dataout = (data * exp + 1) ** (1/exp)\n",
    "    dataout[data < -1/exp] = 0\n",
    "    return dataout\n",
    "\n",
    "def merge_correction_stnerror(outpath, stnlle, stndata, readata_stn, taintestindex, nearstn_locl1, nearstn_distl1,\n",
    "         nearstn_locl2, nearstn_distl2, dividenum, var, hwsize, corrmode, anombound, weightmode):\n",
    "    # use 2-layer cross-validation to estimate the weight and independent data of merge/correction data\n",
    "    # layer-1: aim to obtain correction and merged data at station points through cross-validation\n",
    "    # layer-2: aim to obtain independent evaluation of correction data to support calculating merging weight in layer-1\n",
    "    reanum, nstn, ntimes = np.shape(readata_stn)\n",
    "\n",
    "    # initialization\n",
    "    reacorr_stn = np.nan * np.zeros([reanum, nstn, ntimes], dtype=np.float32)  # corrected reanalysis data\n",
    "    reamerge_weight_stn = np.nan * np.zeros([nstn, reanum])  # weight used to obtain reamerge_stn\n",
    "    reamerge_stn = np.nan * np.zeros([nstn, ntimes], dtype=np.float32)  # merged reanalysis at station points\n",
    "\n",
    "    for lay1 in range(dividenum):\n",
    "        print('Correction/Merging at station points. Layer-1:', lay1)\n",
    "        # extract train and test index for layer-1\n",
    "        if var == 'trange':\n",
    "            vari = 'tmean'  # trange and tmean have the same index\n",
    "        else:\n",
    "            vari = var\n",
    "        trainindex1 = taintestindex[vari + '_trainindex1'][lay1, :]\n",
    "        testindex1 = taintestindex[vari + '_testindex1'][lay1, :]\n",
    "        stndata_trainl1 = stndata[trainindex1, :]\n",
    "        stndata_testl1 = stndata[testindex1, :]\n",
    "        stnlle_trainl1 = stnlle[trainindex1, :]\n",
    "        stnlle_testl1 = stnlle[testindex1, :]\n",
    "\n",
    "        # filename: save inputs for each layer-1\n",
    "        file_reacorrl1 = outpath + '/' + var + '_layer_' + str(lay1 + 1) + '_' + weightmode + '.npz'\n",
    "        if os.path.isfile(file_reacorrl1):\n",
    "            datatemp = np.load(file_reacorrl1)\n",
    "            weight_trainl1 = datatemp['reaweight']\n",
    "            del datatemp\n",
    "        else:\n",
    "\n",
    "            # layer-2: start\n",
    "            reacorr_trainl1 = np.zeros([reanum, len(trainindex1), ntimes], dtype=np.float32)\n",
    "            weight_trainl1 = np.zeros([len(trainindex1), reanum], dtype=np.float32)\n",
    "\n",
    "            for lay2 in range(dividenum):\n",
    "                # print('Correction/Merging at station points. Layer-2:', lay2)\n",
    "                # extract train and test index for layer-2 (subsets of trainindex1)\n",
    "                trainindex2 = taintestindex[vari + '_trainindex2'][lay1, lay2, :]\n",
    "                testindex2 = taintestindex[vari + '_testindex2'][lay1, lay2, :]\n",
    "                stndata_trainl2 = stndata[trainindex2, :]\n",
    "                stndata_testl2 = stndata[testindex2, :]\n",
    "                stnlle_trainl2 = stnlle[trainindex2, :]\n",
    "                stnlle_testl2 = stnlle[testindex2, :]\n",
    "\n",
    "                for rr in range(reanum):\n",
    "                    # print('Correction/Merging at station points. Reanalysis:', rr)\n",
    "                    readata_trainl2 = readata_stn[rr, trainindex2, :]\n",
    "                    readata_testl2 = readata_stn[rr, testindex2, :]\n",
    "\n",
    "                    ### calculate corrected reanalysis data\n",
    "                    # calculate anomaly at the train stations\n",
    "                    anom_ori = calculate_anomaly(readata_trainl2, stndata_trainl2, hwsize, corrmode,\n",
    "                                                 upbound=anombound[1], lowbound=anombound[0])\n",
    "                    # extrapolate the ratio to the test stations\n",
    "                    anom_ext = extrapolation(anom_ori, nearstn_locl2[lay1,lay2,:],nearstn_distl2[lay1,lay2,:])\n",
    "                    # correct data at the test stations\n",
    "                    readata_testl2_corr = error_correction(readata_testl2, anom_ext, mode=corrmode)\n",
    "                    tf, index = ismember(testindex2, trainindex1)\n",
    "                    reacorr_trainl1[rr, index, :] = readata_testl2_corr\n",
    "\n",
    "            if weightmode == 'BMA':\n",
    "                for i in range(len(trainindex1)):\n",
    "                    dobs = stndata_trainl1[i, :]\n",
    "                    drea = reacorr_trainl1[:, i, :].T\n",
    "                    if var == 'prcp':\n",
    "                        # exclude zero precipitation and carry out box-cox transformation\n",
    "                        datatemp = np.zeros([ntimes,reanum+1])\n",
    "                        datatemp[:,0] = dobs\n",
    "                        datatemp[:,1:] = drea\n",
    "                        ind0 = np.sum(datatemp>=0.01, axis=1) == (reanum+1) # positive hit events\n",
    "                        if np.sum(ind0) < 10:\n",
    "                            weight_trainl1[i, :] = np.ones(reanum)/reanum\n",
    "                            continue\n",
    "                        else:\n",
    "                            dobs = box_cox_transform(dobs[ind0])\n",
    "                            drea = box_cox_transform(drea[ind0,:])\n",
    "                    w, sigma, sigma_s = bma(drea, dobs)\n",
    "                    weight_trainl1[i, :] = w\n",
    "            else:\n",
    "                for rr in range(reanum):\n",
    "                    weight_trainl1[:, rr] = calweight(stndata_trainl1, reacorr_trainl1[rr, :, :], weightmode)\n",
    "\n",
    "            np.savez_compressed(file_reacorrl1, reacorr=reacorr_trainl1, stnlle=stnlle_trainl1,\n",
    "                                reaweight=weight_trainl1)\n",
    "            # layer-2: end\n",
    "\n",
    "\n",
    "        # output: repeat error correction using train stations in layer-1 (as in layer-2 only 0.9*0.9=0.81 stations are used)\n",
    "        # extrapolate from train stations to test stations\n",
    "        for rr in range(reanum):\n",
    "            readata_trainl1 = readata_stn[rr, trainindex1, :]\n",
    "            readata_testl1 = readata_stn[rr, testindex1, :]\n",
    "            anom_ori = calculate_anomaly(readata_trainl1, stndata_trainl1, hwsize, corrmode,\n",
    "                                         upbound=anombound[1], lowbound=anombound[0])\n",
    "            anom_ext = extrapolation(anom_ori, nearstn_locl1[lay1,:],nearstn_distl1[lay1,:])\n",
    "            readata_testl1_corr = error_correction(readata_testl1, anom_ext, mode=corrmode)\n",
    "            reacorr_stn[rr, testindex1, :] = readata_testl1_corr\n",
    "\n",
    "        # output: extrapolate the weight from train stations to test stations (in layer-1)\n",
    "        weight_testl1 = extrapolation(weight_trainl1, nearstn_locl1[lay1,:],nearstn_distl1[lay1,:])\n",
    "        reamerge_weight_stn[testindex1, :] = weight_testl1\n",
    "\n",
    "        # output: merge reanalysis products at the test stations\n",
    "        nstn_testl1 = len(testindex1)\n",
    "        mergedata_testl1 = np.nan * np.zeros([nstn_testl1, ntimes])\n",
    "        for i in range(ntimes):\n",
    "            datain = np.zeros([nstn_testl1, reanum], dtype=np.float32)\n",
    "            for rr in range(reanum):\n",
    "                datain[:, rr] = reacorr_stn[rr, testindex1, i]\n",
    "            if var == 'prcp':\n",
    "                datain = box_cox_transform(datain)\n",
    "            dataout = weightmerge(datain, weight_testl1)\n",
    "            if var == 'prcp':\n",
    "                dataout = box_cox_recover(dataout)\n",
    "            mergedata_testl1[:, i] = dataout\n",
    "        reamerge_stn[testindex1, :] = mergedata_testl1\n",
    "\n",
    "    weightsum = np.nansum(reamerge_weight_stn,axis=1)\n",
    "    weightsum[weightsum==0] = np.nan\n",
    "    for rr in range(reanum):\n",
    "        reamerge_weight_stn[:, rr] = reamerge_weight_stn[:, rr] / weightsum\n",
    "    return reamerge_stn, reamerge_weight_stn, reacorr_stn\n",
    "\n",
    "\n",
    "def correct_merge(stndata, readata_raw, readata_stn, reacorr_stn, reamerge_stn, reamerge_weight_stn, neargrid_loc,\n",
    "         neargrid_dist, merge_choice, mask, hwsize, corrmode, anombound, var, weightmode):\n",
    "\n",
    "    nrows, ncols, nearnum = np.shape(neargrid_loc)\n",
    "    reanum, nstn, nday = np.shape(readata_stn)\n",
    "\n",
    "    neargrid_loc = neargrid_loc.copy()\n",
    "    neargrid_dist = neargrid_dist.copy()\n",
    "    mask2 = np.tile(mask[:,:,np.newaxis], (1,1,nearnum))\n",
    "    neargrid_loc[mask2 != 1] = -1\n",
    "    neargrid_dist[mask2 != 1] = np.nan\n",
    "    del mask2\n",
    "\n",
    "\n",
    "    # correct raw gridded reanalysis data using all stations\n",
    "    corr_data = np.nan * np.zeros([reanum, nrows, ncols, nday], dtype=np.float32)\n",
    "    for rr in range(reanum):\n",
    "        # calculate correction ratio at all station point\n",
    "        anom_ori = calculate_anomaly(readata_stn[rr, :, :], stndata[:, :],\n",
    "                                     hwsize, corrmode, upbound=anombound[1], lowbound=anombound[0])\n",
    "        anom_ext = extrapolation(anom_ori, neargrid_loc, neargrid_dist)\n",
    "        corr_data[rr, :, :, :] = error_correction(readata_raw[rr,:,:,:], anom_ext, mode=corrmode)\n",
    "\n",
    "    # first error estimation\n",
    "    corr_error = np.nan * np.zeros([reanum, nrows, ncols, nday])\n",
    "    for rr in range(reanum):\n",
    "        corr_error[rr, :, :, :] = extrapolation(reacorr_stn[rr, :, :] - stndata, neargrid_loc, neargrid_dist)\n",
    "    merge_error0 = extrapolation(reamerge_stn - stndata, neargrid_loc, neargrid_dist)\n",
    "\n",
    "    if var == 'prcp' and weightmode == 'BMA':\n",
    "        corr_data = box_cox_transform(corr_data)\n",
    "        stndata = box_cox_transform(stndata)\n",
    "        reamerge_stn = box_cox_transform(reamerge_stn)\n",
    "        reacorr_stn = box_cox_transform(reacorr_stn)\n",
    "\n",
    "        # correction error in normal space (box-cox)\n",
    "        corr_error_bc = np.nan * np.zeros([reanum, nrows, ncols, nday])\n",
    "        for rr in range(reanum):\n",
    "            corr_error_bc[rr, :, :, :] = extrapolation(reacorr_stn[rr, :, :] - stndata, neargrid_loc, neargrid_dist)\n",
    "        merge_error0_bc = extrapolation(reamerge_stn - stndata, neargrid_loc, neargrid_dist)\n",
    "\n",
    "    # merge reanalysis data\n",
    "    merge_data = np.nan * np.zeros([nrows, ncols, nday], dtype=np.float32)\n",
    "    weight_grid = extrapolation(reamerge_weight_stn, neargrid_loc, neargrid_dist)\n",
    "    for i in range(nday):\n",
    "        datain = np.zeros([nrows, ncols, reanum])\n",
    "        for rr in range(reanum):\n",
    "            datain[:, :, rr] = corr_data[rr, :, :, i]\n",
    "        merge_data[:, :, i] = weightmerge(datain, weight_grid)\n",
    "\n",
    "    # calculate the error of merged data (this is actually independent with merged data estimation)\n",
    "    merge_error = np.nan * np.zeros([nrows, ncols, nday], dtype=np.float32)\n",
    "    for r in range(nrows):\n",
    "        for c in range(ncols):\n",
    "            if not np.isnan(mask[r, c]):\n",
    "                chi = merge_choice[r, c]\n",
    "                if chi > 0:\n",
    "                    merge_error[r, c, :] = corr_error[chi - 1, r, c, :]\n",
    "                else:\n",
    "                    merge_error[r, c, :] = merge_error0[r, c, :]\n",
    "\n",
    "    if var == 'prcp' and weightmode == 'BMA':\n",
    "        merge_error_bc = np.nan * np.zeros([nrows, ncols, nday], dtype=np.float32)\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if not np.isnan(mask[r, c]):\n",
    "                    chi = merge_choice[r, c]\n",
    "                    if chi > 0:\n",
    "                        merge_error_bc[r, c, :] = corr_error_bc[chi - 1, r, c, :]\n",
    "                    else:\n",
    "                        merge_error_bc[r, c, :] = merge_error0_bc[r, c, :]\n",
    "\n",
    "    if var == 'prcp' and weightmode == 'BMA':\n",
    "        merge_error_out = [''] * 2\n",
    "        merge_error_out[0] = merge_error\n",
    "        merge_error_out[1] = merge_error_bc\n",
    "        merge_data = box_cox_recover(merge_data)\n",
    "    else:\n",
    "        merge_error_out = ['']\n",
    "        merge_error_out[0] = merge_error\n",
    "    return corr_data, corr_error, merge_data, merge_error_out\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# var = sys.argv[1]\n",
    "# weightmode = sys.argv[2]\n",
    "\n",
    "########################################################################################################################\n",
    "var = 'prcp'  # ['prcp', 'tmean', 'trange']: this should be input from sbtach script\n",
    "weightmode = 'RMSE'  # (CC, RMSE, BMA). Weight = CC**2, or Weight = 1/RMSE**2, or Weight = BMA\n",
    "# basic settings\n",
    "lontar = np.arange(-180 + 0.05, -50, 0.1)\n",
    "lattar = np.arange(85 - 0.05, 5, -0.1)\n",
    "hwsize = 0  # define time window (2*hwsize+1) used to calculate ratio (as ratio for a specific day is too variable)\n",
    "nearnum = 8  # the number of nearby stations used to extrapolate points to grids (for correction and merging)\n",
    "dividenum = 10  # divide the datasets into X parts, e.g. 10-fold cross-validation\n",
    "anombound = [0.2, 5]  # upper and lower bound when calculating the anomaly for correction\n",
    "year = [2000, 2000]  # year range for merging. note weight is calculated using all data not limited by year\n",
    "\n",
    "# input files\n",
    "# station list and data\n",
    "# gmet_stnfile = '/home/gut428/GMET/eCAI_EMDNA/StnGridInfo/stnlist_whole.txt'\n",
    "# gmet_stndatafile = '/home/gut428/stndata_whole.npz'\n",
    "gmet_stnfile = '/Users/localuser/GMET/pyGMET_NA/stnlist_whole.txt'\n",
    "gmet_stndatafile = '/Users/localuser/GMET/pyGMET_NA/stndata_whole.npz'  # to be saved. only process when absent\n",
    "\n",
    "# mask file\n",
    "# file_mask = '/datastore/GLOBALWATER/CommonData/EMDNA/DEM/NA_DEM_010deg_trim.mat'\n",
    "file_mask = './DEM/NA_DEM_010deg_trim.mat'\n",
    "\n",
    "# downscaled reanalysis: gridded data\n",
    "prefix = ['ERA5_', 'MERRA2_', 'JRA55_']\n",
    "# downscaled reanalysis data at station points\n",
    "# path_readowngrid = ['/datastore/GLOBALWATER/CommonData/EMDNA/ERA5_day_ds',\n",
    "#                    '/datastore/GLOBALWATER/CommonData/EMDNA/MERRA2_day_ds',\n",
    "#                    '/datastore/GLOBALWATER/CommonData/EMDNA/JRA55_day_ds']\n",
    "# file_readownstn = ['/datastore/GLOBALWATER/CommonData/EMDNA/ERA5_day_ds/ERA5_downto_stn.npz',\n",
    "#                    '/datastore/GLOBALWATER/CommonData/EMDNA/MERRA2_day_ds/MERRA2_downto_stn.npz',\n",
    "#                    '/datastore/GLOBALWATER/CommonData/EMDNA/JRA55_day_ds/JRA55_downto_stn.npz']\n",
    "path_readowngrid = ['/Users/localuser/Research/Test',\n",
    "                   '/Users/localuser/Research/Test',\n",
    "                   '/Users/localuser/Research/Test']\n",
    "file_readownstn = ['/Users/localuser/Research/Test/ERA5_downto_stn.npz',\n",
    "                   '/Users/localuser/Research/Test/MERRA2_downto_stn.npz',\n",
    "                   '/Users/localuser/Research/Test/JRA55_downto_stn.npz']\n",
    "\n",
    "# output files\n",
    "# train and test index file\n",
    "ttindexfile = '/Users/localuser/Research/Test/2layer_train_test_index.npz'\n",
    "# ttindexfile = '/datastore/GLOBALWATER/CommonData/EMDNA/ReanalysisCorrMerge/CrossValidate_2layer/2layer_train_test_index.npz'\n",
    "\n",
    "# near stations\n",
    "near_path = '/Users/localuser/Research/Test'\n",
    "# near_path = '/home/gut428/ReanalysisCorrMerge'\n",
    "near_file_GMET = '/Users/localuser/GMET/pyGMET_NA/weight_nearstn.npz'\n",
    "# near_file_GMET = '/datastore/GLOBALWATER/CommonData/EMDNA/PyGMETout/weight.npz'\n",
    "useGMET = False\n",
    "\n",
    "# error and merging at station level\n",
    "path_reastn_cv = '/Users/localuser/Research/Test'\n",
    "# path_reastn_cv = '/datastore/GLOBALWATER/CommonData/EMDNA/ReanalysisCorrMerge/CrossValidate_2layer'\n",
    "file_corrmerge_stn = path_reastn_cv + '/mergecorr_' + var + '_' + weightmode + '.npz'\n",
    "\n",
    "# output corrected and merged data\n",
    "path_reacorr = '/Users/localuser/Research/Test'\n",
    "path_merge = '/Users/localuser/Research/Test'\n",
    "# path_reacorr = '/home/gut428/ReanalysisCorrMerge/Reanalysis_corr'\n",
    "# path_merge = '/home/gut428/ReanalysisCorrMerge/Reanalysis_merge'\n",
    "file_mergechoice = path_merge + '/mergechoice_' + var + '_' +  weightmode + '.npz'\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# basic processing\n",
    "print('start basic processing')\n",
    "# decide correction mode according to variables\n",
    "if var == 'prcp' or var == 'trange':\n",
    "    corrmode = 'ratio'  # ratio or diff: mode for error correction\n",
    "elif var == 'tmean':\n",
    "    corrmode = 'diff'\n",
    "else:\n",
    "    sys.exit('Unknown correction mode')\n",
    "\n",
    "if corrmode == 'diff':\n",
    "    # default settings in this study since diff is for tmean and trange\n",
    "    hwsize = 0\n",
    "    anombound = [-999, 999]\n",
    "\n",
    "# mask\n",
    "mask = io.loadmat(file_mask)\n",
    "mask = mask['DEM']\n",
    "mask[~np.isnan(mask)] = 1  # 1: valid pixels\n",
    "# attributes\n",
    "reanum = len(file_readownstn)\n",
    "nrows, ncols = np.shape(mask)\n",
    "lontarm, lattarm = np.meshgrid(lontar, lattar)\n",
    "lontarm[np.isnan(mask)] = np.nan\n",
    "lattarm[np.isnan(mask)] = np.nan\n",
    "\n",
    "# date\n",
    "date_list, date_number = m_DateList(1979, 2018, 'ByYear')\n",
    "\n",
    "# load observations for all stations\n",
    "datatemp = np.load(gmet_stndatafile)\n",
    "stndata = datatemp[var + '_stn']\n",
    "stnlle = datatemp['stn_lle']\n",
    "nstn, ntimes = np.shape(stndata)\n",
    "del datatemp\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# design a two-layer cross-validation: generate station combinations\n",
    "# index1 extracts 90% stations for merging and 10% stations for validation\n",
    "# index2 divides the 90% from index1 into 90% and 10% again for error correction\n",
    "if not os.path.isfile(ttindexfile):\n",
    "    print('divide stations for 2-layer cross validation')\n",
    "    prcp_trainindex1, prcp_testindex1, prcp_trainindex2, prcp_testindex2, \\\n",
    "    tmean_trainindex1, tmean_testindex1, tmean_trainindex2, tmean_testindex2 = \\\n",
    "        double_cvindex(gmet_stndatafile, dividenum, rndseed=123)\n",
    "    np.savez_compressed(ttindexfile, prcp_trainindex1=prcp_trainindex1, prcp_testindex1=prcp_testindex1,\n",
    "                        prcp_trainindex2=prcp_trainindex2, prcp_testindex2=prcp_testindex2,\n",
    "                        tmean_trainindex1=tmean_trainindex1, tmean_testindex1=tmean_testindex1,\n",
    "                        tmean_trainindex2=tmean_trainindex2, tmean_testindex2=tmean_testindex2)\n",
    "    del prcp_trainindex1, prcp_testindex1, prcp_trainindex2, prcp_testindex2, \\\n",
    "        tmean_trainindex1, tmean_testindex1, tmean_trainindex2, tmean_testindex2\n",
    "\n",
    "taintestindex = np.load(ttindexfile)\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# find near stations for all grids and station\n",
    "\n",
    "if var == 'trange':\n",
    "    vari = 'tmean'  # trange and tmean have the same index\n",
    "else:\n",
    "    vari = var\n",
    "\n",
    "near_stnfile = near_path + '/near_stn_' + vari + '.npz'\n",
    "near_gridfile = near_path + '/near_grid_' + vari + '.npz'\n",
    "\n",
    "if os.path.isfile(near_stnfile):\n",
    "    print('load near station information for points')\n",
    "    with np.load(near_stnfile) as datatemp:\n",
    "        nearstn_locl1 = datatemp['nearstn_locl1']\n",
    "        nearstn_distl1 = datatemp['nearstn_distl1']\n",
    "        nearstn_locl2 = datatemp['nearstn_locl2']\n",
    "        nearstn_distl2 = datatemp['nearstn_distl2']\n",
    "    del datatemp\n",
    "else:\n",
    "    print('find near stations for station points')\n",
    "    # layer-1\n",
    "    nstn_testl1 = np.shape(taintestindex[vari + '_testindex1'])[1]\n",
    "    nearstn_locl1 = -1 * np.ones([dividenum, nstn_testl1, nearnum], dtype=int)\n",
    "    nearstn_distl1 = -1 * np.ones([dividenum, nstn_testl1, nearnum], dtype=np.float32)\n",
    "    for lay1 in range(dividenum):\n",
    "        trainindex1 = taintestindex[vari + '_trainindex1'][lay1, :]\n",
    "        testindex1 = taintestindex[vari + '_testindex1'][lay1, :]\n",
    "        nearstn_locl1[lay1, :, :], nearstn_distl1[lay1, :, :] \\\n",
    "            = findnearstn(stnlle[trainindex1, 0], stnlle[trainindex1, 1],\n",
    "                          stnlle[testindex1, 0], stnlle[testindex1, 1], nearnum, 0)\n",
    "    # layer-2\n",
    "    nstn_testl2 = np.shape(taintestindex[vari + '_testindex2'])[2]\n",
    "    nearstn_locl2 = -1 * np.ones([dividenum, dividenum, nstn_testl2, nearnum], dtype=int)\n",
    "    nearstn_distl2 = -1 * np.ones([dividenum, dividenum, nstn_testl2, nearnum], dtype=np.float32)\n",
    "    for lay1 in range(dividenum):\n",
    "        for lay2 in range(dividenum):\n",
    "            trainindex2 = taintestindex[vari + '_trainindex2'][lay1, lay2, :]\n",
    "            testindex2 = taintestindex[vari + '_testindex2'][lay1, lay2, :]\n",
    "            nearstn_locl2[lay1, lay2, :, :], nearstn_distl2[lay1, lay2, :, :] \\\n",
    "                = findnearstn(stnlle[trainindex2, 0], stnlle[trainindex2, 1],\n",
    "                              stnlle[testindex2, 0], stnlle[testindex2, 1], nearnum, 0)\n",
    "\n",
    "    np.savez_compressed(near_stnfile, nearstn_locl1=nearstn_locl1, nearstn_distl1=nearstn_distl1,\n",
    "                        nearstn_locl2=nearstn_locl2, nearstn_distl2=nearstn_distl2)\n",
    "\n",
    "if os.path.isfile(near_gridfile):\n",
    "    print('load near station information for grids')\n",
    "    with np.load(near_gridfile) as datatemp:\n",
    "        neargrid_loc = datatemp['neargrid_loc']\n",
    "        neargrid_dist = datatemp['neargrid_dist']\n",
    "else:\n",
    "    print('find near stations for grids')\n",
    "    stnlle_in = stnlle.copy()\n",
    "    stnlle_in[np.isnan(stndata[:, 0]), 0:2] = np.nan\n",
    "    neargrid_loc, neargrid_dist = findnearstn(stnlle_in[:, 0], stnlle_in[:, 1], lattarm, lontarm, nearnum, 0)\n",
    "    np.savez_compressed(near_gridfile,neargrid_loc=neargrid_loc,neargrid_dist=neargrid_dist)\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# load downscaled reanalysis for all stations\n",
    "print('load downscaled reanalysis data at station points')\n",
    "readata_stn = np.nan * np.zeros([reanum, nstn, ntimes], dtype=np.float32)\n",
    "for rr in range(reanum):\n",
    "    dr = np.load(file_readownstn[rr])\n",
    "    temp = dr[var + '_readown']\n",
    "    if prefix[rr] == 'MERRA2_':  # unify the time length of all data as MERRA2 lacks 1979\n",
    "        add = np.nan * np.zeros([nstn, 365])\n",
    "        temp = np.concatenate((add, temp), axis=1)\n",
    "    readata_stn[rr, :, :] = temp\n",
    "    del dr, temp\n",
    "if var == 'prcp':\n",
    "    readata_stn[readata_stn<0] = 0\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# estimate the error of corrected and merged data at station points using cross-validation\n",
    "# target outputs: weights for merging, merge choice, error of the merged dataset\n",
    "# 0. load regression estimates and observations for all stations\n",
    "# 1. select layer-1 stations (90% train, 10% test)\n",
    "# 2. select layer-2 stations: 10 combinations (90% train, 10% test) from the 90% stations in layer-1\n",
    "# 3. for all combinations in layer-2\n",
    "# 3.1 perform error correction (1) at station points, (2) extrapolate to grids using training stations\n",
    "# 3.2 perform evaluation using test stations\n",
    "# 3.3 loop for all combinations and do evaluation at all stations (the 90% training stations from layer-1)\n",
    "# 3.4 extrapolate evaluation accuracy indicators to the domain\n",
    "# 4. merge three reanalysis using their indicators in 3.4\n",
    "# 5. repeat 1-4 for all combinations in layer-1, and get accuracy indicators of the merged dataset for the domain\n",
    "# 6. use indicators from 3.4 and 5, and select the best one among three reanalysis and merged datasets for each grid\n",
    "# 7. get the final merged dataset and its accuracy indicators from steo-6\n",
    "\n",
    "# get merged and corrected reanalysis data at all station points using two-layer cross-validation\n",
    "if os.path.isfile(file_corrmerge_stn):\n",
    "    print('load independent merged/corrected data at station points')\n",
    "    datatemp = np.load(file_corrmerge_stn)\n",
    "    reamerge_stn = datatemp['reamerge_stn']\n",
    "    reamerge_weight_stn = datatemp['reamerge_weight_stn']\n",
    "    reacorr_stn = datatemp['reacorr_stn']\n",
    "    del datatemp\n",
    "else:\n",
    "    reamerge_stn = np.nan * np.zeros([nstn, ntimes], dtype=np.float32)\n",
    "    reamerge_weight_stn = np.nan * np.zeros([12, nstn, reanum], dtype=np.float32)\n",
    "    reacorr_stn = np.nan * np.zeros([reanum, nstn, ntimes], dtype=np.float32)\n",
    "    # for each month\n",
    "    for m in range(12):\n",
    "        print('month', m+1)\n",
    "        outpathm = path_reastn_cv + '/month_' + str(m + 1)\n",
    "        if not os.path.isdir(outpathm):\n",
    "            os.mkdir(outpathm)\n",
    "        indm = date_number['mm'] == (m + 1)\n",
    "        reamerge_stnm, reamerge_weight_stnm, reacorr_stnm = \\\n",
    "            merge_correction_stnerror(outpathm, stnlle, stndata[:,indm], readata_stn[:,:,indm], taintestindex,\n",
    "                                      nearstn_locl1, nearstn_distl1, nearstn_locl2, nearstn_distl2,\n",
    "                                      dividenum, var, hwsize, corrmode, anombound, weightmode)\n",
    "        reamerge_stn[:, indm] = reamerge_stnm\n",
    "        reacorr_stn[:, :, indm] = reacorr_stnm\n",
    "        reamerge_weight_stn[m, :, :] = reamerge_weight_stnm\n",
    "    # the variables are independent with their concurrent stations. thus, station data can be used to evaluate them\n",
    "    np.savez_compressed(file_corrmerge_stn, reamerge_stn=reamerge_stn, reamerge_weight_stn=reamerge_weight_stn,\n",
    "                        reacorr_stn=reacorr_stn, date_list=date_list)\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# which is the best for each grid (all reanalysis and the merged reanalysis)\n",
    "if os.path.isfile(file_mergechoice):\n",
    "    print('load merge choice file')\n",
    "    datatemp = np.load(file_mergechoice)\n",
    "    merge_choice = datatemp['merge_choice']\n",
    "    del datatemp\n",
    "else:\n",
    "    print('determine the best data choice for each grid cell')\n",
    "    merge_choice = -1 * np.zeros([12,nrows,ncols], dtype=int)\n",
    "    for m in range(12):\n",
    "        print('month', m+1)\n",
    "        indm = date_number['mm'] == (m + 1)\n",
    "        # evaluate merge and corrected reanalysis\n",
    "        # this evaluation is feasible as merge or corrected data are all obtained using independent stations\n",
    "        met_merge_stn = calmetric(reamerge_stn[:, indm], stndata[:, indm], metname='RMSE')\n",
    "        met_corr_stn = np.nan * np.zeros([nstn, reanum])\n",
    "        for rr in range(reanum):\n",
    "            met_corr_stn[:, rr] = calmetric(reacorr_stn[rr][ :, indm], stndata[:, indm], metname='RMSE')\n",
    "\n",
    "        metric_all = np.zeros([nrows, ncols, reanum + 1])\n",
    "        met_merge_grid = extrapolation(met_merge_stn, neargrid_loc, neargrid_dist)\n",
    "        met_corr_grid = extrapolation(met_corr_stn, neargrid_loc, neargrid_dist)\n",
    "        metric_all[:, :, 0] = met_merge_grid\n",
    "        metric_all[:, :, 1:] = met_corr_grid\n",
    "        merge_choicem = np.argmax(metric_all, axis=2)  # 0: merge, 1 to N: corresponding corrected reanalysis\n",
    "        merge_choicem[mask != 1] = -1\n",
    "        merge_choice[m, :, :] = merge_choicem\n",
    "        del metric_all, met_merge_grid, met_corr_grid\n",
    "    np.savez_compressed(file_mergechoice, merge_choice=merge_choice)\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# final merging with data and error\n",
    "\n",
    "# use the same weight with GMET estimation?\n",
    "if useGMET == True:\n",
    "    print('use near station information from GMET regression')\n",
    "    del neargrid_loc, neargrid_dist\n",
    "    datatemp = np.load(near_file_GMET)\n",
    "    if var == 'prcp':\n",
    "        neargrid_loc = datatemp['near_grid_prcpLoc']\n",
    "        neargrid_dist = datatemp['near_grid_prcpDist']\n",
    "    else:\n",
    "        neargrid_loc = datatemp['near_grid_tempDist']\n",
    "        neargrid_dist = datatemp['near_grid_tempDist']\n",
    "    neargrid_loc = np.flipud(neargrid_loc)\n",
    "    neargrid_dist = np.flipud(neargrid_dist)\n",
    "\n",
    "# start ...\n",
    "for y in range(year[0], year[1] + 1):\n",
    "    print('Correction and Merge: year',y)\n",
    "    filemerge = path_merge + 'mergedata_' + var + '_' + str(y) + weightmode + '.npz'\n",
    "    filecorr = path_reacorr + 'reacorrdata_' + var + '_' + str(y) + '.npz'\n",
    "    if os.path.isfile(filemerge) and os.path.isfile(filecorr):\n",
    "        print('file exists ... continue')\n",
    "        continue\n",
    "\n",
    "    # initilization\n",
    "    nday = np.sum(date_number['yyyy'] == y)\n",
    "    corr_data = np.nan * np.zeros([reanum, nrows, ncols, nday], dtype=np.float32)\n",
    "    corr_error = np.nan * np.zeros([reanum, nrows, ncols, nday], dtype=np.float32)\n",
    "    merge_data = np.nan * np.zeros([nrows, ncols, nday], dtype=np.float32)\n",
    "    merge_error = np.nan * np.zeros([nrows, ncols, nday], dtype=np.float32)\n",
    "    if var == 'prcp' and weightmode == 'BMA':\n",
    "        merge_error_bc = np.nan * np.zeros([nrows, ncols, nday], dtype=np.float32) # error in normal space (box-cox)\n",
    "\n",
    "    # read raw gridded reanalysis data\n",
    "    readata_raw = np.nan * np.zeros([reanum, nrows, ncols, nday], dtype=np.float32)\n",
    "    for rr in range(reanum):\n",
    "        if not (prefix[rr] == 'MERRA2_' and y == 1979):\n",
    "            filer = path_readowngrid[rr] + '/' + prefix[rr] + var + '_' + str(y) + '.npz'\n",
    "            d = np.load(filer)\n",
    "            readata_raw[rr, :, :, :] = d['data']\n",
    "            del d\n",
    "\n",
    "    # process for each month\n",
    "    for m in range(12):\n",
    "        print('Correction and Merge: month', m+1)\n",
    "        indym = (date_number['yyyy'] == y) & (date_number['mm'] == m+1)\n",
    "        ym = date_number['mm'][date_number['yyyy'] == y]\n",
    "        indm = ym == m+1\n",
    "\n",
    "        corr_datam, corr_errorm, merge_datam, merge_errorm = \\\n",
    "            correct_merge(stndata[:, indym], readata_raw[:,:,:,indm], readata_stn[:, :, indym], reacorr_stn[:, :, indym],\n",
    "                          reamerge_stn[:, indym], reamerge_weight_stn[m, :, :], neargrid_loc, neargrid_dist,\n",
    "                          merge_choice[m, :, :], mask, hwsize, corrmode, anombound, var, weightmode)\n",
    "        corr_data[:,:,:,indm] = corr_datam\n",
    "        corr_error[:, :, :, indm] = corr_errorm\n",
    "        merge_data[:, :, indm] = merge_datam\n",
    "        merge_error[:, :, indm] = merge_errorm[0]\n",
    "        if var == 'prcp' and weightmode == 'BMA':\n",
    "            merge_error_bc[:, :, indm] = merge_errorm[1]\n",
    "\n",
    "    np.savez_compressed(filecorr, corr_data=corr_data, corr_error=corr_error,\n",
    "                        reaname=prefix, latitude=lattar, longitude=lontar)\n",
    "    if var == 'prcp' and weightmode == 'BMA':\n",
    "        np.savez_compressed(filemerge, merge_data=merge_data, merge_error=merge_error, merge_error_bc=merge_error_bc,\n",
    "                            latitude=lattar, longitude=lontar, reaname=prefix)\n",
    "    else:\n",
    "        np.savez_compressed(filemerge, merge_data=merge_data, merge_error=merge_error,\n",
    "                            latitude=lattar, longitude=lontar, reaname=prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use near station information from GMET regression\n",
      "Correction and Merge: year 2000\n",
      "Correction and Merge: month 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:94: RuntimeWarning: Mean of empty slice\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:95: RuntimeWarning: Mean of empty slice\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:98: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:98: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:106: RuntimeWarning: invalid value encountered in greater\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:107: RuntimeWarning: invalid value encountered in less\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:284: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correction and Merge: month 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-191-1b9e9631058e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mcorr_datam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorr_errorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_datam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_errorm\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             correct_merge(stndata[:, indym], readata_raw[:,:,:,indm], readata_stn[:, :, indym], reacorr_stn[:, :, indym],\n\u001b[0m\u001b[1;32m     51\u001b[0m                           \u001b[0mreamerge_stn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindym\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreamerge_weight_stn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneargrid_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneargrid_dist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                           merge_choice[m, :, :], mask, hwsize, corrmode, anombound, var, weightmode)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "useGMET=True\n",
    "if useGMET == True:\n",
    "    print('use near station information from GMET regression')\n",
    "    del neargrid_loc, neargrid_dist\n",
    "    datatemp = np.load(near_file_GMET)\n",
    "    if var == 'prcp':\n",
    "        neargrid_loc = datatemp['near_grid_prcpLoc']\n",
    "        neargrid_dist = datatemp['near_grid_prcpDist']\n",
    "    else:\n",
    "        neargrid_loc = datatemp['near_grid_tempDist']\n",
    "        neargrid_dist = datatemp['near_grid_tempDist']\n",
    "    neargrid_loc = np.flipud(neargrid_loc)\n",
    "    neargrid_dist = np.flipud(neargrid_dist)\n",
    "\n",
    "# start ...\n",
    "for y in range(year[0], year[1] + 1):\n",
    "    print('Correction and Merge: year',y)\n",
    "    filemerge = path_merge + 'mergedata_' + var + '_' + str(y) + weightmode + '.npz'\n",
    "    filecorr = path_reacorr + 'reacorrdata_' + var + '_' + str(y) + '.npz'\n",
    "    if os.path.isfile(filemerge) and os.path.isfile(filecorr):\n",
    "        print('file exists ... continue')\n",
    "        continue\n",
    "\n",
    "    # initilization\n",
    "    nday = np.sum(date_number['yyyy'] == y)\n",
    "    corr_data = np.nan * np.zeros([reanum, nrows, ncols, nday], dtype=np.float32)\n",
    "    corr_error = np.nan * np.zeros([reanum, nrows, ncols, nday], dtype=np.float32)\n",
    "    merge_data = np.nan * np.zeros([nrows, ncols, nday], dtype=np.float32)\n",
    "    merge_error = np.nan * np.zeros([nrows, ncols, nday], dtype=np.float32)\n",
    "    if var == 'prcp' and weightmode == 'BMA':\n",
    "        merge_error_bc = np.nan * np.zeros([nrows, ncols, nday], dtype=np.float32) # error in normal space (box-cox)\n",
    "\n",
    "    # read raw gridded reanalysis data\n",
    "    readata_raw = np.nan * np.zeros([reanum, nrows, ncols, nday], dtype=np.float32)\n",
    "    for rr in range(reanum):\n",
    "        if not (prefix[rr] == 'MERRA2_' and y == 1979):\n",
    "            filer = path_readowngrid[rr] + '/' + prefix[rr] + var + '_' + str(y) + '.npz'\n",
    "            d = np.load(filer)\n",
    "            readata_raw[rr, :, :, :] = d['data']\n",
    "            del d\n",
    "\n",
    "    # process for each month\n",
    "    for m in range(12):\n",
    "        print('Correction and Merge: month', m+1)\n",
    "        indym = (date_number['yyyy'] == y) & (date_number['mm'] == m+1)\n",
    "        ym = date_number['mm'][date_number['yyyy'] == y]\n",
    "        indm = ym == m+1\n",
    "\n",
    "        corr_datam, corr_errorm, merge_datam, merge_errorm = \\\n",
    "            correct_merge(stndata[:, indym], readata_raw[:,:,:,indm], readata_stn[:, :, indym], reacorr_stn[:, :, indym],\n",
    "                          reamerge_stn[:, indym], reamerge_weight_stn[m, :, :], neargrid_loc, neargrid_dist,\n",
    "                          merge_choice[m, :, :], mask, hwsize, corrmode, anombound, var, weightmode)\n",
    "        corr_data[:,:,:,indm] = corr_datam\n",
    "        corr_error[:, :, :, indm] = corr_errorm\n",
    "        merge_data[:, :, indm] = merge_datam\n",
    "        merge_error[:, :, indm] = merge_errorm[0]\n",
    "        if var == 'prcp' and weightmode == 'BMA':\n",
    "            merge_error_bc[:, :, indm] = merge_errorm[1]\n",
    "\n",
    "    np.savez_compressed(filecorr, corr_data=corr_data, corr_error=corr_error,\n",
    "                        reaname=prefix, latitude=lattar, longitude=lontar)\n",
    "    if var == 'prcp' and weightmode == 'BMA':\n",
    "        np.savez_compressed(filemerge, merge_data=merge_data, merge_error=merge_error, merge_error_bc=merge_error_bc,\n",
    "                            latitude=lattar, longitude=lontar, reaname=prefix)\n",
    "    else:\n",
    "        np.savez_compressed(filemerge, merge_data=merge_data, merge_error=merge_error,\n",
    "                            latitude=lattar, longitude=lontar, reaname=prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(merge_error[:,:,3])\n",
    "plt.colorbar()\n",
    "plt.clim([0,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
