{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import auxiliary as au\n",
    "import regression as reg\n",
    "import datetime as dt\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import io\n",
    "from scipy.interpolate import interp2d\n",
    "import os\n",
    "import sys\n",
    "from scipy.interpolate import griddata\n",
    "import h5py\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "def divide_train_test(data, dividenum, randseed=-1):\n",
    "    if randseed == -1:\n",
    "        random.seed(time.time())\n",
    "    num = len(data)\n",
    "    subnum = int(num / dividenum)\n",
    "    data_train = np.zeros([dividenum, num - subnum],dtype=int)\n",
    "    data_test = np.zeros([dividenum, subnum],dtype=int)\n",
    "    randindex = random.sample(range(num), num)\n",
    "    for i in range(dividenum):\n",
    "        data_test[i, :] = np.sort(data[randindex[i * subnum:(i + 1) * subnum]])\n",
    "        data_train[i, :] = np.setdiff1d(data, data_test[i])\n",
    "    return data_train, data_test\n",
    "\n",
    "\n",
    "def double_cvindex(gmet_stndatafile, dividenum):\n",
    "    # index for double cross-validation\n",
    "    datatemp = np.load(gmet_stndatafile)\n",
    "    prcp_stn0 = datatemp['prcp_stn'][:, 0]\n",
    "    tmean_stn0 = datatemp['tmean_stn'][:, 0]\n",
    "    prcp_stnindex = np.argwhere(~np.isnan(prcp_stn0))\n",
    "    prcp_stnindex = prcp_stnindex.flatten()\n",
    "    tmean_stnindex = np.argwhere(~np.isnan(tmean_stn0))\n",
    "    tmean_stnindex = tmean_stnindex.flatten()\n",
    "\n",
    "    subnum1 = int(len(prcp_stnindex) / dividenum)\n",
    "    subnum2 = int((len(prcp_stnindex) - subnum1) / dividenum)\n",
    "    # prcp_testindex1 = np.zeros([dividenum,subnum1])\n",
    "    # prcp_trainindex1 = np.zeros([dividenum,len(prcp_stnindex) - subnum1])\n",
    "    prcp_trainindex1, prcp_testindex1 = divide_train_test(prcp_stnindex, dividenum, randseed=123)\n",
    "    prcp_testindex2 = np.zeros([dividenum, dividenum, subnum2], dtype=int)\n",
    "    prcp_trainindex2 = np.zeros([dividenum, dividenum, len(prcp_stnindex) - subnum1 - subnum2], dtype=int)\n",
    "    for i in range(dividenum):\n",
    "        traini, testi = divide_train_test(prcp_trainindex1[i, :], dividenum, randseed=123)\n",
    "        prcp_trainindex2[i, :, :] = traini\n",
    "        prcp_testindex2[i, :, :] = testi\n",
    "\n",
    "    subnum1 = int(len(tmean_stnindex) / dividenum)\n",
    "    subnum2 = int((len(tmean_stnindex) - subnum1) / dividenum)\n",
    "    # tmean_testindex1 = np.zeros([dividenum,subnum1])\n",
    "    # tmean_trainindex1 = np.zeros([dividenum,len(tmean_stnindex) - subnum1])\n",
    "    tmean_trainindex1, tmean_testindex1 = divide_train_test(tmean_stnindex, dividenum, randseed=123)\n",
    "    tmean_testindex2 = np.zeros([dividenum, dividenum, subnum2], dtype=int)\n",
    "    tmean_trainindex2 = np.zeros([dividenum, dividenum, len(tmean_stnindex) - subnum1 - subnum2], dtype=int)\n",
    "    for i in range(dividenum):\n",
    "        traini, testi = divide_train_test(tmean_trainindex1[i, :], dividenum, randseed=123)\n",
    "        tmean_trainindex2[i, :, :] = traini\n",
    "        tmean_testindex2[i, :, :] = testi\n",
    "    return prcp_trainindex1, prcp_testindex1, prcp_trainindex2, prcp_testindex2, \\\n",
    "           tmean_trainindex1, tmean_testindex1, tmean_trainindex2, tmean_testindex2\n",
    "\n",
    "def calculate_anomaly(datatar, dataref, hwsize, amode, upbound=5, lowbound=0.2):\n",
    "    # datatar, dataref: 2D [nstn, ntime]\n",
    "    # amode: anomaly mode ('ratio' or 'diff')\n",
    "    # hwsize: define time window (2*hwsize+1) used to calculate ratio (as ratio for a specific day is too variable)\n",
    "    # upbound/lowbound: upper and lower limitation of ratio/difference\n",
    "    if np.ndim(datatar) == 1: # only one time step\n",
    "        datatar = datatar[:,np.newaxis]\n",
    "        dataref = dataref[:,np.newaxis]\n",
    "\n",
    "    nstn, ntime = np.shape(datatar)\n",
    "    if ntime < hwsize * 2 + 1:\n",
    "        print('The window size is larger than time steps when calculating ratio between tar and ref datasets')\n",
    "        print('Please set a smaller hwsize')\n",
    "        sys.exit()\n",
    "\n",
    "    indnan = np.isnan(datatar) | np.isnan(dataref)\n",
    "    datatar[indnan] = np.nan\n",
    "    dataref[indnan] = np.nan\n",
    "    del indnan\n",
    "\n",
    "    anom = np.ones([nstn, ntime])\n",
    "\n",
    "    for i in range(ntime):\n",
    "        if i < hwsize:\n",
    "            windex = np.arange(hwsize * 2 + 1)\n",
    "        elif i >= ntime - hwsize:\n",
    "            windex = np.arange(ntime - hwsize * 2 - 1, ntime)\n",
    "        else:\n",
    "            windex = np.arange(i - hwsize, i + hwsize + 1)\n",
    "        dtari = np.nanmean(datatar[:, windex], axis=1)\n",
    "        drefi = np.nanmean(dataref[:, windex], axis=1)\n",
    "\n",
    "        if amode == 'ratio':\n",
    "            temp = drefi / dtari\n",
    "            temp[(dtari == 0) & (drefi == 0)] = 1\n",
    "            anom[:, i] = temp\n",
    "        elif amode == 'diff':\n",
    "            anom[:, i] = drefi - dtari\n",
    "        else:\n",
    "            sys.exit('Unknow amode. Please use either ratio or diff')\n",
    "\n",
    "    anom[anom > upbound] = upbound\n",
    "    anom[anom < lowbound] = lowbound\n",
    "    return anom\n",
    "\n",
    "\n",
    "def extrapolation(latin, lonin, datain, latout, lonout, nearnum):\n",
    "    # datain: one or multiple time steps\n",
    "    wexp = 3\n",
    "    if np.ndim(datain) == 1:  # add time axis\n",
    "        datain = datain[:, np.newaxis]\n",
    "    latin[np.isnan(datain[:, 0])] = np.nan\n",
    "    lonin[np.isnan(datain[:, 0])] = np.nan\n",
    "\n",
    "    if np.ndim(latout) == 1: # extrapolate to station points\n",
    "        nearstn_loc, nearstn_dist = findnearstn(latin, lonin, latout, lonout, nearnum, 1)\n",
    "        num = len(latout)\n",
    "        ntimes = np.shape(datain)[1]\n",
    "        dataout = np.zeros([num, ntimes])\n",
    "        for i in range(num):\n",
    "            if np.mod(i,500) == 0:\n",
    "                print(i)\n",
    "            dataini = datain[nearstn_loc[i, :], :]\n",
    "            disti = nearstn_dist[i, :]\n",
    "            weighti = au.distanceweight(disti, np.max(disti) + 1, wexp)\n",
    "            weighti = weighti / np.sum(weighti)\n",
    "            for j in range(ntimes):\n",
    "                dataout[i, j] = np.sum(dataini[:, j] * weighti)\n",
    "\n",
    "    elif np.ndim(latout) == 2: # extrapolate to gridds\n",
    "        nearstn_loc, nearstn_dist = findnearstn(latin, lonin, latout, lonout, nearnum, 0)\n",
    "        nrows, ncols, ntimes = np.shape(datain)\n",
    "        dataout = np.zeros([nrows, ncols, ntimes])\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                dataini = datain[nearstn_loc[r, c, :], :]\n",
    "                disti = nearstn_dist[r, c, :]\n",
    "                weighti = au.distanceweight(disti, np.max(disti) + 1, wexp)\n",
    "                weighti = weighti / np.sum(weighti)\n",
    "                for j in range(ntimes):\n",
    "                    dataout[r, c, j] = np.sum(dataini[:, j] * weighti)\n",
    "    else:\n",
    "        print('The dimensions of tarlat or tarlon are larger than 2')\n",
    "        sys.exit()\n",
    "\n",
    "    return dataout\n",
    "\n",
    "\n",
    "def findnearstn(stnlat, stnlon, tarlat, tarlon, nearnum, noself):\n",
    "    # only use lat/lon to find near stations without considering distance in km\n",
    "    # stnlat/stnlon: 1D\n",
    "    # tarlat/tarlon: 1D or 2D\n",
    "    # noself: 1--stnlat and tarlat have overlapped stations, which should be excluded from stnlat\n",
    "\n",
    "    stnll = np.zeros([len(stnlat), 2])\n",
    "    stnll[:, 0] = stnlat\n",
    "    stnll[:, 1] = stnlon\n",
    "\n",
    "    if len(np.shape(tarlat)) == 1:\n",
    "        num = len(tarlat)\n",
    "        nearstn_loc = np.zeros([num, nearnum],dtype=int)\n",
    "        nearstn_dist = np.zeros([num, nearnum],dtype=float)\n",
    "        for i in range(num):\n",
    "            tari = np.array([tarlat[i], tarlon[i]])\n",
    "            dist = au.distance(tari, stnll)\n",
    "            dist[np.isnan(dist)] = 1000000000\n",
    "            if noself == 1:\n",
    "                dist[dist == 0] = np.inf  # may not be perfect, but work for SCDNA\n",
    "            indi = np.argsort(dist)\n",
    "            nearstn_loc[i, :] = indi[0:nearnum]\n",
    "            nearstn_dist[i, :] = dist[nearstn_loc[i, :]]\n",
    "    elif len(np.shape(tarlat)) == 2:\n",
    "        nrows, ncols = np.shape(tarlat)\n",
    "        nearstn_loc = np.zeros([nrows, ncols, nearnum],dtype=int)\n",
    "        nearstn_dist = np.zeros([nrows, ncols, nearnum],dtype=float)\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                tari = np.array([tarlat[r, c], tarlon[r, c]])\n",
    "                dist = au.distance(tari, stnll)\n",
    "                dist[np.isnan(dist)] = 1000000000\n",
    "                indi = np.argsort(dist)\n",
    "                nearstn_loc[r, c, :] = indi[0:nearnum]\n",
    "                nearstn_dist[r, c, :] = dist[nearstn_loc[r, c, :]]\n",
    "    else:\n",
    "        print('The dimensions of tarlat or tarlon are larger than 2')\n",
    "        sys.exit()\n",
    "\n",
    "    return nearstn_loc, nearstn_dist\n",
    "\n",
    "\n",
    "def error_correction(dataori, anomaly, mode='ratio'):\n",
    "    # default: time is the last dimension\n",
    "    if mode == 'ratio':\n",
    "        datacorr = dataori * anomaly\n",
    "    elif mode == 'diff':\n",
    "        datacorr = dataori + anomaly\n",
    "    else:\n",
    "        sys.exit('Wrong error correction mode')\n",
    "    return datacorr\n",
    "\n",
    "\n",
    "def calweight(obsall, preall, mode='RMSE', preprocess=True):\n",
    "    nstn, ntime = np.shape(obsall)\n",
    "    met = np.zeros(nstn)\n",
    "    for i in range(nstn):\n",
    "        obs = obsall[i, :]\n",
    "        pre = preall[i, :]\n",
    "        if preprocess:\n",
    "            # delete the nan values\n",
    "            ind_nan = np.isnan(obs) | np.isnan(pre)\n",
    "            obs = obs[~ind_nan]\n",
    "            pre = pre[~ind_nan]\n",
    "        if mode == 'RMSE':\n",
    "            met[i] = np.sqrt(np.sum(np.square(obs - pre)) / len(obs))  # RMSE\n",
    "        elif mode == 'CC':\n",
    "            temp = np.corrcoef(obs, pre)\n",
    "            met[i] = temp[0][1]  # CC\n",
    "        else:\n",
    "            sys.exit('Unknown inputs for calmetric')\n",
    "\n",
    "    if mode == 'RMSE':\n",
    "        weight = 1 / (met ** 2)\n",
    "    elif mode == 'CC':\n",
    "        met[met < 0] = 0\n",
    "        weight = met ** 2\n",
    "    else:\n",
    "        sys.exit('Unknown inputs for calmetric')\n",
    "\n",
    "    return weight\n",
    "\n",
    "\n",
    "def calrmse(dtar, dref):\n",
    "    if np.ndim(dtar) == 1:\n",
    "        dtar = dtar[np.newaxis, :]\n",
    "        dref = dref[np.newaxis, :]\n",
    "    nstn, ntimes = np.shape(dtar)\n",
    "    rmse = np.nan * np.zeros(nstn)\n",
    "    for i in range(nstn):\n",
    "        rmse[i] = np.sqrt(np.nansum(np.square(dtar[i, :] - dref[i, :])) / ntimes)  # RMSE\n",
    "\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def ismember(a, b):\n",
    "    # tf = np.in1d(a,b) # for newer versions of numpy\n",
    "    tf = np.array([i in b for i in a])\n",
    "    u = np.unique(a[tf])\n",
    "    index = np.array([(np.where(b == i))[0][-1] if t else 0 for i, t in zip(a, tf)])\n",
    "    return tf, index\n",
    "\n",
    "\n",
    "def weightmerge(data, weight):\n",
    "    if np.ndim(data) == 2:\n",
    "        num, nmodel = np.shape(data)\n",
    "        dataout = np.zeros(num)\n",
    "        for i in range(num):\n",
    "            dataout[i] = np.sum(data[i, :] * weight[i, :]) / np.sum(weight[i, :])\n",
    "    elif np.ndim(data) == 3:\n",
    "        nrows, ncols, nmodel = np.shape(data)\n",
    "        dataout = np.zeros([nrows, ncols])\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if not np.isnan(data[r, c, 0]):\n",
    "                    dataout[r, c] = np.sum(data[r, c, :] * weight[r, c, :]) / np.sum(weight[r, c, :])\n",
    "    return dataout\n",
    "\n",
    "def m_DateList(year_start, year_end, mode):\n",
    "    # generate a date list (yyyymmdd) between start year and end year\n",
    "    # mode: 'ByDay', 'ByMonth', 'ByYear': time scales of input files\n",
    "    date_start = datetime.date(year_start, 1, 1)\n",
    "    date_end = datetime.date(year_end, 12, 31)\n",
    "    daynum = (date_end - date_start).days + 1\n",
    "\n",
    "    # generate date in format: yyyymmdd\n",
    "    date_ymd = np.zeros(daynum, dtype=int)\n",
    "    dated = date_start\n",
    "    for d in range(daynum):\n",
    "        if d > 0:\n",
    "            dated = dated + datetime.timedelta(days=1)\n",
    "        date_ymd[d] = int(dated.strftime(\"%Y%m%d\"))\n",
    "    date_number = {'yyyymmdd': date_ymd,\n",
    "                   'yyyymm': np.floor(date_ymd / 100).astype(int),\n",
    "                   'yyyy': np.floor(date_ymd / 10000).astype(int),\n",
    "                   'mm': np.floor(np.mod(date_ymd, 10000) / 100).astype(int),\n",
    "                   'dd': np.mod(date_ymd, 100).astype(int)}\n",
    "\n",
    "    # generate file list\n",
    "    if mode == 'ByDay':\n",
    "        datemode = date_number['yyyymmdd']\n",
    "    else:\n",
    "        if mode == 'ByMonth':\n",
    "            datemode = date_number['yyyymm']\n",
    "        elif mode == 'ByYear':\n",
    "            datemode = date_number['yyyy']\n",
    "        datemode = np.unique(datemode)\n",
    "\n",
    "    date_list = [' '] * len(datemode)\n",
    "    for i in range(len(datemode)):\n",
    "        date_list[i] = str(datemode[i])\n",
    "\n",
    "    return date_list, date_number\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# basic settings\n",
    "lontar = np.arange(-180 + 0.05, -50, 0.1)\n",
    "lattar = np.arange(85 - 0.05, 5, -0.1)\n",
    "var = 'prcp'   # ['prcp', 'tmean', 'trange']: this should be input from sbtach script\n",
    "corrmode = 'ratio'  # ratio or diff: mode for error correction\n",
    "hwsize = 15  # define time window (2*hwsize+1) used to calculate ratio (as ratio for a specific day is too variable)\n",
    "nearnum = 8  # the number of nearby stations used to extrapolate points to grids (for correction and merging)\n",
    "weightmode = 'RMSE'  # the metric used to guide merging (CC or RMSE). Weight = CC**2 or 1/RMSE**2\n",
    "dividenum = 10  # divide the datasets into X parts, e.g. 10-fold cross-validation\n",
    "anombound = [0.2, 5] # upper and lower bound when calculating the anomaly between target and reference data for correction\n",
    "\n",
    "if corrmode == 'diff':\n",
    "    # default settings in this study since diff is for tmean and trange\n",
    "    hwsize = 0\n",
    "    anombound = [-999, 999]\n",
    "\n",
    "# input files\n",
    "# station list and data\n",
    "# gmet_stnfile = '/home/gut428/GMET/eCAI_EMDNA/StnGridInfo/stnlist_whole.txt'\n",
    "# gmet_stndatafile = '/home/gut428/stndata_whole.npz'\n",
    "gmet_stnfile = '/Users/localuser/GMET/pyGMET_NA/stnlist_whole.txt'\n",
    "gmet_stndatafile = '/Users/localuser/GMET/pyGMET_NA/stndata_whole.npz' # to be saved. only process when absent\n",
    "\n",
    "# downscaled reanalysis data at station points\n",
    "# file_readownstn = ['/ERA5_downto_stn.npz',\n",
    "#                    '/MERRA2_downto_stn.npz',\n",
    "#                    '/JRA55_downto_stn.npz']\n",
    "file_readownstn = ['/Users/localuser/Research/Test/ERA5_downto_stn.npz',\n",
    "                  '/Users/localuser/Research/Test/MERRA2_downto_stn.npz',\n",
    "                  '/Users/localuser/Research/Test/JRA55_downto_stn.npz']\n",
    "\n",
    "# mask file\n",
    "# file_mask = '/datastore/GLOBALWATER/CommonData/EMDNA/DEM/NA_DEM_010deg_trim.mat'\n",
    "file_mask = './DEM/NA_DEM_010deg_trim.mat'\n",
    "\n",
    "# downscaled reanalysis: gridded data\n",
    "path_readown = ['', '', '']\n",
    "prefix = ['ERA5_', 'MERAA2_', 'JRA55_']\n",
    "\n",
    "# output files\n",
    "# train and test index file\n",
    "ttindexfile = '/Users/localuser/Research/Test/2layer_train_test_index.npz'\n",
    "\n",
    "# output corrected and merged data\n",
    "path_reacorr = ['', '', '']\n",
    "path_merge = ''\n",
    "file_error_corr = ['', '', ''] # the error at all station points for corrected reanalysis data (based on cross-validation)\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# basic processing\n",
    "# mask\n",
    "mask = io.loadmat(file_mask)\n",
    "mask = mask['DEM']\n",
    "mask[~np.isnan(mask)] = 1  # 1: valid pixels\n",
    "# attributes\n",
    "reanum = len(file_readownstn)\n",
    "nrows, ncols = np.shape(mask)\n",
    "lontarm, lattarm = np.meshgrid(lontar, lattar)\n",
    "\n",
    "# date\n",
    "date_list, date_number = m_DateList(1979, 2018, 'ByYear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(ttindexfile):\n",
    "    prcp_trainindex1, prcp_testindex1, prcp_trainindex2, prcp_testindex2, \\\n",
    "    tmean_trainindex1, tmean_testindex1, tmean_trainindex2, tmean_testindex2 = double_cvindex(gmet_stndatafile,\n",
    "                                                                                              dividenum)\n",
    "    np.savez_compressed(ttindexfile, prcp_trainindex1=prcp_trainindex1, prcp_testindex1=prcp_testindex1,\n",
    "                        prcp_trainindex2=prcp_trainindex2, prcp_testindex2=prcp_testindex2,\n",
    "                        tmean_trainindex1=tmean_trainindex1, tmean_testindex1=tmean_testindex1,\n",
    "                        tmean_trainindex2=tmean_trainindex2, tmean_testindex2=tmean_testindex2)\n",
    "    del prcp_trainindex1, prcp_testindex1, prcp_trainindex2, prcp_testindex2, \\\n",
    "        tmean_trainindex1, tmean_testindex1, tmean_trainindex2, tmean_testindex2\n",
    "\n",
    "taintestindex = np.load(ttindexfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load downscaled reanalysis for all stations\n",
    "readata_stn = [''] * reanum\n",
    "for rr in range(reanum):\n",
    "    dr = np.load(file_readownstn[rr])\n",
    "    temp = dr[var + '_readown']\n",
    "    if prefix[rr] != 'MERRA2_': # unify the time length of all data as MERRA2 lacks 1979\n",
    "        temp = temp[:,365:]\n",
    "    readata_stn[rr] = temp\n",
    "    del dr\n",
    "\n",
    "# load observations for all stations\n",
    "datatemp = np.load(gmet_stndatafile)\n",
    "stndata = datatemp[var + '_stn']\n",
    "stndata = stndata[:, 365:]\n",
    "stnlle = datatemp['stn_lle']\n",
    "nstn, ntimes = np.shape(stndata)\n",
    "del datatemp\n",
    "\n",
    "# initialize\n",
    "metric_merge_stn = np.nan * np.zeros(nstn) # accuracy metric of merged reanalysis at station points\n",
    "error_merge_stn = np.nan * np.zeros([nstn,ntimes]) # mean error (merge minus observation)\n",
    "metric_reacorr_stn = [''] * reanum # accuracy metric of corrected reanalysis at station points\n",
    "error_reacorr_stn = [''] * reanum\n",
    "for i in range(reanum):\n",
    "    metric_reacorr_stn[i] = np.nan * np.zeros(nstn)\n",
    "    error_reacorr_stn[i] = np.nan * np.zeros([nstn,ntimes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "    lay1=1\n",
    "    # extract train and test index for layer-1\n",
    "    if var == 'trange':\n",
    "        vari = 'tmean'  # trange and tmean have the same index\n",
    "    else:\n",
    "        vari = var\n",
    "    trainindex1 = taintestindex[vari + '_trainindex1'][lay1, :]\n",
    "    testindex1 = taintestindex[vari + '_testindex1'][lay1, :]\n",
    "    stndata_trainl1 = stndata[trainindex1, :]\n",
    "    stndata_testl1 = stndata[testindex1, :]\n",
    "    stnlle_trainl1 = stnlle[trainindex1, :]\n",
    "    stnlle_testl1 = stnlle[testindex1, :]\n",
    "\n",
    "    # merging weight of different reanalysis products at station points (trainindex1)\n",
    "    reacorr_trainl1 = ['']*reanum\n",
    "    for rr in range(reanum):\n",
    "        reacorr_trainl1[rr] = np.zeros(np.shape(stndata_trainl1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "        lay2=3\n",
    "        # extract train and test index for layer-2 (subsets of trainindex1)\n",
    "        trainindex2 = taintestindex[vari + '_trainindex2'][lay1, lay2, :]\n",
    "        testindex2 = taintestindex[vari + '_testindex2'][lay1, lay2, :]\n",
    "        stndata_trainl2 = stndata[trainindex2, :]\n",
    "        stndata_testl2 = stndata[testindex2, :]\n",
    "        stnlle_trainl2 = stnlle[trainindex2, :]\n",
    "        stnlle_testl2 = stnlle[testindex2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:96: RuntimeWarning: Mean of empty slice\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:97: RuntimeWarning: Mean of empty slice\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:100: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:100: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:108: RuntimeWarning: invalid value encountered in greater\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:109: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "        for rr in range(reanum):\n",
    "            readata_trainl2 = readata_stn[rr][trainindex2, :]\n",
    "            readata_testl2 = readata_stn[rr][testindex2, :]\n",
    "\n",
    "            # calculate corrected reanalysis data\n",
    "            # calculate anomaly at the train stations\n",
    "            anom_ori = calculate_anomaly(readata_trainl2, stndata_trainl2, hwsize, corrmode,\n",
    "                                         upbound=anombound[1], lowbound=[0])\n",
    "            # extrapolate the ratio to the test stations (in layer-2)\n",
    "            anom_ext = extrapolation(stnlle_trainl2[:, 0], stnlle_trainl2[:, 1], anom_ori,\n",
    "                                      stnlle_testl2[:, 0], stnlle_testl2[:, 1], nearnum)\n",
    "            # correct data at the test stations\n",
    "            readata_testl2_corr = error_correction(readata_testl2, anom_ext, mode=corrmode)\n",
    "            tf, index = ismember(testindex2, trainindex1)\n",
    "            reacorr_trainl1[rr][index, :] = readata_testl2_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.107183  , 0.55093508, 0.86073827, 0.46299675, 0.61045051,\n",
       "       0.03836566, 0.31159921, 0.46142338, 0.90550796, 0.33012609])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=np.random.rand(10) \n",
    "np.convolve(z, np.ones((1,)) / 1, mode='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91510361, 0.54219097, 0.80540832, 0.94073488, 0.43766869,\n",
       "       0.32288161, 0.00549538, 0.50277364, 0.73863323, 0.37885796])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
