{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start/end year [1979, 2018]\n",
      "Downscale to station: year 1979\n",
      "station 0 27275\n",
      "Downscale to station: year 1980\n",
      "station 0 27275\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import auxiliary as au\n",
    "import regression as reg\n",
    "import datetime as dt\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import io\n",
    "from scipy.interpolate import interp2d\n",
    "import os\n",
    "import sys\n",
    "from scipy.interpolate import griddata\n",
    "import h5py\n",
    "\n",
    "\n",
    "def demread(file, lattar, lontar):\n",
    "    datatemp = io.loadmat(file)\n",
    "    demori = datatemp['DEM']\n",
    "    demori[np.isnan(demori)] = 0\n",
    "    info = datatemp['Info'][0][0]\n",
    "    latori = np.arange(info['yll'] + info['Ysize'] * info['nrows'] - info['Ysize'] / 2, info['yll'], -info['Ysize'])\n",
    "    lonori = np.arange(info['xll'] + info['Xsize'] / 2, info['xll'] + info['Xsize'] * info['ncols'], info['Xsize'])\n",
    "    f = interp2d(lonori, latori, demori, kind='linear')\n",
    "    demtar = f(lontar.flatten(), lattar.flatten())\n",
    "    demtar = np.flipud(demtar)\n",
    "    return demtar\n",
    "\n",
    "\n",
    "def neargrid(rowtar, coltar, rowori, colori, hwsize):\n",
    "    # inputs are 1D matrices\n",
    "    # tar is target area\n",
    "    # ori is original area\n",
    "    # hwsize is half window size (e.g., 4 means the space window width/length is 2*4+1)\n",
    "    # find a space window centering the target grid in the original area and calculate the weights\n",
    "    nrows = len(rowtar)\n",
    "    ncols = len(coltar)\n",
    "    rowse = np.zeros([nrows, ncols, 2]).astype(int)  # se: start/end\n",
    "    colse = np.zeros([nrows, ncols, 2]).astype(int)  # se: start/end\n",
    "    weight = np.nan * np.zeros([nrows, ncols, (hwsize * 2 + 1) ** 2])  # from left to right/from top to bottom weight\n",
    "\n",
    "    for rr in range(nrows):\n",
    "        rowloc = np.argmin(np.abs(rowori - rowtar[rr]))\n",
    "        rowse[rr, :, 0] = rowloc - hwsize\n",
    "        rowse[rr, :, 1] = rowloc + hwsize\n",
    "\n",
    "    for cc in range(ncols):\n",
    "        colloc = np.argmin(np.abs(colori - coltar[cc]))\n",
    "        colse[:, cc, 0] = colloc - hwsize\n",
    "        colse[:, cc, 1] = colloc + hwsize\n",
    "\n",
    "    rowse[rowse < 0] = 0\n",
    "    rowse[rowse > nrows] = nrows\n",
    "    colse[colse < 0] = 0\n",
    "    colse[colse > ncols] = nrows\n",
    "\n",
    "    maxdist = (hwsize + 0.5) * np.sqrt(2) + 0.5\n",
    "    for rr in range(nrows):\n",
    "        rowloc = np.argmin(np.abs(rowori - rowtar[rr]))\n",
    "        for cc in range(ncols):\n",
    "            colloc = np.argmin(np.abs(colori - coltar[cc]))\n",
    "\n",
    "            rowse_rc = rowse[rr, cc, :]\n",
    "            colse_rc = colse[rr, cc, :]\n",
    "            flag = 0\n",
    "            for i in range(rowse_rc[0], rowse_rc[1] + 1):\n",
    "                for j in range(colse_rc[0], colse_rc[1] + 1):\n",
    "                    dist = ((rowloc - i) ** 2 + (colloc - j) ** 2) ** 0.5\n",
    "                    weight[rr, cc, flag] = au.distanceweight(dist, maxdist, 3)\n",
    "                    flag = flag + 1\n",
    "\n",
    "            weight[rr, cc, :] = weight[rr, cc, :] / np.nansum(weight[rr, cc, :])\n",
    "\n",
    "    return rowse, colse, weight\n",
    "\n",
    "\n",
    "def readownscale(dataori, latori, lonori, demori, lattar, lontar, demtar, rowse, colse, weight, mask):\n",
    "    nrows = len(lattar)\n",
    "    ncols = len(lontar)\n",
    "    ntimes = np.shape(dataori)[2]\n",
    "    lonori, latori = np.meshgrid(lonori, latori)\n",
    "    datatar = np.nan * np.zeros([nrows, ncols, ntimes])\n",
    "\n",
    "    for rr in range(nrows):\n",
    "        for cc in range(ncols):\n",
    "            if mask[rr, cc] == 1:\n",
    "                rloc = rowse[rr, cc, :]\n",
    "                cloc = colse[rr, cc, :]\n",
    "                latnear = latori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "                lonnear = lonori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "                demnear = demori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "                nnum = np.size(latnear)\n",
    "                latnear = np.reshape(latnear, nnum)\n",
    "                lonnear = np.reshape(lonnear, nnum)\n",
    "                demnear = np.reshape(demnear, nnum)\n",
    "                weightnear = np.zeros([nnum, nnum])\n",
    "                for i in range(nnum):\n",
    "                    weightnear[i, i] = weight[rr, cc, i]\n",
    "\n",
    "                nearinfo = np.zeros([nnum, 4])\n",
    "                nearinfo[:, 0] = 1\n",
    "                nearinfo[:, 1] = latnear\n",
    "                nearinfo[:, 2] = lonnear\n",
    "                nearinfo[:, 3] = demnear\n",
    "\n",
    "                tarinfo = np.zeros(4)\n",
    "                tarinfo[0] = 1\n",
    "                tarinfo[1] = lattar[rr]\n",
    "                tarinfo[2] = lontar[cc]\n",
    "                tarinfo[3] = demtar[rr, cc]\n",
    "\n",
    "                tx_red = np.transpose(nearinfo)\n",
    "                twx_red = np.matmul(tx_red, weightnear)\n",
    "\n",
    "                for tt in range(ntimes):\n",
    "                    datanear = dataori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1, tt]\n",
    "                    datanear = np.reshape(datanear, nnum)\n",
    "\n",
    "                    # upper and lower boundary for the downscaled data\n",
    "                    # this is a conservative limitation\n",
    "                    lowbound = np.min(datanear)\n",
    "                    upbound = np.max(datanear)\n",
    "\n",
    "                    b = reg.least_squares(nearinfo, datanear, twx_red)\n",
    "                    datatemp = np.dot(tarinfo, b)\n",
    "                    if np.all(b == 0) or datatemp > upbound or datatemp < lowbound:\n",
    "                        # use nearest neighbor interpolation\n",
    "                        weightnear = weight[rr, cc, 0:nnum]\n",
    "                        mloc = np.argmax(weightnear)\n",
    "                        datatar[rr, cc, tt] = datanear[mloc]\n",
    "                    else:\n",
    "                        datatar[rr, cc, tt] = datatemp\n",
    "    return datatar\n",
    "\n",
    "\n",
    "def readownscale_tostn(dataori, latori, lonori, demori, lattar, lontar, demtar, rowse, colse, weight, stn_row, stn_col,\n",
    "                       data0):\n",
    "    nstn = len(stn_row)\n",
    "    ntimes = np.shape(dataori)[2]\n",
    "    lonori, latori = np.meshgrid(lonori, latori)\n",
    "    datatar = np.nan * np.zeros([nstn, ntimes])\n",
    "\n",
    "    for gg in range(10):\n",
    "        if np.mod(gg, 5000) == 0:\n",
    "            print('station', gg, nstn)\n",
    "\n",
    "        if np.isnan(data0[gg]):\n",
    "            continue  # station does not have observations, thus does not need downscaling\n",
    "\n",
    "        rr = stn_row[gg]\n",
    "        cc = stn_col[gg]\n",
    "        rloc = rowse[rr, cc, :]\n",
    "        cloc = colse[rr, cc, :]\n",
    "        latnear = latori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "        lonnear = lonori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "        demnear = demori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "        nnum = np.size(latnear)\n",
    "        latnear = np.reshape(latnear, nnum)\n",
    "        lonnear = np.reshape(lonnear, nnum)\n",
    "        demnear = np.reshape(demnear, nnum)\n",
    "        weightnear = np.zeros([nnum, nnum])\n",
    "        for i in range(nnum):\n",
    "            weightnear[i, i] = weight[rr, cc, i]\n",
    "\n",
    "        nearinfo = np.zeros([nnum, 4])\n",
    "        nearinfo[:, 0] = 1\n",
    "        nearinfo[:, 1] = latnear\n",
    "        nearinfo[:, 2] = lonnear\n",
    "        nearinfo[:, 3] = demnear\n",
    "\n",
    "        tarinfo = np.zeros(4)\n",
    "        tarinfo[0] = 1\n",
    "        tarinfo[1] = lattar[rr]\n",
    "        tarinfo[2] = lontar[cc]\n",
    "        tarinfo[3] = demtar[rr, cc]\n",
    "\n",
    "        tx_red = np.transpose(nearinfo)\n",
    "        twx_red = np.matmul(tx_red, weightnear)\n",
    "\n",
    "        for tt in range(ntimes):\n",
    "            datanear = dataori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1, tt]\n",
    "            datanear = np.reshape(datanear, nnum)\n",
    "\n",
    "            # upper and lower boundary for the downscaled data\n",
    "            # this is a conservative limitation\n",
    "            lowbound = np.min(datanear)\n",
    "            upbound = np.max(datanear)\n",
    "\n",
    "            b = reg.least_squares(nearinfo, datanear, twx_red)\n",
    "            datatemp = np.dot(tarinfo, b)\n",
    "            if np.all(b == 0) or datatemp > upbound or datatemp < lowbound:\n",
    "                # use nearest neighbor interpolation\n",
    "                weightnear = weight[rr, cc, 0:nnum]\n",
    "                mloc = np.argmax(weightnear)\n",
    "                datatar[gg, tt] = datanear[mloc]\n",
    "            else:\n",
    "                datatar[gg, tt] = datatemp\n",
    "    return datatar\n",
    "\n",
    "\n",
    "def readstndata(inpath, stnID, ndays):\n",
    "    nstn = len(stnID)\n",
    "    prcp_stn = np.nan * np.zeros([nstn, ndays])\n",
    "    tmin_stn = np.nan * np.zeros([nstn, ndays])\n",
    "    tmax_stn = np.nan * np.zeros([nstn, ndays])\n",
    "\n",
    "    for i in range(nstn):\n",
    "        if np.mod(i, 1000) == 0:\n",
    "            print('station', i, nstn)\n",
    "        file = inpath + '/' + stnID[i] + '.nc'\n",
    "        fid = nc.Dataset(file)\n",
    "        varlist = fid.variables.keys()\n",
    "        if 'prcp' in varlist:\n",
    "            prcp_stn[i, :] = fid['prcp'][:].data\n",
    "        if 'tmin' in varlist:\n",
    "            tmin_stn[i, :] = fid['tmin'][:].data\n",
    "        if 'tmax' in varlist:\n",
    "            tmax_stn[i, :] = fid['tmax'][:].data\n",
    "        fid.close()\n",
    "\n",
    "    tmean_stn = (tmin_stn + tmax_stn) / 2\n",
    "    trange_stn = np.abs(tmax_stn - tmin_stn)\n",
    "\n",
    "    return prcp_stn, tmean_stn, trange_stn\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# time periods: inside or outside\n",
    "# outside\n",
    "# a = int(sys.argv[1])\n",
    "# b = int(sys.argv[2])\n",
    "# year = [a, b]\n",
    "# inside\n",
    "year = [1979, 2018]\n",
    "print('start/end year', year)\n",
    "########################################################################################################################\n",
    "\n",
    "# basic information: be set before running\n",
    "# mac\n",
    "filedem = './DEM/NA_DEM_010deg_trim.mat'\n",
    "# plato\n",
    "# filedem = '/datastore/GLOBALWATER/CommonData/EMDNA/DEM/NA_DEM_010deg_trim.mat'\n",
    "vars = ['prcp', 'tmin', 'tmax']\n",
    "lontar = np.arange(-180 + 0.05, -50, 0.1)\n",
    "lattar = np.arange(85 - 0.05, 5, -0.1)\n",
    "hwsize = 2  # use (2*2+1)**2 grids to perform regression\n",
    "\n",
    "# station information\n",
    "# mac\n",
    "gmet_stnfile = '/Users/localuser/GMET/pyGMET_NA/stnlist_whole.txt'\n",
    "gmet_stnpath = '/Users/localuser/GMET/StnInput_daily'\n",
    "gmet_stndatafile = '/Users/localuser/GMET/pyGMET_NA/stndata_whole.npz' # to be saved. only process when absent\n",
    "# plato\n",
    "# gmet_stnfile = '/home/gut428/GMET/eCAI_EMDNA/StnGridInfo/stnlist_whole.txt'\n",
    "# gmet_stnpath = '/home/gut428/GMET/StnInput_daily'\n",
    "# gmet_stndatafile = '/home/gut428/stndata_whole.npz'  # to be saved. only process when absent\n",
    "\n",
    "# reanalysis path: ERA-5\n",
    "# mac\n",
    "filedem_era = './DEM/ERA5_DEM2.mat'\n",
    "inpath = '/Users/localuser/Research/Test'\n",
    "outpath = '/Users/localuser/Research'\n",
    "# plato\n",
    "# filedem_era = '/datastore/GLOBALWATER/CommonData/EMDNA/DEM/ERA5_DEM2.mat'\n",
    "# inpath = '/datastore/GLOBALWATER/CommonData/EMDNA/ERA5_day_raw'  # downscale to 0.1 degree\n",
    "# outpath = '/home/gut428/ERA5_day_ds'\n",
    "file_readownstn = outpath + '/ERA5_downto_stn.npz'  # downscale to station points (1979-2018)\n",
    "filenear = outpath + '/weight_dem.npz'\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# read some basic infomation\n",
    "datatemp = io.loadmat(filedem)\n",
    "demtar = datatemp['DEM']  # this is consistent with lontar lattar\n",
    "mask = demtar.copy()\n",
    "mask[~np.isnan(mask)] = 1\n",
    "\n",
    "stn_ID = np.genfromtxt(gmet_stnfile, dtype='str', skip_header=1, comments='#', delimiter=',', usecols=(0), unpack=False)\n",
    "stn_lle = np.loadtxt(gmet_stnfile, dtype=float, skiprows=1, comments='#', delimiter=',', usecols=(1, 2, 3),\n",
    "                     unpack=False)\n",
    "stn_row = ((85 - stn_lle[:, 0]) / 0.1).astype(int)\n",
    "stn_col = ((stn_lle[:, 1] + 180) / 0.1).astype(int)\n",
    "nstn = len(stn_ID)\n",
    "ndays = 14610  # days from 1979 to 2018\n",
    "\n",
    "# read all station data and save to facilitate analysis in the future\n",
    "if not os.path.isfile(gmet_stndatafile):\n",
    "    prcp_stn, tmean_stn, trange_stn = readstndata(gmet_stnpath, stn_ID, ndays)\n",
    "    prcp_stn = np.float32(prcp_stn)\n",
    "    tmean_stn = np.float32(tmean_stn)\n",
    "    trange_stn = np.float32(trange_stn)\n",
    "    np.savez_compressed(gmet_stndatafile, prcp_stn=prcp_stn, tmean_stn=tmean_stn, trange_stn=trange_stn,\n",
    "                        stn_ID=stn_ID, stn_lle=stn_lle, stn_row=stn_row, stn_col=stn_col)\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# downscale reanalysis to 0.1 degree\n",
    "# for y in range(year[0], year[1] + 1):\n",
    "#     for v in range(len(vars)):\n",
    "#         print('year--var:', y, vars[v])\n",
    "#         infile = inpath + '/ERA5_' + vars[v] + '_' + str(y) + '.mat'\n",
    "#         outfile_grid = outpath + '/ERA5_' + vars[v] + '_' + str(y) + '.npz'\n",
    "#         if os.path.isfile(outfile_grid):\n",
    "#             continue\n",
    "#\n",
    "#         # load original daily reanalysis data\n",
    "#         datatemp = {}\n",
    "#         f = h5py.File(infile, 'r')\n",
    "#         for k, v in f.items():\n",
    "#             datatemp[k] = np.array(v)\n",
    "#         latori = datatemp['latitude'][0]\n",
    "#         lonori = datatemp['longitude'][0]\n",
    "#         dataori = datatemp['data']\n",
    "#         dataori = np.transpose(dataori, [2, 1, 0])\n",
    "#         del datatemp\n",
    "#         f.close()\n",
    "#\n",
    "#         # read location information\n",
    "#         if not os.path.isfile(filenear):\n",
    "#             rowse, colse, weight = neargrid(lattar, lontar, latori, lonori, hwsize)\n",
    "#             # extract ori dem\n",
    "#             demori = demread(filedem_era, latori, lonori)\n",
    "#             io.savemat(filenear, {'rowse': rowse, 'colse': colse, 'weight': weight, 'demori': demori})\n",
    "#         else:\n",
    "#             datatemp = io.loadmat(filenear)\n",
    "#             rowse = datatemp['rowse']\n",
    "#             colse = datatemp['colse']\n",
    "#             weight = datatemp['weight']\n",
    "#             demori = datatemp['demori']\n",
    "#             del datatemp\n",
    "#\n",
    "#         # downscale the reanalysis to 0.1 degree\n",
    "#         datatar = readownscale(dataori, latori, lonori, demori, lattar, lontar, demtar, rowse, colse, weight, mask)\n",
    "#         datatar = np.float32(datatar)\n",
    "#         np.savez_compressed(outfile_grid, data=datatar, latitude=lattar, longitude=lontar)\n",
    "\n",
    "########################################################################################################################\n",
    "# downscale to station points\n",
    "# original station data\n",
    "datatemp = np.load(gmet_stndatafile)\n",
    "prcp_stn0 = datatemp['prcp_stn'][:, 0]\n",
    "tmean_stn0 = datatemp['tmean_stn'][:, 0]\n",
    "del datatemp\n",
    "\n",
    "if not os.path.isfile(file_readownstn):\n",
    "    prcp_readown = np.float32(np.nan * np.zeros([nstn, ndays]))\n",
    "    tmean_readown = np.float32(np.nan * np.zeros([nstn, ndays]))\n",
    "    trange_readown = np.float32(np.nan * np.zeros([nstn, ndays]))\n",
    "\n",
    "    # load nearby grid information\n",
    "    datatemp = io.loadmat(filenear)\n",
    "    rowse = datatemp['rowse']\n",
    "    colse = datatemp['colse']\n",
    "    weight = datatemp['weight']\n",
    "    demori = datatemp['demori']\n",
    "\n",
    "    flag = 0\n",
    "    for y in range(1979, 1981):\n",
    "        print('Downscale to station: year', y)\n",
    "        # prcp\n",
    "        infile = inpath + '/ERA5_prcp_' + str(y) + '.mat'\n",
    "        datatemp = {}\n",
    "        f = h5py.File(infile, 'r')\n",
    "        for k, v in f.items():\n",
    "            datatemp[k] = np.array(v)\n",
    "        latori = datatemp['latitude'][0]\n",
    "        lonori = datatemp['longitude'][0]\n",
    "        dataori = datatemp['data']\n",
    "        dataori = np.transpose(dataori, [2, 1, 0])\n",
    "        del datatemp\n",
    "        f.close()\n",
    "        prcptar = readownscale_tostn(dataori, latori, lonori, demori, lattar, lontar, demtar, rowse, colse, weight,\n",
    "                                     stn_row, stn_col, prcp_stn0)\n",
    "        prcptar = np.float32(prcptar)\n",
    "\n",
    "        # tmin\n",
    "#         infile = inpath + '/ERA5_tmin_' + str(y) + '.mat'\n",
    "#         datatemp = {}\n",
    "#         f = h5py.File(infile, 'r')\n",
    "#         for k, v in f.items():\n",
    "#             datatemp[k] = np.array(v)\n",
    "#         latori = datatemp['latitude'][0]\n",
    "#         lonori = datatemp['longitude'][0]\n",
    "#         dataori = datatemp['data']\n",
    "#         dataori = np.transpose(dataori, [2, 1, 0])\n",
    "#         del datatemp\n",
    "#         f.close()\n",
    "#         tmintar = readownscale_tostn(dataori, latori, lonori, demori, lattar, lontar, demtar, rowse, colse, weight,\n",
    "#                                      stn_row, stn_col, tmean_stn0)\n",
    "#         tmintar = np.float32(tmintar)\n",
    "\n",
    "        # tmax\n",
    "#         infile = inpath + '/ERA5_tmax_' + str(y) + '.mat'\n",
    "#         datatemp = {}\n",
    "#         f = h5py.File(infile, 'r')\n",
    "#         for k, v in f.items():\n",
    "#             datatemp[k] = np.array(v)\n",
    "#         latori = datatemp['latitude'][0]\n",
    "#         lonori = datatemp['longitude'][0]\n",
    "#         dataori = datatemp['data']\n",
    "#         dataori = np.transpose(dataori, [2, 1, 0])\n",
    "#         del datatemp\n",
    "#         f.close()\n",
    "#         tmaxtar = readownscale_tostn(dataori, latori, lonori, demori, lattar, lontar, demtar, rowse, colse, weight,\n",
    "#                                      stn_row, stn_col, tmean_stn0)\n",
    "#         tmaxtar = np.float32(tmaxtar)\n",
    "\n",
    "        # merge\n",
    "        dayy = np.shape(prcptar)[1]\n",
    "        prcp_readown[:, flag:flag + dayy] = prcptar\n",
    "#         tmean_readown[:, :, flag:flag + dayy] = (tmintar + tmaxtar) / 2\n",
    "#         trange_readown[:, :, flag:flag + dayy] = np.abs(tmaxtar - tmintar)\n",
    "        flag = flag + dayy\n",
    "\n",
    "#     np.savez_compressed(file_readownstn, prcp_readown=prcp_readown, tmean_readown=tmean_readown,\n",
    "#                         trange_readown=trange_readown,\n",
    "#                         latitude=lattar, longitude=lontar, stn_ID=stn_ID, stn_lle=stn_lle, stn_row=stn_row,\n",
    "#                         stn_col=stn_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(a[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2]), array([    0, -2323,     2]), array([0, 1, 2])]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=[1]*3\n",
    "for i in range(3):\n",
    "    z[i] = np.arange(3)\n",
    "z[1][1]=-2323\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
