{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start/end year [2018, 2018]\n",
      "downtostn_method GWR\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import auxiliary as au\n",
    "import regression as reg\n",
    "import datetime as dt\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import io\n",
    "from scipy.interpolate import interp2d\n",
    "import os\n",
    "import sys\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "from scipy import interpolate\n",
    "import h5py\n",
    "from auxiliary_merge import m_DateList\n",
    "\n",
    "\n",
    "def dist2coast_cal(filedist2coast, xynew):\n",
    "    d = np.load(filedist2coast)\n",
    "    latdist = d['latdist']\n",
    "    londist = d['londist']\n",
    "    dist2coast = d['dist2coast']\n",
    "    xynew[xynew[:, 0] < np.min(latdist), 0] = np.min(latdist)\n",
    "    xynew[xynew[:, 0] > np.max(latdist), 0] = np.max(latdist)\n",
    "    xynew[xynew[:, 1] < np.min(londist), 1] = np.min(londist)\n",
    "    xynew[xynew[:, 1] > np.max(londist), 1] = np.max(londist)\n",
    "\n",
    "    latdist = np.flipud(latdist)  # ascending latitude\n",
    "    dist2coast = np.flipud(dist2coast)\n",
    "    rg = RegularGridInterpolator((latdist, londist), dist2coast, method='linear')\n",
    "    distnew = rg(xynew)\n",
    "    return distnew\n",
    "\n",
    "\n",
    "def demread(file, lattar, lontar):\n",
    "    datatemp = io.loadmat(file)\n",
    "    demori = datatemp['DEM']\n",
    "    ratio = datatemp['pixelratio']\n",
    "    demori[ratio < 0.5] = 0  # ocean pixels\n",
    "    demori[np.isnan(demori)] = 0\n",
    "    info = datatemp['Info'][0][0]\n",
    "    latori = np.arange(info['yll'] + info['Ysize'] * info['nrows'] - info['Ysize'] / 2, info['yll'], -info['Ysize'])\n",
    "    lonori = np.arange(info['xll'] + info['Xsize'] / 2, info['xll'] + info['Xsize'] * info['ncols'], info['Xsize'])\n",
    "    f = interp2d(lonori, latori, demori, kind='linear')\n",
    "    demtar = f(lontar.flatten(), lattar.flatten())\n",
    "    demtar = np.flipud(demtar)\n",
    "    return demtar\n",
    "\n",
    "\n",
    "def neargrid(rowtar, coltar, rowori, colori, hwsize):\n",
    "    # inputs are 1D matrices\n",
    "    # tar is target area\n",
    "    # ori is original area\n",
    "    # hwsize is half window size (e.g., 4 means the space window width/length is 2*4+1)\n",
    "    # find a space window centering the target grid in the original area and calculate the weights\n",
    "    nrows = len(rowtar)\n",
    "    ncols = len(coltar)\n",
    "    rowse = np.zeros([nrows, ncols, 2]).astype(int)  # se: start/end\n",
    "    colse = np.zeros([nrows, ncols, 2]).astype(int)  # se: start/end\n",
    "    weight = np.nan * np.zeros([nrows, ncols, (hwsize * 2 + 1) ** 2])  # from left to right/from top to bottom weight\n",
    "\n",
    "    for rr in range(nrows):\n",
    "        rowloc = np.argmin(np.abs(rowori - rowtar[rr]))\n",
    "        rowse[rr, :, 0] = rowloc - hwsize\n",
    "        rowse[rr, :, 1] = rowloc + hwsize\n",
    "\n",
    "    for cc in range(ncols):\n",
    "        colloc = np.argmin(np.abs(colori - coltar[cc]))\n",
    "        colse[:, cc, 0] = colloc - hwsize\n",
    "        colse[:, cc, 1] = colloc + hwsize\n",
    "\n",
    "    rowse[rowse < 0] = 0\n",
    "    rowse[rowse > nrows] = nrows\n",
    "    colse[colse < 0] = 0\n",
    "    colse[colse > ncols] = nrows\n",
    "\n",
    "    maxdist = (hwsize + 0.5) * np.sqrt(2) + 0.5\n",
    "    for rr in range(nrows):\n",
    "        rowloc = np.argmin(np.abs(rowori - rowtar[rr]))\n",
    "        for cc in range(ncols):\n",
    "            colloc = np.argmin(np.abs(colori - coltar[cc]))\n",
    "\n",
    "            rowse_rc = rowse[rr, cc, :]\n",
    "            colse_rc = colse[rr, cc, :]\n",
    "            flag = 0\n",
    "            for i in range(rowse_rc[0], rowse_rc[1] + 1):\n",
    "                for j in range(colse_rc[0], colse_rc[1] + 1):\n",
    "                    dist = ((rowloc - i) ** 2 + (colloc - j) ** 2) ** 0.5\n",
    "                    weight[rr, cc, flag] = au.distanceweight(dist, maxdist, 3)\n",
    "                    flag = flag + 1\n",
    "\n",
    "            weight[rr, cc, :] = weight[rr, cc, :] / np.nansum(weight[rr, cc, :])\n",
    "\n",
    "    return rowse, colse, weight\n",
    "\n",
    "\n",
    "def readownscale_grid_GWR(dataori, latori, lonori, demori, dist2coast_ori, lattar, lontar, demtar, dist2coast_tar,\n",
    "                          rowse, colse, weight, mask):\n",
    "    nrows = len(lattar)\n",
    "    ncols = len(lontar)\n",
    "    ntimes = np.shape(dataori)[2]\n",
    "    lonori, latori = np.meshgrid(lonori, latori)\n",
    "    datatar = np.nan * np.zeros([nrows, ncols, ntimes])\n",
    "\n",
    "    for rr in range(nrows):\n",
    "        print('row',rr,nrows)\n",
    "        for cc in range(ncols):\n",
    "            if mask[rr, cc] == 1:\n",
    "                rloc = rowse[rr, cc, :]\n",
    "                cloc = colse[rr, cc, :]\n",
    "                midr = int((rloc[0] + rloc[1]) / 2)\n",
    "                midc = int((cloc[0] + cloc[1]) / 2)\n",
    "\n",
    "                latnear = latori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "                lonnear = lonori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "                demnear = demori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "                distnear = dist2coast_ori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "\n",
    "                nnum = np.size(latnear)\n",
    "                latnear = np.reshape(latnear, nnum)\n",
    "                lonnear = np.reshape(lonnear, nnum)\n",
    "                demnear = np.reshape(demnear, nnum)\n",
    "                distnear = np.reshape(distnear, nnum)\n",
    "                weightnear = np.zeros([nnum, nnum])\n",
    "                for i in range(nnum):\n",
    "                    weightnear[i, i] = weight[rr, cc, i]\n",
    "\n",
    "                if demori[midr, midc] == 0:\n",
    "                    induse = demnear == 0  # if the station is within a land grid, exclude ocean grids from nearby grids\n",
    "                else:\n",
    "                    induse = demnear != 0  # if the station is within a ocean grid, exclude land grids from nearby grids\n",
    "\n",
    "                if np.sum(induse) < 10:\n",
    "                    # few nearby grids can be used for regression, thus will just use nearest neighbor\n",
    "                    datatar[rr, cc, :] = dataori[midr, midc, :]\n",
    "                    continue\n",
    "\n",
    "                nearinfo = np.zeros([nnum, 5])\n",
    "                nearinfo[:, 0] = 1\n",
    "                nearinfo[:, 1] = latnear\n",
    "                nearinfo[:, 2] = lonnear\n",
    "                nearinfo[:, 3] = demnear\n",
    "                nearinfo[:, 4] = distnear\n",
    "                nearinfo = nearinfo[induse, :]\n",
    "                weightnear = weightnear[induse, :]\n",
    "                weightnear = weightnear[:, induse]\n",
    "\n",
    "                tarinfo = np.zeros(5)\n",
    "                tarinfo[0] = 1\n",
    "                tarinfo[1] = lattar[rr]\n",
    "                tarinfo[2] = lontar[cc]\n",
    "                tarinfo[3] = demtar[rr, cc]\n",
    "                tarinfo[4] = dist2coast_tar[rr, cc]\n",
    "\n",
    "                tx_red = np.transpose(nearinfo)\n",
    "                twx_red = np.matmul(tx_red, weightnear)\n",
    "\n",
    "                for tt in range(ntimes):\n",
    "                    datanear = dataori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1, tt]\n",
    "                    datanear = np.reshape(datanear, nnum)\n",
    "                    datanear = datanear[induse]\n",
    "\n",
    "                    # upper and lower boundary for the downscaled data\n",
    "                    # this is a conservative limitation\n",
    "                    lowbound = np.min(datanear)\n",
    "                    upbound = np.max(datanear)\n",
    "                    if np.sign(lowbound) == 1:\n",
    "                        lowbound = lowbound * 0.8\n",
    "                    else:\n",
    "                        lowbound = lowbound * 1.2\n",
    "                    if np.sign(upbound) == 1:\n",
    "                        upbound = upbound * 1.2\n",
    "                    else:\n",
    "                        upbound = upbound * 0.8\n",
    "\n",
    "                    b = reg.least_squares(nearinfo, datanear, twx_red)\n",
    "                    datatemp = np.dot(tarinfo, b)\n",
    "                    if np.all(b == 0) or datatemp > upbound or datatemp < lowbound or np.isnan(datatemp):\n",
    "                        datatar[rr, cc, tt] = dataori[midr, midc, tt]\n",
    "                    else:\n",
    "                        datatar[rr, cc, tt] = datatemp\n",
    "    return datatar\n",
    "\n",
    "\n",
    "def readownscale_tostn(dataori, latori, lonori, demori, dist2coast_ori, rowse, colse, weight, stn_row, stn_col,\n",
    "                       data0, method, stn_lle, dist2coast_stn, tlr):\n",
    "    nstn = len(stn_row)\n",
    "    ntimes = np.shape(dataori)[2]\n",
    "    datatar = np.nan * np.zeros([nstn, ntimes], dtype=np.float32)\n",
    "\n",
    "    if method == 'linear' or method == 'nearest':\n",
    "        xynew = stn_lle[:, [0, 1]]\n",
    "        latori = np.flipud(latori)  # ascending latitude\n",
    "        dataori = np.flipud(dataori)\n",
    "        for i in range(ntimes):\n",
    "            # print('Time step:',i,ntimes)\n",
    "            rg = RegularGridInterpolator((latori, lonori), dataori[:, :, i], method=method)\n",
    "            datatar[:, i] = rg(xynew)\n",
    "\n",
    "    elif method == 'GWR':\n",
    "        lonori, latori = np.meshgrid(lonori, latori)\n",
    "        for gg in range(nstn):\n",
    "            if np.mod(gg, 5000) == 0:\n",
    "                print('station', gg, nstn)\n",
    "\n",
    "            if np.isnan(data0[gg]):\n",
    "                continue  # station does not have observations, thus does not need downscaling\n",
    "\n",
    "            rr = stn_row[gg]\n",
    "            cc = stn_col[gg]\n",
    "            rloc = rowse[rr, cc, :]\n",
    "            cloc = colse[rr, cc, :]\n",
    "            midr = int((rloc[0] + rloc[1]) / 2)\n",
    "            midc = int((cloc[0] + cloc[1]) / 2)\n",
    "            latnear = latori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "            lonnear = lonori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "            demnear = demori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "            distnear = dist2coast_ori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "\n",
    "            nnum = np.size(latnear)\n",
    "            latnear = np.reshape(latnear, nnum)\n",
    "            lonnear = np.reshape(lonnear, nnum)\n",
    "            demnear = np.reshape(demnear, nnum)\n",
    "            distnear = np.reshape(distnear, nnum)\n",
    "            weightnear = np.zeros([nnum, nnum])\n",
    "            for i in range(nnum):\n",
    "                weightnear[i, i] = weight[rr, cc, i]\n",
    "\n",
    "            if demori[midr, midc] == 0:\n",
    "                induse = demnear == 0  # if the station is within a land grid, exclude ocean grids from nearby grids\n",
    "            else:\n",
    "                induse = demnear != 0  # if the station is within a ocean grid, exclude land grids from nearby grids\n",
    "\n",
    "            if np.sum(induse) < 10:\n",
    "                # few nearby grids can be used for regression, thus will just use nearest neighbor\n",
    "                datatar[gg, :] = dataori[midr, midc, :]\n",
    "                continue\n",
    "\n",
    "            nearinfo = np.zeros([nnum, 5])\n",
    "            nearinfo[:, 0] = 1\n",
    "            nearinfo[:, 1] = latnear\n",
    "            nearinfo[:, 2] = lonnear\n",
    "            nearinfo[:, 3] = demnear\n",
    "            nearinfo[:, 4] = distnear\n",
    "            nearinfo = nearinfo[induse, :]\n",
    "            weightnear = weightnear[induse, :]\n",
    "            weightnear = weightnear[:, induse]\n",
    "\n",
    "            tarinfo = np.zeros(5)\n",
    "            tarinfo[0] = 1\n",
    "            tarinfo[1] = stn_lle[gg, 0]\n",
    "            tarinfo[2] = stn_lle[gg, 1]\n",
    "            tarinfo[3] = stn_lle[gg, 2]\n",
    "            tarinfo[4] = dist2coast_stn[gg]\n",
    "\n",
    "            tx_red = np.transpose(nearinfo)\n",
    "            twx_red = np.matmul(tx_red, weightnear)\n",
    "\n",
    "            for tt in range(ntimes):\n",
    "                datanear = dataori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1, tt]\n",
    "                datanear = np.reshape(datanear, nnum)\n",
    "                datanear = datanear[induse]\n",
    "\n",
    "                # upper and lower boundary for the downscaled data\n",
    "                # this is a conservative limitation\n",
    "                lowbound = np.min(datanear)\n",
    "                upbound = np.max(datanear)\n",
    "                if np.sign(lowbound) == 1:\n",
    "                    lowbound = lowbound * 0.8\n",
    "                else:\n",
    "                    lowbound = lowbound * 1.2\n",
    "                if np.sign(upbound) == 1:\n",
    "                    upbound = upbound * 1.2\n",
    "                else:\n",
    "                    upbound = upbound * 0.8\n",
    "\n",
    "                b = reg.least_squares(nearinfo, datanear, twx_red)\n",
    "                datatemp = np.dot(tarinfo, b)\n",
    "                if np.all(b == 0) or datatemp > upbound or datatemp < lowbound or np.isnan(datatemp):\n",
    "                    datatar[gg, tt] = dataori[midr, midc, tt]\n",
    "                else:\n",
    "                    datatar[gg, tt] = datatemp\n",
    "\n",
    "    elif method == 'TLR1' or method == 'TLR2':\n",
    "        # downscale original data to sea level\n",
    "        demori2 = demori[:, :, np.newaxis] / 1000\n",
    "        demori2 = np.tile(demori2, [1, 1, ntimes])\n",
    "        dataori = dataori - demori2 * tlr\n",
    "\n",
    "        # linearly interpolated dataori to stations\n",
    "        xynew = stn_lle[:, [0, 1]]\n",
    "        latori = np.flipud(latori)  # ascending latitude\n",
    "        dataori = np.flipud(dataori)\n",
    "        for i in range(ntimes):\n",
    "            # print('Time step:',i,ntimes)\n",
    "            rg = RegularGridInterpolator((latori, lonori), dataori[:, :, i], method='linear')\n",
    "            datatar[:, i] = rg(xynew)\n",
    "\n",
    "        # recover from sea level to actual elevation\n",
    "        for i in range(nstn):\n",
    "            rowi = np.argmin(abs(stn_lle[i, 0] - latori))\n",
    "            coli = np.argmin(abs(stn_lle[i, 1] - lonori))\n",
    "            tlri = tlr[rowi, coli, :]\n",
    "            datatar[i, :] = datatar[i, :] + stn_lle[i, 2] / 1000 * tlri\n",
    "    else:\n",
    "        sys.exit('Unknown downscaling method')\n",
    "    return datatar\n",
    "\n",
    "\n",
    "def readstndata(inpath_raw, stnID, ndays):\n",
    "    nstn = len(stnID)\n",
    "    prcp_stn = np.nan * np.zeros([nstn, ndays], dtype=np.float32)\n",
    "    tmin_stn = np.nan * np.zeros([nstn, ndays], dtype=np.float32)\n",
    "    tmax_stn = np.nan * np.zeros([nstn, ndays], dtype=np.float32)\n",
    "\n",
    "    for i in range(nstn):\n",
    "        if np.mod(i, 1000) == 0:\n",
    "            print('station', i, nstn)\n",
    "        file = inpath_raw + '/' + stnID[i] + '.nc'\n",
    "        fid = nc.Dataset(file)\n",
    "        varlist = fid.variables.keys()\n",
    "        if 'prcp' in varlist:\n",
    "            prcp_stn[i, :] = fid['prcp'][:].data\n",
    "        if 'tmin' in varlist:\n",
    "            tmin_stn[i, :] = fid['tmin'][:].data\n",
    "        if 'tmax' in varlist:\n",
    "            tmax_stn[i, :] = fid['tmax'][:].data\n",
    "        fid.close()\n",
    "\n",
    "    tmean_stn = (tmin_stn + tmax_stn) / 2\n",
    "    trange_stn = np.abs(tmax_stn - tmin_stn)\n",
    "\n",
    "    return prcp_stn, tmean_stn, trange_stn\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# time periods and methods\n",
    "# method choices: \"GWR\", \"nearest\", \"linear\", \"TLR1\", \"TLR2\". Note: TLR1 is fixed tlr of -6.5. TLR2 is MERRA2-based tlr.\n",
    "\n",
    "\n",
    "# read from inputs\n",
    "# a = int(sys.argv[1])\n",
    "# b = int(sys.argv[2])\n",
    "# downtostn_method = sys.argv[3]\n",
    "# year = [a, b]\n",
    "\n",
    "# fixed\n",
    "year = [2018, 2018]\n",
    "downtostn_method = 'GWR'\n",
    "print('start/end year', year)\n",
    "print('downtostn_method', downtostn_method)\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# basic information: to be set before running\n",
    "vars = ['prcp', 'tmin', 'tmax']\n",
    "lontar = np.arange(-180 + 0.05, -50, 0.1)\n",
    "lattar = np.arange(85 - 0.05, 5, -0.1)\n",
    "hwsize = 2  # use (2*2+1)**2 grids to perform regression\n",
    "\n",
    "### Local Mac settings\n",
    "# input files/paths\n",
    "filedem = './DEM/NA_DEM_010deg_trim.mat' # DEM for the target region(0.1 degree)\n",
    "filedem_era = './DEM/MERRA2_DEM2.mat' # DEM for the original reanalysis\n",
    "filedist2coast = '/Users/localuser/Research/EMDNA/dist2coast_001.npz' # 0.01-degree distance to coast\n",
    "filetlr = '/Users/localuser/Research/EMDNA/MERRA2_TLR.mat' # temperature lapse rate from MERRA2\n",
    "gmet_stnfile = '/Users/localuser/GMET/pyGMET_NA/stnlist_whole.txt' # station lists\n",
    "gmet_stnpath = '/Users/localuser/GMET/StnInput_daily' # station files for gmet_stnfile\n",
    "\n",
    "# output files/paths (can also be used as inputs once generated)\n",
    "filenear = '/Users/localuser/Research/EMDNA/downscale/MERRA2/MERRA2_weight_dem.npz' # file of dem and near gird information\n",
    "gmet_stndatafile = '/Users/localuser/Research/EMDNA/stndata_whole.npz' # to be saved. only process when absent\n",
    "inpath_raw = '/Users/localuser/Research/EMDNA/downscale/MERRA2' # path for original gridded reanalysis data\n",
    "outpath_ds = '/Users/localuser/Research/EMDNA/downscale/MERRA2' # path for saving downscaled reanalysis data\n",
    "### Local Mac settings\n",
    "\n",
    "# ### Plato settings\n",
    "# # input files/paths\n",
    "# filedem = '/datastore/GLOBALWATER/CommonData/EMDNA/DEM/NA_DEM_010deg_trim.mat'  # DEM for the target region(0.1 degree)\n",
    "# filedem_era = '/datastore/GLOBALWATER/CommonData/EMDNA/DEM/DEM/MERRA2_DEM2.mat'  # DEM for the original reanalysis\n",
    "# filedist2coast = '/datastore/GLOBALWATER/CommonData/EMDNA/Auxiliary/dist2coast_001.npz'  # 0.01-degree distance to coast\n",
    "# filetlr = '/datastore/GLOBALWATER/CommonData/EMDNA/Auxiliary/EMDNA/MERRA2_TLR.mat'  # temperature lapse rate from MERRA2\n",
    "# gmet_stnfile = '/home/gut428/GMET/eCAI_EMDNA/StnGridInfo/stnlist_whole.txt'  # station lists\n",
    "# gmet_stnpath = '/home/gut428/GMET/StnInput_daily'  # station files for gmet_stnfile\n",
    "#\n",
    "# # output files/paths (can also be used as inputs once generated)\n",
    "# filenear = '/home/gut428/MERRA2_day_ds/MERRA2_weight_dem.npz'  # file of dem and near gird information\n",
    "# gmet_stndatafile = '/datastore/GLOBALWATER/CommonData/EMDNA/stndata_whole.npz'  # to be saved. only process when absent\n",
    "# inpath_raw = '/datastore/GLOBALWATER/CommonData/EMDNA/MERRA2_day_raw'  # path for original gridded reanalysis data\n",
    "# outpath_ds = '/home/gut428/MERRA2_day_ds'  # path for saving downscaled reanalysis data\n",
    "# ### Plato settings\n",
    "\n",
    "# output downscale file at station points\n",
    "file_readownstn = outpath_ds + '/MERRA2_downto_stn_' + downtostn_method + '.npz'  # downscale to station points (1979-2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "\n",
    "# prepare basic infomation\n",
    "\n",
    "# 1. date lists from 1979 to 2018 (this corresponds to station data). this should be changed in future\n",
    "date_list, date_number = m_DateList(1979, 2018, 'ByYear')\n",
    "ndays = len(date_number['yyyymmdd'])  # days from 1979 to 2018\n",
    "\n",
    "# 2. basic lat/lon/elev information\n",
    "# read DEM/mask\n",
    "datatemp = io.loadmat(filedem)\n",
    "demtar = datatemp['DEM']  # this is consistent with lontar lattar\n",
    "mask = demtar.copy()\n",
    "mask[~np.isnan(mask)] = 1\n",
    "\n",
    "# station lat/lon/elev and their row/col in the target DEM/mask\n",
    "stn_ID = np.genfromtxt(gmet_stnfile, dtype='str', skip_header=1, comments='#', delimiter=',', usecols=(0), unpack=False)\n",
    "stn_lle = np.loadtxt(gmet_stnfile, dtype=float, skiprows=1, comments='#', delimiter=',', usecols=(1, 2, 3),\n",
    "                     unpack=False)\n",
    "stn_row = ((85 - stn_lle[:, 0]) / 0.1).astype(int)\n",
    "stn_col = ((stn_lle[:, 1] + 180) / 0.1).astype(int)\n",
    "nstn = len(stn_ID)\n",
    "\n",
    "# load the lat/lon of original data\n",
    "infile = inpath_raw + '/MERRA2_' + vars[0] + '_' + str(year[1]) + '.mat'\n",
    "datatemp = {}\n",
    "f = h5py.File(infile, 'r')\n",
    "for k, v in f.items():\n",
    "    datatemp[k] = np.array(v)\n",
    "latori = datatemp['latitude'][0]\n",
    "lonori = datatemp['longitude'][0]\n",
    "del datatemp\n",
    "\n",
    "# 3. load temperature lapse rate (TLR) to support \"TLR\" downscaling. 12 climatological monthly maps.\n",
    "dt = io.loadmat(filetlr)\n",
    "TLRori = dt['TLRori']\n",
    "latM0 = dt['latM0']\n",
    "lonM0 = dt['lonM0']\n",
    "del dt\n",
    "\n",
    "TLRnew = np.nan * np.zeros([len(latori), len(lonori), np.shape(TLRori)[2]])\n",
    "for i in range(np.shape(TLRori)[2]):\n",
    "    f = interpolate.interp2d(latM0, lonM0, TLRori[:, :, 0], kind='linear')\n",
    "    tlrnew = f(latori, lonori)\n",
    "    tlrnew = np.flipud(tlrnew.T)\n",
    "    TLRnew[:, :, i] = tlrnew\n",
    "TLRuse = np.nan * np.zeros([len(latori), len(lonori), 12])\n",
    "for i in range(12):\n",
    "    TLRuse[:, :, i] = np.nanmean(TLRnew[:, :, np.arange(i, np.shape(TLRori)[2], 12)], axis=2)\n",
    "\n",
    "del TLRori, TLRnew\n",
    "\n",
    "# 4. calculate distance to coast from a 0.01 degree map using linear interpolation\n",
    "dist2coast_stn = dist2coast_cal(filedist2coast, stn_lle[:, [0, 1]])\n",
    "\n",
    "lonorim, latorim = np.meshgrid(lonori, latori)\n",
    "xynew = np.vstack((latorim.flatten(), lonorim.flatten())).T\n",
    "dist2coast_ori = dist2coast_cal(filedist2coast, xynew)\n",
    "dist2coast_ori = np.reshape(dist2coast_ori, np.shape(latorim))\n",
    "\n",
    "lontarm, lattarm = np.meshgrid(lontar, lattar)\n",
    "xynew = np.vstack((lattarm.flatten(), lontarm.flatten())).T\n",
    "dist2coast_tar = dist2coast_cal(filedist2coast, xynew)\n",
    "dist2coast_tar = np.reshape(dist2coast_tar, np.shape(lattarm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all station data and save to facilitate analysis in the future\n",
    "if not os.path.isfile(gmet_stndatafile):\n",
    "    prcp_stn, tmean_stn, trange_stn = readstndata(gmet_stnpath, stn_ID, ndays)\n",
    "    np.savez_compressed(gmet_stndatafile, prcp_stn=prcp_stn, tmean_stn=tmean_stn, trange_stn=trange_stn,\n",
    "                        stn_ID=stn_ID, stn_lle=stn_lle, stn_row=stn_row, stn_col=stn_col)\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# for 0.1 degree grids, calculate its near grid (raw reanalysis resolutino)\n",
    "if not os.path.isfile(filenear):\n",
    "    rowse, colse, weight = neargrid(lattar, lontar, latori, lonori, hwsize)\n",
    "    # extract ori dem\n",
    "    demori = demread(filedem_era, latori, lonori)\n",
    "    io.savemat(filenear, {'rowse': rowse, 'colse': colse, 'weight': weight, 'demori': demori})\n",
    "else:\n",
    "    datatemp = io.loadmat(filenear)\n",
    "    rowse = datatemp['rowse']\n",
    "    colse = datatemp['colse']\n",
    "    weight = datatemp['weight']\n",
    "    demori = datatemp['demori']\n",
    "    del datatemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year--var: 2018 prcp\n",
      "row 0 800\n",
      "row 1 800\n",
      "row 2 800\n",
      "row 3 800\n",
      "row 4 800\n",
      "row 5 800\n",
      "row 6 800\n",
      "row 7 800\n",
      "row 8 800\n",
      "row 9 800\n",
      "row 10 800\n",
      "row 11 800\n",
      "row 12 800\n",
      "row 13 800\n",
      "row 14 800\n",
      "row 15 800\n",
      "row 16 800\n",
      "row 17 800\n",
      "row 18 800\n",
      "row 19 800\n",
      "row 20 800\n",
      "row 21 800\n",
      "row 22 800\n",
      "row 23 800\n"
     ]
    }
   ],
   "source": [
    "# downscale reanalysis to 0.1 degree\n",
    "for y in range(year[0], year[1] + 1):\n",
    "    for v in range(len(vars)):\n",
    "        print('year--var:', y, vars[v])\n",
    "        infile = inpath_raw + '/MERRA2_' + vars[v] + '_' + str(y) + '.mat'\n",
    "        outfile_grid = outpath_ds + '/MERRA2_ds_' + vars[v] + '_' + str(y) + '.npz'\n",
    "        if os.path.isfile(outfile_grid):\n",
    "            continue\n",
    "\n",
    "        # load original daily reanalysis data\n",
    "        datatemp = {}\n",
    "        f = h5py.File(infile, 'r')\n",
    "        for k, v in f.items():\n",
    "            datatemp[k] = np.array(v)\n",
    "        dataori = datatemp['data']\n",
    "        dataori = np.transpose(dataori, [2, 1, 0])\n",
    "        del datatemp\n",
    "        f.close()\n",
    "\n",
    "        # downscale the reanalysis to 0.1 degree\n",
    "        datatar = readownscale_grid_GWR(dataori, latori, lonori, demori, dist2coast_ori,\n",
    "                                        lattar, lontar, demtar, dist2coast_tar, rowse, colse, weight, mask)\n",
    "        datatar = np.float32(datatar)\n",
    "        np.savez_compressed(outfile_grid, data=datatar, latitude=lattar, longitude=lontar)\n",
    "\n",
    "    # tmin/tmax to tmean/trange\n",
    "    infile1 = outpath_ds + '/MERRA2_ds_tmin_' + str(y) + '.npz'\n",
    "    infile2 = outpath_ds + '/MERRA2_ds_tmax_' + str(y) + '.npz'\n",
    "    outfile1 = outpath_ds + '/MERRA2_ds_tmean_' + str(y) + '.npz'\n",
    "    outfile2 = outpath_ds + '/MERRA2_ds_trange_' + str(y) + '.npz'\n",
    "    if os.path.isfile(outfile1) and os.path.isfile(outfile2):\n",
    "        continue\n",
    "    tmin = np.load(infile1)\n",
    "    tmin = tmin['data']\n",
    "    tmax = np.load(infile2)\n",
    "    tmax=tmax['data']\n",
    "    tmean = (tmin+tmax)/2\n",
    "    trange = np.abs(tmax-tmin)\n",
    "    np.savez_compressed(outfile1, data=np.float32(tmean), latitude=lattar, longitude=lontar)\n",
    "    np.savez_compressed(outfile2, data=np.float32(trange), latitude=lattar, longitude=lontar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
