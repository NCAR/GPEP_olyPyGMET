{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import auxiliary as au\n",
    "import regression as reg\n",
    "import datetime as dt\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import io\n",
    "from scipy.interpolate import interp2d\n",
    "import os\n",
    "import sys\n",
    "from scipy.interpolate import griddata\n",
    "import h5py\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "from bma_merge import bma\n",
    "\n",
    "\n",
    "def divide_train_test(data, dividenum, randseed=-1):\n",
    "    if randseed == -1:\n",
    "        random.seed(time.time())\n",
    "    num = len(data)\n",
    "    subnum = int(num / dividenum)\n",
    "    data_train = np.zeros([dividenum, num - subnum], dtype=int)\n",
    "    data_test = np.zeros([dividenum, subnum], dtype=int)\n",
    "    randindex = random.sample(range(num), num)\n",
    "    for i in range(dividenum):\n",
    "        data_test[i, :] = np.sort(data[randindex[i * subnum:(i + 1) * subnum]])\n",
    "        data_train[i, :] = np.setdiff1d(data, data_test[i])\n",
    "    return data_train, data_test\n",
    "\n",
    "\n",
    "def double_cvindex(gmet_stndatafile, dividenum, rndseed=123):\n",
    "    # index for double cross-validation\n",
    "    datatemp = np.load(gmet_stndatafile)\n",
    "    prcp_stn0 = datatemp['prcp_stn'][:, 0]\n",
    "    tmean_stn0 = datatemp['tmean_stn'][:, 0]\n",
    "    prcp_stnindex = np.argwhere(~np.isnan(prcp_stn0))\n",
    "    prcp_stnindex = prcp_stnindex.flatten()\n",
    "    tmean_stnindex = np.argwhere(~np.isnan(tmean_stn0))\n",
    "    tmean_stnindex = tmean_stnindex.flatten()\n",
    "\n",
    "    subnum1 = int(len(prcp_stnindex) / dividenum)\n",
    "    subnum2 = int((len(prcp_stnindex) - subnum1) / dividenum)\n",
    "    # prcp_testindex1 = np.zeros([dividenum,subnum1])\n",
    "    # prcp_trainindex1 = np.zeros([dividenum,len(prcp_stnindex) - subnum1])\n",
    "    prcp_trainindex1, prcp_testindex1 = divide_train_test(prcp_stnindex, dividenum, randseed=rndseed)\n",
    "    prcp_testindex2 = np.zeros([dividenum, dividenum, subnum2], dtype=int)\n",
    "    prcp_trainindex2 = np.zeros([dividenum, dividenum, len(prcp_stnindex) - subnum1 - subnum2], dtype=int)\n",
    "    for i in range(dividenum):\n",
    "        traini, testi = divide_train_test(prcp_trainindex1[i, :], dividenum, randseed=rndseed)\n",
    "        prcp_trainindex2[i, :, :] = traini\n",
    "        prcp_testindex2[i, :, :] = testi\n",
    "\n",
    "    subnum1 = int(len(tmean_stnindex) / dividenum)\n",
    "    subnum2 = int((len(tmean_stnindex) - subnum1) / dividenum)\n",
    "    # tmean_testindex1 = np.zeros([dividenum,subnum1])\n",
    "    # tmean_trainindex1 = np.zeros([dividenum,len(tmean_stnindex) - subnum1])\n",
    "    tmean_trainindex1, tmean_testindex1 = divide_train_test(tmean_stnindex, dividenum, randseed=rndseed)\n",
    "    tmean_testindex2 = np.zeros([dividenum, dividenum, subnum2], dtype=int)\n",
    "    tmean_trainindex2 = np.zeros([dividenum, dividenum, len(tmean_stnindex) - subnum1 - subnum2], dtype=int)\n",
    "    for i in range(dividenum):\n",
    "        traini, testi = divide_train_test(tmean_trainindex1[i, :], dividenum, randseed=rndseed)\n",
    "        tmean_trainindex2[i, :, :] = traini\n",
    "        tmean_testindex2[i, :, :] = testi\n",
    "    return prcp_trainindex1, prcp_testindex1, prcp_trainindex2, prcp_testindex2, \\\n",
    "           tmean_trainindex1, tmean_testindex1, tmean_trainindex2, tmean_testindex2\n",
    "\n",
    "\n",
    "def calculate_anomaly(datatar, dataref, hwsize, amode, upbound=5, lowbound=0.2):\n",
    "    # datatar, dataref: 2D [nstn, ntime]\n",
    "    # amode: anomaly mode ('ratio' or 'diff')\n",
    "    # hwsize: define time window (2*hwsize+1) used to calculate ratio (as ratio for a specific day is too variable)\n",
    "    # upbound/lowbound: upper and lower limitation of ratio/difference\n",
    "    if np.ndim(datatar) == 1:  # only one time step\n",
    "        datatar = datatar[:, np.newaxis]\n",
    "        dataref = dataref[:, np.newaxis]\n",
    "\n",
    "    nstn, ntime = np.shape(datatar)\n",
    "    if ntime < hwsize * 2 + 1:\n",
    "        print('The window size is larger than time steps when calculating ratio between tar and ref datasets')\n",
    "        print('Please set a smaller hwsize')\n",
    "        sys.exit()\n",
    "\n",
    "    anom = np.ones([nstn, ntime])\n",
    "\n",
    "    for i in range(ntime):\n",
    "        if i < hwsize:\n",
    "            windex = np.arange(hwsize * 2 + 1)\n",
    "        elif i >= ntime - hwsize:\n",
    "            windex = np.arange(ntime - hwsize * 2 - 1, ntime)\n",
    "        else:\n",
    "            windex = np.arange(i - hwsize, i + hwsize + 1)\n",
    "        dtari = np.nanmean(datatar[:, windex], axis=1)\n",
    "        drefi = np.nanmean(dataref[:, windex], axis=1)\n",
    "\n",
    "        if amode == 'ratio':\n",
    "            temp = drefi / dtari\n",
    "            temp[(dtari == 0) & (drefi == 0)] = 1\n",
    "            anom[:, i] = temp\n",
    "        elif amode == 'diff':\n",
    "            anom[:, i] = drefi - dtari\n",
    "        else:\n",
    "            sys.exit('Unknow amode. Please use either ratio or diff')\n",
    "\n",
    "    anom[anom > upbound] = upbound\n",
    "    anom[anom < lowbound] = lowbound\n",
    "    return anom\n",
    "\n",
    "\n",
    "def extrapolation(latin, lonin, datain, nearstn_loc, nearstn_dist):\n",
    "    # datain: one or multiple time steps\n",
    "    wexp = 3\n",
    "    if np.ndim(datain) == 1:  # add time axis\n",
    "        datain = datain[:, np.newaxis]\n",
    "    latin[np.isnan(datain[:, 0])] = np.nan\n",
    "    lonin[np.isnan(datain[:, 0])] = np.nan\n",
    "\n",
    "    if np.ndim(nearstn_loc) == 2:  # extrapolate to station points\n",
    "        num = np.shape(nearstn_loc)[0]\n",
    "        ntimes = np.shape(datain)[1]\n",
    "        dataout = np.zeros([num, ntimes])\n",
    "        for i in range(num):\n",
    "            dataini = datain[nearstn_loc[i, :], :]\n",
    "            disti = nearstn_dist[i, :]\n",
    "            weighti = au.distanceweight(disti, np.max(disti) + 1, wexp)\n",
    "            weighti = weighti / np.sum(weighti)\n",
    "            weighti2 = np.tile(weighti,[ntimes,1]).T\n",
    "            dataout[i, :] = np.sum(dataini * weighti2, axis=0)\n",
    "    elif np.ndim(nearstn_loc) == 3:  # extrapolate to gridds\n",
    "        nrows, ncols, ntimes = np.shape(datain)\n",
    "        dataout = np.zeros([nrows, ncols, ntimes])\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                dataini = datain[nearstn_loc[r, c, :], :]\n",
    "                disti = nearstn_dist[r, c, :]\n",
    "                weighti = au.distanceweight(disti, np.max(disti) + 1, wexp)\n",
    "                weighti = weighti / np.sum(weighti)\n",
    "                weighti2 = np.tile(weighti, [ntimes, 1]).T\n",
    "                dataout[r, c, :] = np.sum(dataini * weighti2, axis=0)\n",
    "    else:\n",
    "        print('The dimensions of tarlat or tarlon are larger than 2')\n",
    "        sys.exit()\n",
    "\n",
    "    return dataout\n",
    "\n",
    "\n",
    "def findnearstn(stnlat, stnlon, tarlat, tarlon, nearnum, noself):\n",
    "    # only use lat/lon to find near stations without considering distance in km\n",
    "    # stnlat/stnlon: 1D\n",
    "    # tarlat/tarlon: 1D or 2D\n",
    "    # noself: 1--stnlat and tarlat have overlapped stations, which should be excluded from stnlat\n",
    "\n",
    "    stnll = np.zeros([len(stnlat), 2])\n",
    "    stnll[:, 0] = stnlat\n",
    "    stnll[:, 1] = stnlon\n",
    "\n",
    "    if len(np.shape(tarlat)) == 1:\n",
    "        num = len(tarlat)\n",
    "        nearstn_loc = np.nan * np.zeros([num, nearnum], dtype=int)\n",
    "        nearstn_dist = np.nan * np.zeros([num, nearnum], dtype=float)\n",
    "        for i in range(num):\n",
    "            if np.isnan(tarlat[i]) or np.isnan(tarlon[i]):\n",
    "                continue\n",
    "            tari = np.array([tarlat[i], tarlon[i]])\n",
    "            dist = au.distance(tari, stnll)\n",
    "            dist[np.isnan(dist)] = 1000000000\n",
    "            if noself == 1:\n",
    "                dist[dist == 0] = np.inf  # may not be perfect, but work for SCDNA\n",
    "            indi = np.argsort(dist)\n",
    "            nearstn_loc[i, :] = indi[0:nearnum]\n",
    "            nearstn_dist[i, :] = dist[nearstn_loc[i, :]]\n",
    "    elif len(np.shape(tarlat)) == 2:\n",
    "        nrows, ncols = np.shape(tarlat)\n",
    "        nearstn_loc = np.nan * np.zeros([nrows, ncols, nearnum], dtype=int)\n",
    "        nearstn_dist = np.nan * np.zeros([nrows, ncols, nearnum], dtype=float)\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if np.isnan(tarlat[r, c]) or np.isnan(tarlon[r, c]):\n",
    "                    continue\n",
    "                tari = np.array([tarlat[r, c], tarlon[r, c]])\n",
    "                dist = au.distance(tari, stnll)\n",
    "                dist[np.isnan(dist)] = 1000000000\n",
    "                indi = np.argsort(dist)\n",
    "                nearstn_loc[r, c, :] = indi[0:nearnum]\n",
    "                nearstn_dist[r, c, :] = dist[nearstn_loc[r, c, :]]\n",
    "    else:\n",
    "        print('The dimensions of tarlat or tarlon are larger than 2')\n",
    "        sys.exit()\n",
    "\n",
    "    return nearstn_loc, nearstn_dist\n",
    "\n",
    "\n",
    "def error_correction(dataori, anomaly, mode='ratio'):\n",
    "    # default: time is the last dimension\n",
    "    if mode == 'ratio':\n",
    "        datacorr = dataori * anomaly\n",
    "    elif mode == 'diff':\n",
    "        datacorr = dataori + anomaly\n",
    "    else:\n",
    "        sys.exit('Wrong error correction mode')\n",
    "    return datacorr\n",
    "\n",
    "\n",
    "def calweight(obsall, preall, mode='RMSE', preprocess=True):\n",
    "    nstn, ntime = np.shape(obsall)\n",
    "    met = np.nan * np.zeros(nstn)\n",
    "    for i in range(nstn):\n",
    "        obs = obsall[i, :]\n",
    "        pre = preall[i, :]\n",
    "        if preprocess:\n",
    "            # delete the nan values\n",
    "            ind_nan = np.isnan(obs) | np.isnan(pre)\n",
    "            obs = obs[~ind_nan]\n",
    "            pre = pre[~ind_nan]\n",
    "\n",
    "        if len(obs) < 3:\n",
    "            continue\n",
    "\n",
    "        if mode == 'RMSE':\n",
    "            met[i] = np.sqrt(np.sum(np.square(obs - pre)) / len(obs))  # RMSE\n",
    "        elif mode == 'CC':\n",
    "            temp = np.corrcoef(obs, pre)\n",
    "            met[i] = temp[0][1]  # CC\n",
    "        else:\n",
    "            sys.exit('Unknown inputs for calmetric')\n",
    "\n",
    "    if mode == 'RMSE':\n",
    "        weight = 1 / (met ** 2)\n",
    "    elif mode == 'CC':\n",
    "        met[met < 0] = 0\n",
    "        weight = met ** 2\n",
    "    else:\n",
    "        sys.exit('Unknown inputs for calmetric')\n",
    "\n",
    "    return weight\n",
    "\n",
    "\n",
    "def calmetric(dtar, dref, metname='RMSE'):\n",
    "    if np.ndim(dtar) == 1:\n",
    "        dtar = dtar[np.newaxis, :]\n",
    "        dref = dref[np.newaxis, :]\n",
    "    nstn, ntimes = np.shape(dtar)\n",
    "    metout = np.nan * np.zeros(nstn, dtype=np.float32)\n",
    "    if metname == 'RMSE':\n",
    "        for i in range(nstn):\n",
    "            metout[i] = np.sqrt(np.nansum(np.square(dtar[i, :] - dref[i, :])) / ntimes)  # RMSE\n",
    "    elif metname == 'CC':\n",
    "        for i in range(nstn):\n",
    "            temp = np.corrcoef(dtar[i, :], dref[i, :])\n",
    "            metout[i] = temp[0, 1]\n",
    "    else:\n",
    "        sys.exit('Unkown metric name')\n",
    "    return metout\n",
    "\n",
    "\n",
    "def ismember(a, b):\n",
    "    # tf = np.in1d(a,b) # for newer versions of numpy\n",
    "    tf = np.array([i in b for i in a])\n",
    "    u = np.unique(a[tf])\n",
    "    index = np.array([(np.where(b == i))[0][-1] if t else 0 for i, t in zip(a, tf)])\n",
    "    return tf, index\n",
    "\n",
    "\n",
    "def weightmerge(data, weight):\n",
    "    if np.ndim(data) == 2:\n",
    "        num, nmodel = np.shape(data)\n",
    "        dataout = np.zeros(num)\n",
    "        for i in range(num):\n",
    "            dataout[i] = np.sum(data[i, :] * weight[i, :]) / np.sum(weight[i, :])\n",
    "    elif np.ndim(data) == 3:\n",
    "        nrows, ncols, nmodel = np.shape(data)\n",
    "        for i in range(nmodel):\n",
    "            data[:, :, i] = data[:, :, i] * weight[:, :, i]\n",
    "        dataout = np.nansum(data, axis=2) / np.nansum(weight, axis=2)\n",
    "    return dataout\n",
    "\n",
    "\n",
    "def m_DateList(year_start, year_end, mode):\n",
    "    # generate a date list (yyyymmdd) between start year and end year\n",
    "    # mode: 'ByDay', 'ByMonth', 'ByYear': time scales of input files\n",
    "    date_start = datetime.date(year_start, 1, 1)\n",
    "    date_end = datetime.date(year_end, 12, 31)\n",
    "    daynum = (date_end - date_start).days + 1\n",
    "\n",
    "    # generate date in format: yyyymmdd\n",
    "    date_ymd = np.zeros(daynum, dtype=int)\n",
    "    dated = date_start\n",
    "    for d in range(daynum):\n",
    "        if d > 0:\n",
    "            dated = dated + datetime.timedelta(days=1)\n",
    "        date_ymd[d] = int(dated.strftime(\"%Y%m%d\"))\n",
    "    date_number = {'yyyymmdd': date_ymd,\n",
    "                   'yyyymm': np.floor(date_ymd / 100).astype(int),\n",
    "                   'yyyy': np.floor(date_ymd / 10000).astype(int),\n",
    "                   'mm': np.floor(np.mod(date_ymd, 10000) / 100).astype(int),\n",
    "                   'dd': np.mod(date_ymd, 100).astype(int)}\n",
    "\n",
    "    # generate file list\n",
    "    if mode == 'ByDay':\n",
    "        datemode = date_number['yyyymmdd']\n",
    "    else:\n",
    "        if mode == 'ByMonth':\n",
    "            datemode = date_number['yyyymm']\n",
    "        elif mode == 'ByYear':\n",
    "            datemode = date_number['yyyy']\n",
    "        datemode = np.unique(datemode)\n",
    "\n",
    "    date_list = [' '] * len(datemode)\n",
    "    for i in range(len(datemode)):\n",
    "        date_list[i] = str(datemode[i])\n",
    "\n",
    "    return date_list, date_number\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# basic settings\n",
    "lontar = np.arange(-180 + 0.05, -50, 0.1)\n",
    "lattar = np.arange(85 - 0.05, 5, -0.1)\n",
    "var = 'tmean'  # ['prcp', 'tmean', 'trange']: this should be input from sbtach script\n",
    "corrmode = 'diff'  # ratio or diff: mode for error correction\n",
    "hwsize = 0  # define time window (2*hwsize+1) used to calculate ratio (as ratio for a specific day is too variable)\n",
    "nearnum = 8  # the number of nearby stations used to extrapolate points to grids (for correction and merging)\n",
    "weightmode = 'RMSE'  # (CC, RMSE, BMA). Weight = CC**2, or Weight = 1/RMSE**2, or Weight = BMA\n",
    "dividenum = 10  # divide the datasets into X parts, e.g. 10-fold cross-validation\n",
    "anombound = [0.2,\n",
    "             5]  # upper and lower bound when calculating the anomaly between target and reference data for correction\n",
    "year = [2000, 2000]  # year range for merging. note weight is calculated using all data not limited by year\n",
    "\n",
    "if corrmode == 'diff':\n",
    "    # default settings in this study since diff is for tmean and trange\n",
    "    hwsize = 0\n",
    "    anombound = [-999, 999]\n",
    "\n",
    "# input files\n",
    "# station list and data\n",
    "# gmet_stnfile = '/home/gut428/GMET/eCAI_EMDNA/StnGridInfo/stnlist_whole.txt'\n",
    "# gmet_stndatafile = '/home/gut428/stndata_whole.npz'\n",
    "gmet_stnfile = '/Users/localuser/GMET/pyGMET_NA/stnlist_whole.txt'\n",
    "gmet_stndatafile = '/Users/localuser/GMET/pyGMET_NA/stndata_whole.npz'  # to be saved. only process when absent\n",
    "\n",
    "# mask file\n",
    "# file_mask = '/datastore/GLOBALWATER/CommonData/EMDNA/DEM/NA_DEM_010deg_trim.mat'\n",
    "file_mask = './DEM/NA_DEM_010deg_trim.mat'\n",
    "\n",
    "# downscaled reanalysis: gridded data\n",
    "path_readown = ['', '', '']\n",
    "prefix = ['ERA5_', 'MERRA2_', 'JRA55_']\n",
    "# downscaled reanalysis data at station points\n",
    "file_readownstn = ['/Users/localuser/Research/Test/ERA5_downto_stn.npz',\n",
    "                   '/Users/localuser/Research/Test/MERRA2_downto_stn.npz',\n",
    "                   '/Users/localuser/Research/Test/JRA55_downto_stn.npz']\n",
    "\n",
    "# output files\n",
    "# train and test index file\n",
    "ttindexfile = '/Users/localuser/Research/Test/2layer_train_test_index.npz'\n",
    "\n",
    "# near stations\n",
    "near_stnfile = '/Users/localuser/Research/Test/near_stn.npz'\n",
    "near_gridfile = '/Users/localuser/Research/Test/near_grid.npz'\n",
    "\n",
    "# error and merging at station level\n",
    "path_reastn_cv = '/Users/localuser/Research/Test'\n",
    "file_corrmerge_stn = path_reastn_cv + 'merge_corr_' + var + '_stn.npz'\n",
    "\n",
    "# output corrected and merged data\n",
    "path_reacorr = ''\n",
    "path_merge = ''\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# basic processing\n",
    "# mask\n",
    "mask = io.loadmat(file_mask)\n",
    "mask = mask['DEM']\n",
    "mask[~np.isnan(mask)] = 1  # 1: valid pixels\n",
    "# attributes\n",
    "reanum = len(file_readownstn)\n",
    "nrows, ncols = np.shape(mask)\n",
    "lontarm, lattarm = np.meshgrid(lontar, lattar)\n",
    "lontarm[np.isnan(mask)] = np.nan\n",
    "lattarm[np.isnan(mask)] = np.nan\n",
    "\n",
    "# date\n",
    "date_list, date_number = m_DateList(1979, 2018, 'ByYear')\n",
    "\n",
    "# load observations for all stations\n",
    "datatemp = np.load(gmet_stndatafile)\n",
    "stndata = datatemp[var + '_stn']\n",
    "stnlle = datatemp['stn_lle']\n",
    "nstn, ntimes = np.shape(stndata)\n",
    "del datatemp\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# design a two-layer cross-validation: generate station combinations\n",
    "# index1 extracts 90% stations for merging and 10% stations for validation\n",
    "# index2 divides the 90% from index1 into 90% and 10% again for error correction\n",
    "if not os.path.isfile(ttindexfile):\n",
    "    prcp_trainindex1, prcp_testindex1, prcp_trainindex2, prcp_testindex2, \\\n",
    "    tmean_trainindex1, tmean_testindex1, tmean_trainindex2, tmean_testindex2 = \\\n",
    "        double_cvindex(gmet_stndatafile, dividenum, rndseed=123)\n",
    "    np.savez_compressed(ttindexfile, prcp_trainindex1=prcp_trainindex1, prcp_testindex1=prcp_testindex1,\n",
    "                        prcp_trainindex2=prcp_trainindex2, prcp_testindex2=prcp_testindex2,\n",
    "                        tmean_trainindex1=tmean_trainindex1, tmean_testindex1=tmean_testindex1,\n",
    "                        tmean_trainindex2=tmean_trainindex2, tmean_testindex2=tmean_testindex2)\n",
    "    del prcp_trainindex1, prcp_testindex1, prcp_trainindex2, prcp_testindex2, \\\n",
    "        tmean_trainindex1, tmean_testindex1, tmean_trainindex2, tmean_testindex2\n",
    "\n",
    "taintestindex = np.load(ttindexfile)\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# find near stations for all grids and station\n",
    "if var == 'trange':\n",
    "    vari = 'tmean'  # trange and tmean have the same index\n",
    "else:\n",
    "    vari = var\n",
    "\n",
    "if os.path.isfile(near_stnfile):\n",
    "    with np.load(near_stnfile) as datatemp:\n",
    "        nearstn_locl1 = datatemp['nearstn_locl1']\n",
    "        nearstn_distl1 = datatemp['nearstn_distl1']\n",
    "        nearstn_locl2 = datatemp['nearstn_locl2']\n",
    "        nearstn_distl2 = datatemp['nearstn_distl2']\n",
    "    del datatemp\n",
    "else:\n",
    "    # layer-1\n",
    "    nstn_testl1 = np.shape(taintestindex[vari + '_testindex1'])[1]\n",
    "    nearstn_locl1 = np.nan * np.zeros([dividenum, nstn_testl1, nearnum], dtype=int)\n",
    "    nearstn_distl1 = np.nan * np.zeros([dividenum, nstn_testl1, nearnum], dtype=np.float32)\n",
    "    for lay1 in range(dividenum):\n",
    "        trainindex1 = taintestindex[vari + '_trainindex1'][lay1, :]\n",
    "        testindex1 = taintestindex[vari + '_testindex1'][lay1, :]\n",
    "        nearstn_locl1[lay1, :, :], nearstn_distl1[lay1, :, :] \\\n",
    "            = findnearstn(stnlle[trainindex1, 0], stnlle[trainindex1, 1],\n",
    "                          stnlle[testindex1, 0], stnlle[testindex1, 1], nearnum, 0)\n",
    "    # layer-2\n",
    "    nstn_testl2 = np.shape(taintestindex[vari + '_testindex2'])[2]\n",
    "    nearstn_locl2 = np.nan * np.zeros([dividenum, dividenum, nstn_testl2, nearnum], dtype=int)\n",
    "    nearstn_distl2 = np.nan * np.zeros([dividenum, dividenum, nstn_testl2, nearnum], dtype=np.float32)\n",
    "    for lay1 in range(dividenum):\n",
    "        for lay2 in range(dividenum):\n",
    "            trainindex2 = taintestindex[vari + '_trainindex2'][lay1, lay2, :]\n",
    "            testindex2 = taintestindex[vari + '_testindex2'][lay1, lay2, :]\n",
    "            nearstn_locl2[lay1, lay2, :, :], nearstn_distl2[lay1, lay2, :, :] \\\n",
    "                = findnearstn(stnlle[trainindex2, 0], stnlle[trainindex2, 1],\n",
    "                              stnlle[testindex2, 0], stnlle[testindex2, 1], nearnum, 0)\n",
    "\n",
    "    np.savez_compressed(near_stnfile, nearstn_locl1=nearstn_locl1, nearstn_distl1=nearstn_distl1,\n",
    "                        nearstn_locl2=nearstn_locl2, nearstn_distl2=nearstn_distl2)\n",
    "\n",
    "if os.path.isfile(near_gridfile):\n",
    "    with np.load(near_gridfile) as datatemp:\n",
    "        neargrid_loc = datatemp['neargrid_loc']\n",
    "        neargrid_dist = datatemp['neargrid_dist']\n",
    "else:\n",
    "    neargrid_loc, neargrid_dist = findnearstn(stnlle[:, 0], stnlle[:, 1], lattarm, lontarm, nearnum, 0)\n",
    "    np.savez_compressed(near_gridfile,neargrid_loc=neargrid_loc,neargrid_dist=neargrid_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load downscaled reanalysis for all stations\n",
    "readata_stn = np.nan * np.zeros([reanum, nstn, ntimes], dtype=np.float32)\n",
    "for rr in range(reanum):\n",
    "    dr = np.load(file_readownstn[rr])\n",
    "    temp = dr[var + '_readown']\n",
    "    if prefix[rr] == 'MERRA2_':  # unify the time length of all data as MERRA2 lacks 1979\n",
    "        add = np.nan * np.zeros([nstn, 365])\n",
    "        temp = np.concatenate((add, temp), axis=1)\n",
    "    readata_stn[rr, :, :] = temp\n",
    "    del dr, temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correction/Merging at station points. Layer-1: 0\n",
      "Correction/Merging at station points. Layer-2: 0\n",
      "Correction/Merging at station points. Reanalysis: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:94: RuntimeWarning: Mean of empty slice\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:106: RuntimeWarning: invalid value encountered in greater\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:107: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 18098 is out of bounds for axis 0 with size 15919",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-212-bdcca1006a97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m                     \u001b[0;31m# extrapolate the ratio to the test stations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                     anom_ext = extrapolation(stnlle_trainl2[:, 0], stnlle_trainl2[:, 1], anom_ori,\n\u001b[0;32m---> 55\u001b[0;31m                                              nearstn_locl2[lay1,lay2,:],nearstn_distl2[lay1,lay2,:])\n\u001b[0m\u001b[1;32m     56\u001b[0m                     \u001b[0;31m# correct data at the test stations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                     \u001b[0mreadata_testl2_corr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_correction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadata_testl2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manom_ext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorrmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-210-62d2dc4fea74>\u001b[0m in \u001b[0;36mextrapolation\u001b[0;34m(latin, lonin, datain, nearstn_loc, nearstn_dist)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mdataout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntimes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mdataini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnearstn_loc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0mdisti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnearstn_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mweighti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mau\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistanceweight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisti\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 18098 is out of bounds for axis 0 with size 15919"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(file_corrmerge_stn):\n",
    "    # initialization\n",
    "    reacorr_stn = np.nan * np.zeros([reanum, nstn, ntimes], dtype=np.float32)  # corrected reanalysis data\n",
    "    reamerge_weight_stn = np.nan * np.zeros([nstn, reanum])  # weight used to obtain reamerge_stn\n",
    "    reamerge_stn = np.nan * np.zeros([nstn, ntimes], dtype=np.float32)  # merged reanalysis at station points\n",
    "\n",
    "    for lay1 in range(dividenum):\n",
    "        print('Correction/Merging at station points. Layer-1:', lay1)\n",
    "        # extract train and test index for layer-1\n",
    "        if var == 'trange':\n",
    "            vari = 'tmean'  # trange and tmean have the same index\n",
    "        else:\n",
    "            vari = var\n",
    "        trainindex1 = taintestindex[vari + '_trainindex1'][lay1, :]\n",
    "        testindex1 = taintestindex[vari + '_testindex1'][lay1, :]\n",
    "        stndata_trainl1 = stndata[trainindex1, :]\n",
    "        stndata_testl1 = stndata[testindex1, :]\n",
    "        stnlle_trainl1 = stnlle[trainindex1, :]\n",
    "        stnlle_testl1 = stnlle[testindex1, :]\n",
    "\n",
    "        # filename: save inputs for each layer-1\n",
    "        file_reacorrl1 = path_reastn_cv + 'reacorr_' + var + '_layer1_' + str(lay1 + 1) + '.npz'\n",
    "        if os.path.isfile(file_reacorrl1):\n",
    "            datatemp = np.load(file_reacorrl1)\n",
    "            reacorr_trainl1 = datatemp['reacorr']\n",
    "            weight_trainl1 = datatemp['reaweight']\n",
    "            del datatemp\n",
    "        else:\n",
    "\n",
    "            # layer-2: start\n",
    "            reacorr_trainl1 = np.zeros([reanum, len(trainindex1), ntimes], dtype=np.float32)\n",
    "            weight_trainl1 = np.zeros([len(trainindex1), reanum], dtype=np.float32)\n",
    "\n",
    "            for lay2 in range(dividenum):\n",
    "                print('Correction/Merging at station points. Layer-2:', lay2)\n",
    "                # extract train and test index for layer-2 (subsets of trainindex1)\n",
    "                trainindex2 = taintestindex[vari + '_trainindex2'][lay1, lay2, :]\n",
    "                testindex2 = taintestindex[vari + '_testindex2'][lay1, lay2, :]\n",
    "                stndata_trainl2 = stndata[trainindex2, :]\n",
    "                stndata_testl2 = stndata[testindex2, :]\n",
    "                stnlle_trainl2 = stnlle[trainindex2, :]\n",
    "                stnlle_testl2 = stnlle[testindex2, :]\n",
    "\n",
    "                for rr in range(reanum):\n",
    "                    print('Correction/Merging at station points. Reanalysis:', rr)\n",
    "                    readata_trainl2 = readata_stn[rr, trainindex2, :]\n",
    "                    readata_testl2 = readata_stn[rr, testindex2, :]\n",
    "\n",
    "                    ### calculate corrected reanalysis data\n",
    "                    # calculate anomaly at the train stations\n",
    "                    anom_ori = calculate_anomaly(readata_trainl2, stndata_trainl2, hwsize, corrmode,\n",
    "                                                 upbound=anombound[1], lowbound=[0])\n",
    "                    # extrapolate the ratio to the test stations\n",
    "                    anom_ext = extrapolation(stnlle_trainl2[:, 0], stnlle_trainl2[:, 1], anom_ori,\n",
    "                                             nearstn_locl2[lay1,lay2,:],nearstn_distl2[lay1,lay2,:])\n",
    "                    # correct data at the test stations\n",
    "                    readata_testl2_corr = error_correction(readata_testl2, anom_ext, mode=corrmode)\n",
    "                    tf, index = ismember(testindex2, trainindex1)\n",
    "                    reacorr_trainl1[rr, index, :] = readata_testl2_corr\n",
    "\n",
    "            if weightmode == 'BMA':\n",
    "                for i in range(len(trainindex1)):\n",
    "                    dobs = stndata_trainl1[i, :]\n",
    "                    drea = reacorr_trainl1[:,i,:].T\n",
    "                    w, sigma, sigma_s = bma(dobs, drea)\n",
    "                    weight_trainl1[i, :] = w\n",
    "            else:\n",
    "                for rr in range(reanum):\n",
    "                    weight_trainl1[:, rr] = calweight(stndata_trainl1, reacorr_trainl1[rr, :, :], weightmode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_trainl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-193-ba48f3022bd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstndata_trainl1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdrea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreacorr_trainl1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrea\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mwbma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-180-8c4f657aecff>\u001b[0m in \u001b[0;36mbma\u001b[0;34m(D, obs, w0)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mz2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mzm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mz2sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mz2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mz2sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "                wrmse = np.zeros(np.shape(weight_trainl1))\n",
    "                for rr in range(reanum):\n",
    "                    print(rr)\n",
    "                    wrmse[:, rr] = calweight(stndata_trainl1, reacorr_trainl1[rr, :, :], weightmode)\n",
    "                wbma = np.zeros(np.shape(weight_trainl1))\n",
    "                for i in range(len(trainindex1)):\n",
    "                    if np.mod(i,100)==0:\n",
    "                        print(i)\n",
    "                    dobs = stndata_trainl1[i, :]\n",
    "                    drea = reacorr_trainl1[:,i,:].T\n",
    "                    w, sigma, sigma_s = bma(drea, dobs)\n",
    "                    wbma[i, :] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10068317 0.11078887 0.11016374]\n",
      " [0.17315596 0.18486045 0.19565443]\n",
      " [0.05217348 0.05890107 0.0594398 ]\n",
      " [0.05882126 0.06068708 0.0613379 ]\n",
      " [0.15172147 0.16416125 0.17463583]\n",
      " [0.11986128 0.13463821 0.14233773]\n",
      " [0.02637832 0.03287793 0.03079062]\n",
      " [0.01178239 0.01285655 0.01244126]\n",
      " [0.06293496 0.0660488  0.0668599 ]\n",
      " [0.06004377 0.06294926 0.06354615]\n",
      " [0.0725813  0.07313931 0.0724977 ]\n",
      " [0.11599385 0.12997345 0.12728545]\n",
      " [0.07720474 0.08268957 0.07775546]\n",
      " [0.01976308 0.02299577 0.0219798 ]\n",
      " [0.04983064 0.04353477 0.04744932]\n",
      " [0.16960265 0.19896869 0.19581776]\n",
      " [0.06989465 0.0801924  0.07816228]\n",
      " [0.06156565 0.06828994 0.06833201]\n",
      " [0.17563282 0.20270986 0.20195129]\n",
      " [0.02332888 0.02000312 0.02301519]\n",
      " [0.007581   0.00664975 0.00696288]\n",
      " [0.12881564 0.15789201 0.14510746]\n",
      " [0.08021805 0.08532558 0.08483811]\n",
      " [0.11808319 0.13596441 0.1338851 ]\n",
      " [0.12539828 0.14955412 0.14925102]\n",
      " [0.08779936 0.08712703 0.0873094 ]\n",
      " [0.10050001 0.13203069 0.12849345]\n",
      " [0.03342094 0.03729955 0.0353742 ]\n",
      " [0.04887715 0.04933791 0.05225604]\n",
      " [0.04203753 0.05360637 0.0483029 ]\n",
      " [0.14750073 0.14622292 0.14576756]\n",
      " [0.06655479 0.07174006 0.07559776]\n",
      " [0.2025834  0.26098635 0.24671767]\n",
      " [0.1654464  0.19093711 0.18302557]\n",
      " [0.14275154 0.15290941 0.15147855]\n",
      " [0.14379623 0.14897101 0.15037001]\n",
      " [0.06633044 0.06357642 0.06498983]\n",
      " [0.18591319 0.1929853  0.20397164]\n",
      " [0.09933194 0.10020813 0.10196251]\n",
      " [0.15547608 0.1617625  0.15239499]\n",
      " [0.04689694 0.04747502 0.04709171]\n",
      " [0.02039927 0.02106869 0.01820122]\n",
      " [0.06116863 0.06924839 0.06237188]\n",
      " [0.0429524  0.04683976 0.04357916]\n",
      " [0.02232828 0.02426939 0.02000401]\n",
      " [0.03455272 0.03823451 0.03424261]\n",
      " [0.03696155 0.03369061 0.03407233]\n",
      " [0.08732892 0.09045087 0.090341  ]\n",
      " [0.03493309 0.03906864 0.03486715]\n",
      " [0.02127778 0.02133494 0.02137827]\n",
      " [0.08190759 0.09330351 0.09066486]\n",
      " [0.03950027 0.04136102 0.03795813]\n",
      " [0.11994203 0.14154249 0.13553133]\n",
      " [0.01105805 0.01169384 0.01131008]\n",
      " [0.0701994  0.07833158 0.07286291]\n",
      " [0.06576004 0.07830294 0.07675037]\n",
      " [0.05596934 0.05727421 0.05702116]\n",
      " [0.13564114 0.14593654 0.14893943]\n",
      " [0.09318702 0.0952808  0.09939949]\n",
      " [0.02680284 0.02792334 0.02765292]\n",
      " [0.09036971 0.0848813  0.08206309]\n",
      " [0.08121005 0.09613562 0.08823663]\n",
      " [0.02639959 0.03110271 0.02736474]\n",
      " [0.03112778 0.03640335 0.03468822]\n",
      " [0.07622361 0.08592929 0.08092565]\n",
      " [0.14445198 0.17847466 0.17802224]\n",
      " [0.02505313 0.02631233 0.02669938]\n",
      " [0.01361179 0.0140055  0.01396444]\n",
      " [0.01624097 0.01621122 0.01642712]\n",
      " [0.02362079 0.02487651 0.02420229]\n",
      " [0.03066668 0.03060127 0.03113358]\n",
      " [0.01948465 0.02053538 0.01994895]\n",
      " [0.02218194 0.02314604 0.0239008 ]\n",
      " [0.00921977 0.00974267 0.00941295]\n",
      " [0.01001256 0.00988213 0.0105021 ]\n",
      " [0.02162078 0.02388827 0.02314051]\n",
      " [0.00873747 0.00881292 0.00872826]\n",
      " [0.01704904 0.01734243 0.0203281 ]\n",
      " [0.00647637 0.00696459 0.00687971]\n",
      " [0.01306342 0.01343763 0.01371003]\n",
      " [0.01692325 0.01809788 0.01775735]\n",
      " [0.0425245  0.0462056  0.04835595]\n",
      " [0.01721219 0.0191046  0.01849352]\n",
      " [0.02788052 0.02903486 0.0293552 ]\n",
      " [0.00757486 0.00724635 0.00684366]\n",
      " [0.03392945 0.03802653 0.03702081]\n",
      " [0.02705636 0.02710064 0.02720608]\n",
      " [0.01513872 0.01692742 0.01526248]\n",
      " [0.01611119 0.01802732 0.01692191]\n",
      " [0.00559283 0.0057877  0.0056902 ]\n",
      " [0.00402848 0.00424509 0.00408034]\n",
      " [0.01743414 0.01707886 0.01858005]\n",
      " [0.01321274 0.01352292 0.01353913]\n",
      " [0.04580975 0.05033708 0.04938317]\n",
      " [0.02811391 0.02912078 0.02780679]\n",
      " [0.04194763 0.04742662 0.04741448]\n",
      " [0.03189768 0.03369329 0.03274329]\n",
      " [0.00889539 0.00993418 0.0097624 ]\n",
      " [0.07655876 0.08291916 0.08301625]]\n"
     ]
    }
   ],
   "source": [
    "nn=15\n",
    "print(wrmse[1:100,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:29: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:25: RuntimeWarning: Mean of empty slice\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:37: RuntimeWarning: invalid value encountered in greater\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:38: RuntimeWarning: invalid value encountered in less\n"
     ]
    }
   ],
   "source": [
    "        # extrapolate the weight from train stations to test stations (in layer-1)\n",
    "        weight_testl1 = extrapolation(weight_trainl1, nearstn_locl1[lay1,:],nearstn_distl1[lay1,:])\n",
    "        reamerge_weight_stn[testindex1, :] = weight_testl1\n",
    "\n",
    "        # repeat error correction using train stations in layer-1 (as in layer-2 only 0.9*0.9=0.81 stations are used)\n",
    "        # extrapolate from train stations to test stations\n",
    "        for rr in range(reanum):\n",
    "            readata_trainl1 = readata_stn[rr, trainindex1, :]\n",
    "            readata_testl1 = readata_stn[rr, testindex1, :]\n",
    "            anom_ori = calculate_anomaly(readata_trainl1, stndata_trainl1, hwsize, corrmode,\n",
    "                                         upbound=anombound[1], lowbound=anombound[0])\n",
    "            anom_ext = extrapolation(anom_ori, nearstn_locl1[lay1,:],nearstn_distl1[lay1,:])\n",
    "            readata_testl1_corr = error_correction(readata_testl1, anom_ext, mode=corrmode)\n",
    "            reacorr_stn[rr, testindex1, :] = readata_testl1_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # merge reanalysis products at the test stations\n",
    "        nstn_testl1 = len(testindex1)\n",
    "        mergedata_testl1 = np.nan * np.zeros([nstn_testl1, ntimes])\n",
    "        for i in range(ntimes):\n",
    "            datain = np.zeros([nstn_testl1, reanum], dtype=np.float32)\n",
    "            for rr in range(reanum):\n",
    "                datain[:, rr] = reacorr_stn[rr, testindex1, i]\n",
    "            dataout = weightmerge(datain, weight_testl1)\n",
    "            mergedata_testl1[:, i] = dataout\n",
    "        reamerge_stn[testindex1, :] = mergedata_testl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py:390: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/usr/local/lib/python3.7/site-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "/usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py:2526: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar)\n",
      "/usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py:2455: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "/usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py:2455: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n",
      "/Users/localuser/Github/PyGMET/auxiliary.py:150: RuntimeWarning: Mean of empty slice\n",
      "  metout[1] = np.nanmean(pre - obs)  # ME\n",
      "/Users/localuser/Github/PyGMET/auxiliary.py:151: RuntimeWarning: Mean of empty slice\n",
      "  metout[2] = np.nanmean(np.abs(pre - obs))  # MAE\n",
      "/Users/localuser/Github/PyGMET/auxiliary.py:152: RuntimeWarning: invalid value encountered in true_divide\n",
      "  metout[3] = np.sqrt(np.sum(np.square(obs - pre)) / len(obs))  # RMSE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn=1424\n",
    "au.metric(reamerge_stn[testindex1[nn],:],stndata[testindex1[nn],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.97352032 -0.58142096  2.13138008  2.58731547]\n",
      "[ 0.9799671  -0.40956008  2.10034299  2.52328421]\n",
      "[ 0.97670083 -1.14504993  1.90531123  2.3591544 ]\n",
      "[nan nan nan nan]\n",
      "[nan nan nan nan]\n",
      "[nan nan nan nan]\n"
     ]
    }
   ],
   "source": [
    "print(au.metric(readata_stn[0,testindex1[nn],:],stndata[testindex1[nn],:]))\n",
    "print(au.metric(readata_stn[1,testindex1[nn],:],stndata[testindex1[nn],:]))\n",
    "print(au.metric(readata_stn[2,testindex1[nn],:],stndata[testindex1[nn],:]))\n",
    "print(au.metric(reacorr_stn[0,testindex1[nn],:],stndata[testindex1[nn],:]))\n",
    "print(au.metric(reacorr_stn[1,testindex1[nn],:],stndata[testindex1[nn],:]))\n",
    "print(au.metric(reacorr_stn[2,testindex1[nn],:],stndata[testindex1[nn],:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05380522, 0.05148685, 0.05256514])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz=reamerge_weight_stn[testindex1,:]\n",
    "np.median(zz,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "file='/Users/localuser/Downloads/prcp_layer_3_RMSE.npz'\n",
    "d=np.load(file)\n",
    "reaweight=d['reaweight']\n",
    "reacorr=d['reacorr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22245, 2480)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=reacorr[0,:,:]\n",
    "np.shape(np.concatenate((z,z),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
