{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year [1979, 1979]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import auxiliary as au\n",
    "import regression as reg\n",
    "import datetime as dt\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import io\n",
    "from scipy.interpolate import interp2d\n",
    "import os\n",
    "import sys\n",
    "from scipy.interpolate import griddata\n",
    "import h5py\n",
    "\n",
    "\n",
    "def demread(file, lattar, lontar):\n",
    "    datatemp = io.loadmat(file)\n",
    "    demori = datatemp['DEM']\n",
    "    demori[np.isnan(demori)] = 0\n",
    "    info = datatemp['Info'][0][0]\n",
    "    latori = np.arange(info['yll'] + info['Ysize'] * info['nrows'] - info['Ysize'] / 2, info['yll'], -info['Ysize'])\n",
    "    lonori = np.arange(info['xll'] + info['Xsize'] / 2, info['xll'] + info['Xsize'] * info['ncols'], info['Xsize'])\n",
    "    f = interp2d(lonori, latori, demori, kind='linear')\n",
    "    demtar = f(lontar.flatten(), lattar.flatten())\n",
    "    demtar = np.flipud(demtar)\n",
    "    return demtar\n",
    "\n",
    "\n",
    "def ncread(file, var):\n",
    "    # read a variable from netcdf\n",
    "    ncfid = nc.Dataset(file)\n",
    "    data = ncfid[var][:].data\n",
    "    ncfid.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "def neargrid(rowtar, coltar, rowori, colori, hwsize):\n",
    "    # inputs are 1D matrices\n",
    "    # tar is target area\n",
    "    # ori is original area\n",
    "    # hwsize is half window size (e.g., 4 means the space window width/length is 2*4+1)\n",
    "    # find a space window centering the target grid in the original area and calculate the weights\n",
    "    nrows = len(rowtar)\n",
    "    ncols = len(coltar)\n",
    "    rowse = np.zeros([nrows, ncols, 2]).astype(int)  # se: start/end\n",
    "    colse = np.zeros([nrows, ncols, 2]).astype(int)  # se: start/end\n",
    "    weight = np.nan * np.zeros([nrows, ncols, (hwsize * 2 + 1) ** 2])  # from left to right/from top to bottom weight\n",
    "\n",
    "    for rr in range(nrows):\n",
    "        rowloc = np.argmin(np.abs(rowori - rowtar[rr]))\n",
    "        rowse[rr, :, 0] = rowloc - hwsize\n",
    "        rowse[rr, :, 1] = rowloc + hwsize\n",
    "\n",
    "    for cc in range(ncols):\n",
    "        colloc = np.argmin(np.abs(colori - coltar[cc]))\n",
    "        colse[:, cc, 0] = colloc - hwsize\n",
    "        colse[:, cc, 1] = colloc + hwsize\n",
    "\n",
    "    rowse[rowse < 0] = 0\n",
    "    rowse[rowse > nrows] = nrows\n",
    "    colse[colse < 0] = 0\n",
    "    colse[colse > ncols] = nrows\n",
    "\n",
    "    maxdist = (hwsize + 0.5) * np.sqrt(2) + 0.5\n",
    "    for rr in range(nrows):\n",
    "        rowloc = np.argmin(np.abs(rowori - rowtar[rr]))\n",
    "        for cc in range(ncols):\n",
    "            colloc = np.argmin(np.abs(colori - coltar[cc]))\n",
    "\n",
    "            rowse_rc = rowse[rr, cc, :]\n",
    "            colse_rc = colse[rr, cc, :]\n",
    "            flag = 0\n",
    "            for i in range(rowse_rc[0], rowse_rc[1] + 1):\n",
    "                for j in range(colse_rc[0], colse_rc[1] + 1):\n",
    "                    dist = ((rowloc - i) ** 2 + (colloc - j) ** 2) ** 0.5\n",
    "                    weight[rr, cc, flag] = au.distanceweight(dist, maxdist, 3)\n",
    "                    flag = flag + 1\n",
    "\n",
    "            weight[rr, cc, :] = weight[rr, cc, :] / np.nansum(weight[rr, cc, :])\n",
    "\n",
    "    return rowse, colse, weight\n",
    "\n",
    "\n",
    "def readownscale(dataori, latori, lonori, demori, lattar, lontar, demtar, rowse, colse, weight, mask):\n",
    "    nrows = len(lattar)\n",
    "    ncols = len(lontar)\n",
    "    ntimes = np.shape(dataori)[2]\n",
    "    lonori, latori = np.meshgrid(lonori, latori)\n",
    "    datatar = np.nan * np.zeros([nrows, ncols, ntimes])\n",
    "\n",
    "    for rr in range(nrows):\n",
    "        for cc in range(ncols):\n",
    "            if mask[rr, cc] == 1:\n",
    "                rloc = rowse[rr, cc, :]\n",
    "                cloc = colse[rr, cc, :]\n",
    "                latnear = latori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "                lonnear = lonori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "                demnear = demori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "                nnum = np.size(latnear)\n",
    "                latnear = np.reshape(latnear, nnum)\n",
    "                lonnear = np.reshape(lonnear, nnum)\n",
    "                demnear = np.reshape(demnear, nnum)\n",
    "                weightnear = np.zeros([nnum, nnum])\n",
    "                for i in range(nnum):\n",
    "                    weightnear[i, i] = weight[rr, cc, i]\n",
    "\n",
    "                nearinfo = np.zeros([nnum, 4])\n",
    "                nearinfo[:, 0] = 1\n",
    "                nearinfo[:, 1] = latnear\n",
    "                nearinfo[:, 2] = lonnear\n",
    "                nearinfo[:, 3] = demnear\n",
    "\n",
    "                tarinfo = np.zeros(4)\n",
    "                tarinfo[0] = 1\n",
    "                tarinfo[1] = lattar[rr]\n",
    "                tarinfo[2] = lontar[cc]\n",
    "                tarinfo[3] = demtar[rr, cc]\n",
    "\n",
    "                tx_red = np.transpose(nearinfo)\n",
    "                twx_red = np.matmul(tx_red, weightnear)\n",
    "\n",
    "                for tt in range(ntimes):\n",
    "                    datanear = dataori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1, tt]\n",
    "                    datanear = np.reshape(datanear, nnum)\n",
    "\n",
    "                    # upper and lower boundary for the downscaled data\n",
    "                    # this is a conservative limitation\n",
    "                    lowbound = np.min(datanear)\n",
    "                    upbound = np.max(datanear)\n",
    "\n",
    "                    b = reg.least_squares(nearinfo, datanear, twx_red)\n",
    "                    datatemp = np.dot(tarinfo, b)\n",
    "                    if np.all(b == 0) or datatemp>upbound or datatemp<lowbound:\n",
    "                        # use nearest neighbor interpolation\n",
    "                        weightnear = weight[rr, cc, 0:nnum]\n",
    "                        mloc = np.argmax(weightnear)\n",
    "                        datatar[rr, cc, tt] = datanear[mloc]\n",
    "                    else:\n",
    "                        datatar[rr, cc, tt] = datatemp\n",
    "    return datatar\n",
    "\n",
    "def readownscale_tostn(dataori, latori, lonori, demori, lattar, lontar, demtar, rowse, colse, weight, stn_row, stn_col):\n",
    "    nstn = len(stn_row)\n",
    "    ntimes = np.shape(dataori)[2]\n",
    "    lonori, latori = np.meshgrid(lonori, latori)\n",
    "    datatar = np.nan * np.zeros([nstn, ntimes])\n",
    "\n",
    "    for gg in range(nstn):\n",
    "        if np.mod(gg,500)==0:\n",
    "            print('station', gg, nstn)\n",
    "        rr = stn_row[gg]\n",
    "        cc = stn_col[gg]\n",
    "        rloc = rowse[rr, cc, :]\n",
    "        cloc = colse[rr, cc, :]\n",
    "        latnear = latori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "        lonnear = lonori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "        demnear = demori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1]\n",
    "        nnum = np.size(latnear)\n",
    "        latnear = np.reshape(latnear, nnum)\n",
    "        lonnear = np.reshape(lonnear, nnum)\n",
    "        demnear = np.reshape(demnear, nnum)\n",
    "        weightnear = np.zeros([nnum, nnum])\n",
    "        for i in range(nnum):\n",
    "            weightnear[i, i] = weight[rr, cc, i]\n",
    "\n",
    "        nearinfo = np.zeros([nnum, 4])\n",
    "        nearinfo[:, 0] = 1\n",
    "        nearinfo[:, 1] = latnear\n",
    "        nearinfo[:, 2] = lonnear\n",
    "        nearinfo[:, 3] = demnear\n",
    "\n",
    "        tarinfo = np.zeros(4)\n",
    "        tarinfo[0] = 1\n",
    "        tarinfo[1] = lattar[rr]\n",
    "        tarinfo[2] = lontar[cc]\n",
    "        tarinfo[3] = demtar[rr, cc]\n",
    "\n",
    "        tx_red = np.transpose(nearinfo)\n",
    "        twx_red = np.matmul(tx_red, weightnear)\n",
    "\n",
    "        for tt in range(ntimes):\n",
    "            datanear = dataori[rloc[0]:rloc[1] + 1, cloc[0]:cloc[1] + 1, tt]\n",
    "            datanear = np.reshape(datanear, nnum)\n",
    "\n",
    "            # upper and lower boundary for the downscaled data\n",
    "            # this is a conservative limitation\n",
    "            lowbound = np.min(datanear)\n",
    "            upbound = np.max(datanear)\n",
    "\n",
    "            b = reg.least_squares(nearinfo, datanear, twx_red)\n",
    "            datatemp = np.dot(tarinfo, b)\n",
    "            if np.all(b == 0) or datatemp > upbound or datatemp < lowbound:\n",
    "                # use nearest neighbor interpolation\n",
    "                weightnear = weight[rr, cc, 0:nnum]\n",
    "                mloc = np.argmax(weightnear)\n",
    "                datatar[gg, tt] = datanear[mloc]\n",
    "            else:\n",
    "                datatar[gg, tt] = datatemp\n",
    "    return datatar\n",
    "\n",
    "def readstndata(inpath,stnID,ndays):\n",
    "    nstn = len(stnID)\n",
    "    prcp_stn = np.nan * np.zeros([nstn,ndays])\n",
    "    tmin_stn = np.nan * np.zeros([nstn, ndays])\n",
    "    tmax_stn = np.nan * np.zeros([nstn, ndays])\n",
    "\n",
    "\n",
    "    for i in range(nstn):\n",
    "        if np.mod(i,500) == 0:\n",
    "            print('station',i,nstn)\n",
    "\n",
    "        file = inpath + '/' + stnID[i] + '.nc'\n",
    "        fid = nc.Dataset(file)\n",
    "        varlist = fid.variables.keys()\n",
    "        if 'prcp' in varlist:\n",
    "            prcp_stn[i,:] = fid['prcp'][:].data\n",
    "        if 'tmin' in varlist:\n",
    "            tmin_stn[i,:] = fid['tmin'][:].data\n",
    "        if 'tmax' in varlist:\n",
    "            tmax_stn[i,:] = fid['tmax'][:].data\n",
    "        fid.close()\n",
    "\n",
    "    tmean_stn = (tmin_stn+tmax_stn)/2\n",
    "    trange_stn = np.abs(tmax_stn - tmin_stn)\n",
    "\n",
    "    return prcp_stn, tmean_stn, trange_stn\n",
    "\n",
    "\n",
    "# input parameters\n",
    "# a = int(sys.argv[1])\n",
    "# b = int(sys.argv[2])\n",
    "# year = [a, b]\n",
    "# year = [1979, 1979]\n",
    "print('year',year)\n",
    "\n",
    "# basic information\n",
    "# mac\n",
    "filedem = './DEM/NA_DEM_010deg_trim.mat'\n",
    "# plato\n",
    "# filedem = '/datastore/GLOBALWATER/CommonData/EMDNA/DEM/NA_DEM_010deg_trim.mat'\n",
    "vars = ['prcp', 'tmin', 'tmax']\n",
    "lontar = np.arange(-180 + 0.05, -50, 0.1)\n",
    "lattar = np.arange(85 - 0.05, 5, -0.1)\n",
    "hwsize = 2\n",
    "\n",
    "datatemp = io.loadmat(filedem)\n",
    "demtar = datatemp['DEM']  # this is consistent with lontar lattar\n",
    "mask = demtar.copy()\n",
    "mask[~np.isnan(mask)] = 1\n",
    "\n",
    "# station information\n",
    "gmet_stnfile = '/Users/localuser/GMET/pyGMET_NA/stnlist_whole.txt'\n",
    "gmet_stnpath = '/Users/localuser/GMET/StnInput_daily'\n",
    "gmet_stndata = '/Users/localuser/GMET/pyGMET_NA/stndata_whole.npz'\n",
    "stn_ID = np.genfromtxt(gmet_stnfile, dtype='str', skip_header=1, comments='#', delimiter=',', usecols=(0), unpack=False)\n",
    "stn_lle = np.loadtxt(gmet_stnfile, dtype=float, skiprows=1, comments='#', delimiter=',', usecols=(1,2,3), unpack=False)\n",
    "stn_row = ((85 - stn_lle[:,0]) / 0.1).astype(int)\n",
    "stn_col = ((stn_lle[:,1] + 180) / 0.1 ).astype(int)\n",
    "if not os.path.isfile(gmet_stndata):\n",
    "    prcp_stn, tmean_stn, trange_stn = readstndata(gmet_stnpath, stn_ID, 14610)\n",
    "    np.savez_compressed(gmet_stndata, prcp_stn=prcp_stn, tmean_stn=tmean_stn, trange_stn=trange_stn,\n",
    "                        stn_ID=stn_ID, stn_lle=stn_lle, stn_row=stn_row, stn_col=stn_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year--var: 1979 prcp\n",
      "station 0 27275\n",
      "station 500 27275\n",
      "station 1000 27275\n",
      "station 1500 27275\n",
      "station 2000 27275\n",
      "station 2500 27275\n",
      "station 3000 27275\n",
      "station 3500 27275\n",
      "station 4000 27275\n",
      "station 4500 27275\n",
      "station 5000 27275\n",
      "station 5500 27275\n",
      "station 6000 27275\n",
      "station 6500 27275\n",
      "station 7000 27275\n",
      "station 7500 27275\n",
      "station 8000 27275\n",
      "station 8500 27275\n",
      "station 9000 27275\n",
      "station 9500 27275\n",
      "station 10000 27275\n",
      "station 10500 27275\n",
      "station 11000 27275\n",
      "station 11500 27275\n",
      "station 12000 27275\n",
      "station 12500 27275\n",
      "station 13000 27275\n",
      "station 13500 27275\n",
      "station 14000 27275\n",
      "station 14500 27275\n",
      "station 15000 27275\n",
      "station 15500 27275\n",
      "station 16000 27275\n",
      "station 16500 27275\n",
      "station 17000 27275\n",
      "station 17500 27275\n",
      "station 18000 27275\n",
      "station 18500 27275\n",
      "station 19000 27275\n",
      "station 19500 27275\n",
      "station 20000 27275\n",
      "station 20500 27275\n",
      "station 21000 27275\n",
      "station 21500 27275\n",
      "station 22000 27275\n",
      "station 22500 27275\n",
      "station 23000 27275\n",
      "station 23500 27275\n",
      "station 24000 27275\n",
      "station 24500 27275\n",
      "station 25000 27275\n",
      "station 25500 27275\n",
      "station 26000 27275\n",
      "station 26500 27275\n",
      "station 27000 27275\n"
     ]
    }
   ],
   "source": [
    "filedem_era = './DEM/ERA5_DEM2.mat'\n",
    "inpath = '/Users/localuser/Research/Test'\n",
    "outpath = '/Users/localuser/Research'\n",
    "filenear = outpath + '/weight_dem.npz'\n",
    "\n",
    "for y in range(year[0], year[1] + 1):\n",
    "    for v in range(1):\n",
    "        print('year--var:', y, vars[v])\n",
    "        infile = inpath + '/ERA5_' + vars[v] + '_' + str(y) + '.mat'\n",
    "        outfile_grid = outpath + '/ERA5_' + vars[v] + '_' + str(y) + '.npz'\n",
    "        outfile_stn = outpath + '/ERA5_stn_' + vars[v] + '_' + str(y) + '.npz'\n",
    "        if os.path.isfile(outfile_grid) and os.path.isfile(outfile_stn):\n",
    "            continue\n",
    "\n",
    "        # load original daily reanalysis data\n",
    "        datatemp = {}\n",
    "        f = h5py.File(infile,'r')\n",
    "        for k, v in f.items():\n",
    "            datatemp[k] = np.array(v)\n",
    "        latori = datatemp['latitude'][0]\n",
    "        lonori = datatemp['longitude'][0]\n",
    "        dataori = datatemp['data']\n",
    "        dataori = np.transpose(dataori, [2, 1, 0])\n",
    "        del datatemp\n",
    "        f.close()\n",
    "\n",
    "        # read location information\n",
    "        if not os.path.isfile(filenear):\n",
    "            rowse, colse, weight = neargrid(lattar, lontar, latori, lonori, hwsize)\n",
    "            # extract ori dem\n",
    "            demori = demread(filedem_era, latori, lonori)\n",
    "            io.savemat(filenear, {'rowse': rowse, 'colse': colse, 'weight': weight, 'demori': demori})\n",
    "        else:\n",
    "            datatemp = io.loadmat(filenear)\n",
    "            rowse = datatemp['rowse']\n",
    "            colse = datatemp['colse']\n",
    "            weight = datatemp['weight']\n",
    "            demori = datatemp['demori']\n",
    "            del datatemp\n",
    "        \n",
    "        if not os.path.isfile(outfile_stn):\n",
    "            # downscale the reanalysis to 0.1 degree\n",
    "            datatar = readownscale_tostn(dataori, latori, lonori, demori, lattar, lontar, demtar, rowse, colse, weight, stn_row, stn_col)\n",
    "            datatar = np.float32(datatar)\n",
    "            np.savez_compressed(outfile_stn, data=datatar, latitude=lattar, longitude=lontar,\n",
    "                                stn_ID=stn_ID, stn_lle=stn_lle, stn_row=stn_row, stn_col=stn_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatemp = np.load(gmet_stndata)\n",
    "prcp_stn0=datatemp['prcp_stn'][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27275"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prcp_stn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
