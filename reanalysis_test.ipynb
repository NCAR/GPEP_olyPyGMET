{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import auxiliary as au\n",
    "import regression as reg\n",
    "import datetime as dt\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import io\n",
    "from scipy.interpolate import interp2d\n",
    "import os\n",
    "import sys\n",
    "from scipy.interpolate import griddata\n",
    "import h5py\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "def divide_train_test(data, dividenum, randseed=-1):\n",
    "    if randseed == -1:\n",
    "        random.seed(time.time())\n",
    "    num = len(data)\n",
    "    subnum = int(num / dividenum)\n",
    "    data_train = np.zeros([dividenum, num - subnum],dtype=int)\n",
    "    data_test = np.zeros([dividenum, subnum],dtype=int)\n",
    "    randindex = random.sample(range(num), num)\n",
    "    for i in range(dividenum):\n",
    "        data_test[i, :] = np.sort(data[randindex[i * subnum:(i + 1) * subnum]])\n",
    "        data_train[i, :] = np.setdiff1d(data, data_test[i])\n",
    "    return data_train, data_test\n",
    "\n",
    "\n",
    "def double_cvindex(gmet_stndatafile, dividenum):\n",
    "    # index for double cross-validation\n",
    "    datatemp = np.load(gmet_stndatafile)\n",
    "    prcp_stn0 = datatemp['prcp_stn'][:, 0]\n",
    "    tmean_stn0 = datatemp['tmean_stn'][:, 0]\n",
    "    prcp_stnindex = np.argwhere(~np.isnan(prcp_stn0))\n",
    "    prcp_stnindex = prcp_stnindex.flatten()\n",
    "    tmean_stnindex = np.argwhere(~np.isnan(tmean_stn0))\n",
    "    tmean_stnindex = tmean_stnindex.flatten()\n",
    "\n",
    "    subnum1 = int(len(prcp_stnindex) / dividenum)\n",
    "    subnum2 = int((len(prcp_stnindex) - subnum1) / dividenum)\n",
    "    # prcp_testindex1 = np.zeros([dividenum,subnum1])\n",
    "    # prcp_trainindex1 = np.zeros([dividenum,len(prcp_stnindex) - subnum1])\n",
    "    prcp_trainindex1, prcp_testindex1 = divide_train_test(prcp_stnindex, dividenum, randseed=123)\n",
    "    prcp_testindex2 = np.zeros([dividenum, dividenum, subnum2], dtype=int)\n",
    "    prcp_trainindex2 = np.zeros([dividenum, dividenum, len(prcp_stnindex) - subnum1 - subnum2], dtype=int)\n",
    "    for i in range(dividenum):\n",
    "        traini, testi = divide_train_test(prcp_trainindex1[i, :], dividenum, randseed=123)\n",
    "        prcp_trainindex2[i, :, :] = traini\n",
    "        prcp_testindex2[i, :, :] = testi\n",
    "\n",
    "    subnum1 = int(len(tmean_stnindex) / dividenum)\n",
    "    subnum2 = int((len(tmean_stnindex) - subnum1) / dividenum)\n",
    "    # tmean_testindex1 = np.zeros([dividenum,subnum1])\n",
    "    # tmean_trainindex1 = np.zeros([dividenum,len(tmean_stnindex) - subnum1])\n",
    "    tmean_trainindex1, tmean_testindex1 = divide_train_test(tmean_stnindex, dividenum, randseed=123)\n",
    "    tmean_testindex2 = np.zeros([dividenum, dividenum, subnum2], dtype=int)\n",
    "    tmean_trainindex2 = np.zeros([dividenum, dividenum, len(tmean_stnindex) - subnum1 - subnum2], dtype=int)\n",
    "    for i in range(dividenum):\n",
    "        traini, testi = divide_train_test(tmean_trainindex1[i, :], dividenum, randseed=123)\n",
    "        tmean_trainindex2[i, :, :] = traini\n",
    "        tmean_testindex2[i, :, :] = testi\n",
    "    return prcp_trainindex1, prcp_testindex1, prcp_trainindex2, prcp_testindex2, \\\n",
    "           tmean_trainindex1, tmean_testindex1, tmean_trainindex2, tmean_testindex2\n",
    "\n",
    "def calculate_anomaly(datatar, dataref, hwsize, amode, upbound=5, lowbound=0.2):\n",
    "    # datatar, dataref: 2D [nstn, ntime]\n",
    "    # amode: anomaly mode ('ratio' or 'diff')\n",
    "    # hwsize: define time window (2*hwsize+1) used to calculate ratio (as ratio for a specific day is too variable)\n",
    "    # upbound/lowbound: upper and lower limitation of ratio/difference\n",
    "    if np.ndim(datatar) == 1: # only one time step\n",
    "        datatar = datatar[:,np.newaxis]\n",
    "        dataref = dataref[:,np.newaxis]\n",
    "\n",
    "    nstn, ntime = np.shape(datatar)\n",
    "    if ntime < hwsize * 2 + 1:\n",
    "        print('The window size is larger than time steps when calculating ratio between tar and ref datasets')\n",
    "        print('Please set a smaller hwsize')\n",
    "        sys.exit()\n",
    "\n",
    "    indnan = np.isnan(datatar) | np.isnan(dataref)\n",
    "    datatar[indnan] = np.nan\n",
    "    dataref[indnan] = np.nan\n",
    "    del indnan\n",
    "\n",
    "    anom = np.ones([nstn, ntime])\n",
    "\n",
    "    for i in range(ntime):\n",
    "        if i < hwsize:\n",
    "            windex = np.arange(hwsize * 2 + 1)\n",
    "        elif i >= ntime - hwsize:\n",
    "            windex = np.arange(ntime - hwsize * 2 - 1, ntime)\n",
    "        else:\n",
    "            windex = np.arange(i - hwsize, i + hwsize + 1)\n",
    "        dtari = np.nanmean(datatar[:, windex], axis=1)\n",
    "        drefi = np.nanmean(dataref[:, windex], axis=1)\n",
    "\n",
    "        if amode == 'ratio':\n",
    "            temp = drefi / dtari\n",
    "            temp[(dtari == 0) & (drefi == 0)] = 1\n",
    "            anom[:, i] = temp\n",
    "        elif amode == 'diff':\n",
    "            anom[:, i] = drefi - dtari\n",
    "        else:\n",
    "            sys.exit('Unknow amode. Please use either ratio or diff')\n",
    "\n",
    "    anom[anom > upbound] = upbound\n",
    "    anom[anom < lowbound] = lowbound\n",
    "    return anom\n",
    "\n",
    "\n",
    "def extrapolation(latin, lonin, datain, latout, lonout, nearnum):\n",
    "    # datain: one or multiple time steps\n",
    "    wexp = 3\n",
    "    if np.ndim(datain) == 1:  # add time axis\n",
    "        datain = datain[:, np.newaxis]\n",
    "    latin[np.isnan(datain[:, 0])] = np.nan\n",
    "    lonin[np.isnan(datain[:, 0])] = np.nan\n",
    "\n",
    "    if np.ndim(latout) == 1: # extrapolate to station points\n",
    "        nearstn_loc, nearstn_dist = findnearstn(latin, lonin, latout, lonout, nearnum, 1)\n",
    "        num = len(latout)\n",
    "        ntimes = np.shape(datain)[1]\n",
    "        dataout = np.zeros([num, ntimes])\n",
    "        for i in range(num):\n",
    "            if np.mod(i,500) == 0:\n",
    "                print(i)\n",
    "            dataini = datain[nearstn_loc[i, :], :]\n",
    "            disti = nearstn_dist[i, :]\n",
    "            weighti = au.distanceweight(disti, np.max(disti) + 1, wexp)\n",
    "            weighti = weighti / np.sum(weighti)\n",
    "            for j in range(ntimes):\n",
    "                dataout[i, j] = np.sum(dataini[:, j] * weighti)\n",
    "\n",
    "    elif np.ndim(latout) == 2: # extrapolate to gridds\n",
    "        nearstn_loc, nearstn_dist = findnearstn(latin, lonin, latout, lonout, nearnum, 0)\n",
    "        nrows, ncols, ntimes = np.shape(datain)\n",
    "        dataout = np.zeros([nrows, ncols, ntimes])\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                dataini = datain[nearstn_loc[r, c, :], :]\n",
    "                disti = nearstn_dist[r, c, :]\n",
    "                weighti = au.distanceweight(disti, np.max(disti) + 1, wexp)\n",
    "                weighti = weighti / np.sum(weighti)\n",
    "                for j in range(ntimes):\n",
    "                    dataout[r, c, j] = np.sum(dataini[:, j] * weighti)\n",
    "    else:\n",
    "        print('The dimensions of tarlat or tarlon are larger than 2')\n",
    "        sys.exit()\n",
    "\n",
    "    return dataout\n",
    "\n",
    "\n",
    "def findnearstn(stnlat, stnlon, tarlat, tarlon, nearnum, noself):\n",
    "    # only use lat/lon to find near stations without considering distance in km\n",
    "    # stnlat/stnlon: 1D\n",
    "    # tarlat/tarlon: 1D or 2D\n",
    "    # noself: 1--stnlat and tarlat have overlapped stations, which should be excluded from stnlat\n",
    "\n",
    "    stnll = np.zeros([len(stnlat), 2])\n",
    "    stnll[:, 0] = stnlat\n",
    "    stnll[:, 1] = stnlon\n",
    "\n",
    "    if len(np.shape(tarlat)) == 1:\n",
    "        num = len(tarlat)\n",
    "        nearstn_loc = np.zeros([num, nearnum],dtype=int)\n",
    "        nearstn_dist = np.zeros([num, nearnum],dtype=float)\n",
    "        for i in range(num):\n",
    "            tari = np.array([tarlat[i], tarlon[i]])\n",
    "            dist = au.distance(tari, stnll)\n",
    "            dist[np.isnan(dist)] = 1000000000\n",
    "            if noself == 1:\n",
    "                dist[dist == 0] = np.inf  # may not be perfect, but work for SCDNA\n",
    "            indi = np.argsort(dist)\n",
    "            nearstn_loc[i, :] = indi[0:nearnum]\n",
    "            nearstn_dist[i, :] = dist[nearstn_loc[i, :]]\n",
    "    elif len(np.shape(tarlat)) == 2:\n",
    "        nrows, ncols = np.shape(tarlat)\n",
    "        nearstn_loc = np.zeros([nrows, ncols, nearnum],dtype=int)\n",
    "        nearstn_dist = np.zeros([nrows, ncols, nearnum],dtype=float)\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                tari = np.array([tarlat[r, c], tarlon[r, c]])\n",
    "                dist = au.distance(tari, stnll)\n",
    "                dist[np.isnan(dist)] = 1000000000\n",
    "                indi = np.argsort(dist)\n",
    "                nearstn_loc[r, c, :] = indi[0:nearnum]\n",
    "                nearstn_dist[r, c, :] = dist[nearstn_loc[r, c, :]]\n",
    "    else:\n",
    "        print('The dimensions of tarlat or tarlon are larger than 2')\n",
    "        sys.exit()\n",
    "\n",
    "    return nearstn_loc, nearstn_dist\n",
    "\n",
    "\n",
    "def error_correction(dataori, anomaly, mode='ratio'):\n",
    "    # default: time is the last dimension\n",
    "    if mode == 'ratio':\n",
    "        datacorr = dataori * anomaly\n",
    "    elif mode == 'diff':\n",
    "        datacorr = dataori + anomaly\n",
    "    else:\n",
    "        sys.exit('Wrong error correction mode')\n",
    "    return datacorr\n",
    "\n",
    "\n",
    "def calweight(obsall, preall, mode='RMSE', preprocess=True):\n",
    "    nstn, ntime = np.shape(obsall)\n",
    "    met = np.zeros(nstn)\n",
    "    for i in range(nstn):\n",
    "        obs = obsall[i, :]\n",
    "        pre = preall[i, :]\n",
    "        if preprocess:\n",
    "            # delete the nan values\n",
    "            ind_nan = np.isnan(obs) | np.isnan(pre)\n",
    "            obs = obs[~ind_nan]\n",
    "            pre = pre[~ind_nan]\n",
    "        if mode == 'RMSE':\n",
    "            met[i] = np.sqrt(np.sum(np.square(obs - pre)) / len(obs))  # RMSE\n",
    "        elif mode == 'CC':\n",
    "            temp = np.corrcoef(obs, pre)\n",
    "            met[i] = temp[0][1]  # CC\n",
    "        else:\n",
    "            sys.exit('Unknown inputs for calmetric')\n",
    "\n",
    "    if mode == 'RMSE':\n",
    "        weight = 1 / (met ** 2)\n",
    "    elif mode == 'CC':\n",
    "        met[met < 0] = 0\n",
    "        weight = met ** 2\n",
    "    else:\n",
    "        sys.exit('Unknown inputs for calmetric')\n",
    "\n",
    "    return weight\n",
    "\n",
    "\n",
    "def calrmse(dtar, dref):\n",
    "    if np.ndim(dtar) == 1:\n",
    "        dtar = dtar[np.newaxis, :]\n",
    "        dref = dref[np.newaxis, :]\n",
    "    nstn, ntimes = np.shape(dtar)\n",
    "    rmse = np.nan * np.zeros(nstn)\n",
    "    for i in range(nstn):\n",
    "        rmse[i] = np.sqrt(np.nansum(np.square(dtar[i, :] - dref[i, :])) / ntimes)  # RMSE\n",
    "\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def ismember(a, b):\n",
    "    # tf = np.in1d(a,b) # for newer versions of numpy\n",
    "    tf = np.array([i in b for i in a])\n",
    "    u = np.unique(a[tf])\n",
    "    index = np.array([(np.where(b == i))[0][-1] if t else 0 for i, t in zip(a, tf)])\n",
    "    return tf, index\n",
    "\n",
    "\n",
    "def weightmerge(data, weight):\n",
    "    if np.ndim(data) == 2:\n",
    "        num, nmodel = np.shape(data)\n",
    "        dataout = np.zeros(num)\n",
    "        for i in range(num):\n",
    "            dataout[i] = np.sum(data[i, :] * weight[i, :]) / np.sum(weight[i, :])\n",
    "    elif np.ndim(data) == 3:\n",
    "        nrows, ncols, nmodel = np.shape(data)\n",
    "        dataout = np.zeros([nrows, ncols])\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if not np.isnan(data[r, c, 0]):\n",
    "                    dataout[r, c] = np.sum(data[r, c, :] * weight[r, c, :]) / np.sum(weight[r, c, :])\n",
    "    return dataout\n",
    "\n",
    "def m_DateList(year_start, year_end, mode):\n",
    "    # generate a date list (yyyymmdd) between start year and end year\n",
    "    # mode: 'ByDay', 'ByMonth', 'ByYear': time scales of input files\n",
    "    date_start = datetime.date(year_start, 1, 1)\n",
    "    date_end = datetime.date(year_end, 12, 31)\n",
    "    daynum = (date_end - date_start).days + 1\n",
    "\n",
    "    # generate date in format: yyyymmdd\n",
    "    date_ymd = np.zeros(daynum, dtype=int)\n",
    "    dated = date_start\n",
    "    for d in range(daynum):\n",
    "        if d > 0:\n",
    "            dated = dated + datetime.timedelta(days=1)\n",
    "        date_ymd[d] = int(dated.strftime(\"%Y%m%d\"))\n",
    "    date_number = {'yyyymmdd': date_ymd,\n",
    "                   'yyyymm': np.floor(date_ymd / 100).astype(int),\n",
    "                   'yyyy': np.floor(date_ymd / 10000).astype(int),\n",
    "                   'mm': np.floor(np.mod(date_ymd, 10000) / 100).astype(int),\n",
    "                   'dd': np.mod(date_ymd, 100).astype(int)}\n",
    "\n",
    "    # generate file list\n",
    "    if mode == 'ByDay':\n",
    "        datemode = date_number['yyyymmdd']\n",
    "    else:\n",
    "        if mode == 'ByMonth':\n",
    "            datemode = date_number['yyyymm']\n",
    "        elif mode == 'ByYear':\n",
    "            datemode = date_number['yyyy']\n",
    "        datemode = np.unique(datemode)\n",
    "\n",
    "    date_list = [' '] * len(datemode)\n",
    "    for i in range(len(datemode)):\n",
    "        date_list[i] = str(datemode[i])\n",
    "\n",
    "    return date_list, date_number\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# basic settings\n",
    "lontar = np.arange(-180 + 0.05, -50, 0.1)\n",
    "lattar = np.arange(85 - 0.05, 5, -0.1)\n",
    "var = 'prcp'   # ['prcp', 'tmean', 'trange']: this should be input from sbtach script\n",
    "corrmode = 'ratio'  # ratio or diff: mode for error correction\n",
    "hwsize = 15  # define time window (2*hwsize+1) used to calculate ratio (as ratio for a specific day is too variable)\n",
    "nearnum = 8  # the number of nearby stations used to extrapolate points to grids (for correction and merging)\n",
    "weightmode = 'RMSE'  # the metric used to guide merging (CC or RMSE). Weight = CC**2 or 1/RMSE**2\n",
    "dividenum = 10  # divide the datasets into X parts, e.g. 10-fold cross-validation\n",
    "anombound = [0.2, 5] # upper and lower bound when calculating the anomaly between target and reference data for correction\n",
    "\n",
    "if corrmode == 'diff':\n",
    "    # default settings in this study since diff is for tmean and trange\n",
    "    hwsize = 0\n",
    "    anombound = [-999, 999]\n",
    "\n",
    "# input files\n",
    "# station list and data\n",
    "# gmet_stnfile = '/home/gut428/GMET/eCAI_EMDNA/StnGridInfo/stnlist_whole.txt'\n",
    "# gmet_stndatafile = '/home/gut428/stndata_whole.npz'\n",
    "gmet_stnfile = '/Users/localuser/GMET/pyGMET_NA/stnlist_whole.txt'\n",
    "gmet_stndatafile = '/Users/localuser/GMET/pyGMET_NA/stndata_whole.npz' # to be saved. only process when absent\n",
    "\n",
    "# downscaled reanalysis data at station points\n",
    "# file_readownstn = ['/ERA5_downto_stn.npz',\n",
    "#                    '/MERRA2_downto_stn.npz',\n",
    "#                    '/JRA55_downto_stn.npz']\n",
    "file_readownstn = ['/Users/localuser/Research/Test/ERA5_downto_stn.npz',\n",
    "                  '/Users/localuser/Research/Test/MERRA2_downto_stn.npz',\n",
    "                  '/Users/localuser/Research/Test/JRA55_downto_stn.npz']\n",
    "\n",
    "# mask file\n",
    "# file_mask = '/datastore/GLOBALWATER/CommonData/EMDNA/DEM/NA_DEM_010deg_trim.mat'\n",
    "file_mask = './DEM/NA_DEM_010deg_trim.mat'\n",
    "\n",
    "# downscaled reanalysis: gridded data\n",
    "path_readown = ['', '', '']\n",
    "prefix = ['ERA5_', 'MERAA2_', 'JRA55_']\n",
    "\n",
    "# output files\n",
    "# train and test index file\n",
    "ttindexfile = '/Users/localuser/Research/Test/2layer_train_test_index.npz'\n",
    "\n",
    "# output corrected and merged data\n",
    "path_reacorr = ['', '', '']\n",
    "path_merge = ''\n",
    "file_error_corr = ['', '', ''] # the error at all station points for corrected reanalysis data (based on cross-validation)\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# basic processing\n",
    "# mask\n",
    "mask = io.loadmat(file_mask)\n",
    "mask = mask['DEM']\n",
    "mask[~np.isnan(mask)] = 1  # 1: valid pixels\n",
    "# attributes\n",
    "reanum = len(file_readownstn)\n",
    "nrows, ncols = np.shape(mask)\n",
    "lontarm, lattarm = np.meshgrid(lontar, lattar)\n",
    "\n",
    "# date\n",
    "date_list, date_number = m_DateList(1979, 2018, 'ByYear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(ttindexfile):\n",
    "    prcp_trainindex1, prcp_testindex1, prcp_trainindex2, prcp_testindex2, \\\n",
    "    tmean_trainindex1, tmean_testindex1, tmean_trainindex2, tmean_testindex2 = double_cvindex(gmet_stndatafile,\n",
    "                                                                                              dividenum)\n",
    "    np.savez_compressed(ttindexfile, prcp_trainindex1=prcp_trainindex1, prcp_testindex1=prcp_testindex1,\n",
    "                        prcp_trainindex2=prcp_trainindex2, prcp_testindex2=prcp_testindex2,\n",
    "                        tmean_trainindex1=tmean_trainindex1, tmean_testindex1=tmean_testindex1,\n",
    "                        tmean_trainindex2=tmean_trainindex2, tmean_testindex2=tmean_testindex2)\n",
    "    del prcp_trainindex1, prcp_testindex1, prcp_trainindex2, prcp_testindex2, \\\n",
    "        tmean_trainindex1, tmean_testindex1, tmean_trainindex2, tmean_testindex2\n",
    "\n",
    "taintestindex = np.load(ttindexfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load downscaled reanalysis for all stations\n",
    "readata_stn = [''] * reanum\n",
    "for rr in range(reanum):\n",
    "    dr = np.load(file_readownstn[rr])\n",
    "    temp = dr[var + '_readown']\n",
    "    if prefix[rr] != 'MERRA2_': # unify the time length of all data as MERRA2 lacks 1979\n",
    "        temp = temp[:,365:]\n",
    "    readata_stn[rr] = temp\n",
    "    del dr\n",
    "\n",
    "# load observations for all stations\n",
    "datatemp = np.load(gmet_stndatafile)\n",
    "stndata = datatemp[var + '_stn']\n",
    "stndata = stndata[:, 365:]\n",
    "stnlle = datatemp['stn_lle']\n",
    "nstn, ntimes = np.shape(stndata)\n",
    "del datatemp\n",
    "\n",
    "# initialize\n",
    "metric_merge_stn = np.nan * np.zeros(nstn) # accuracy metric of merged reanalysis at station points\n",
    "error_merge_stn = np.nan * np.zeros([nstn,ntimes]) # mean error (merge minus observation)\n",
    "metric_reacorr_stn = [''] * reanum # accuracy metric of corrected reanalysis at station points\n",
    "error_reacorr_stn = [''] * reanum\n",
    "for i in range(reanum):\n",
    "    metric_reacorr_stn[i] = np.nan * np.zeros(nstn)\n",
    "    error_reacorr_stn[i] = np.nan * np.zeros([nstn,ntimes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "    lay1=1\n",
    "    # extract train and test index for layer-1\n",
    "    if var == 'trange':\n",
    "        vari = 'tmean'  # trange and tmean have the same index\n",
    "    else:\n",
    "        vari = var\n",
    "    trainindex1 = taintestindex[vari + '_trainindex1'][lay1, :]\n",
    "    testindex1 = taintestindex[vari + '_testindex1'][lay1, :]\n",
    "    stndata_trainl1 = stndata[trainindex1, :]\n",
    "    stndata_testl1 = stndata[testindex1, :]\n",
    "    stnlle_trainl1 = stnlle[trainindex1, :]\n",
    "    stnlle_testl1 = stnlle[testindex1, :]\n",
    "\n",
    "    # merging weight of different reanalysis products at station points (trainindex1)\n",
    "    reacorr_trainl1 = ['']*reanum\n",
    "    for rr in range(reanum):\n",
    "        reacorr_trainl1[rr] = np.zeros(np.shape(stndata_trainl1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "        lay2=3\n",
    "        # extract train and test index for layer-2 (subsets of trainindex1)\n",
    "        trainindex2 = taintestindex[vari + '_trainindex2'][lay1, lay2, :]\n",
    "        testindex2 = taintestindex[vari + '_testindex2'][lay1, lay2, :]\n",
    "        stndata_trainl2 = stndata[trainindex2, :]\n",
    "        stndata_testl2 = stndata[testindex2, :]\n",
    "        stnlle_trainl2 = stnlle[trainindex2, :]\n",
    "        stnlle_testl2 = stnlle[testindex2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:96: RuntimeWarning: Mean of empty slice\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:97: RuntimeWarning: Mean of empty slice\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:100: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:100: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:108: RuntimeWarning: invalid value encountered in greater\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:109: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "        for rr in range(reanum):\n",
    "            readata_trainl2 = readata_stn[rr][trainindex2, :]\n",
    "            readata_testl2 = readata_stn[rr][testindex2, :]\n",
    "\n",
    "            # calculate corrected reanalysis data\n",
    "            # calculate anomaly at the train stations\n",
    "            anom_ori = calculate_anomaly(readata_trainl2, stndata_trainl2, hwsize, corrmode,\n",
    "                                         upbound=anombound[1], lowbound=[0])\n",
    "            # extrapolate the ratio to the test stations (in layer-2)\n",
    "            anom_ext = extrapolation(stnlle_trainl2[:, 0], stnlle_trainl2[:, 1], anom_ori,\n",
    "                                      stnlle_testl2[:, 0], stnlle_testl2[:, 1], nearnum)\n",
    "            # correct data at the test stations\n",
    "            readata_testl2_corr = error_correction(readata_testl2, anom_ext, mode=corrmode)\n",
    "            tf, index = ismember(testindex2, trainindex1)\n",
    "            reacorr_trainl1[rr][index, :] = readata_testl2_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49492594, 0.31545068, 0.39060605, 0.12309098, 0.98420048,\n",
       "       0.4857062 , 0.69419902, 0.5060986 , 0.33048425, 0.81869946])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=np.random.rand(10) \n",
    "np.convolve(z, np.ones((1,)) / 1, mode='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "file='/Users/localuser/Downloads/output_19900101-19900131.npz'\n",
    "d=np.load(file)\n",
    "d3=d['tmean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-290-9457d88f2a76>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-290-9457d88f2a76>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    io.savemat('test.mat',{'tnew':d1}\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(d1[:,:,0])\n",
    "plt.colorbar()\n",
    "plt.clim([-20,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prcp_stn',\n",
       " 'tmean_stn',\n",
       " 'trange_stn',\n",
       " 'stn_ID',\n",
       " 'stn_lle',\n",
       " 'stn_row',\n",
       " 'stn_col']"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file='/Users/localuser/GMET/pyGMET_NA/stndata_whole.npz'\n",
    "d=np.load(file)\n",
    "d.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-34.24548875 -34.24572272 -34.24580942 -34.24590073 -34.24599655\n",
      " -34.24609675 -34.24620122 -34.24630984 -34.2464225  -34.24653909\n",
      " -34.24665949 -34.24678358 -34.24691125 -34.2470424  -34.24717691\n",
      " -34.24731466 -34.24745556 -34.24759948 -34.24774632 -34.24814125\n",
      " -34.24856776 -34.24899648 -34.24942729 -34.24986011 -34.25029482\n",
      " -34.25073134 -34.25116954 -34.25160933 -34.2520506  -34.25249324\n",
      " -34.25293714 -34.25338218 -34.25382827 -34.25427528 -34.2547231\n",
      " -34.25517161 -34.2556207  -34.25607025 -34.25652014 -34.25697024\n",
      " -34.25742044 -34.25787062 -34.25832065 -34.25877041 -34.25921977\n",
      " -34.25966861 -34.2601168  -34.2605642  -34.26101071 -34.26145618\n",
      " -34.26190048 -34.26234349 -34.26278507 -34.26322509 -34.26366342\n",
      " -34.26409992 -34.26453447 -34.26496692 -34.26539715 -34.26582501\n",
      " -34.26625037 -34.2666731  -34.26709306 -34.26751011 -34.26792411\n",
      " -34.26833494 -34.26874244 -34.26914648 -34.26954693 -34.26994364\n",
      " -34.27033648 -34.27072531 -34.27110998 -34.27149037 -34.27186633\n",
      " -34.27223772 -34.27260441 -34.27296625 -34.27332311 -34.27364537\n",
      " -34.27391214 -34.27417375 -34.27443009 -34.27474497 -34.27506891\n",
      " -34.27538694 -34.27565194 -34.27587966 -34.27612976 -34.27642227\n",
      " -34.27669447 -34.27690165 -34.27710247 -34.27729685 -34.27748469\n",
      " -34.27766588 -34.27784034 -34.27800797 -34.27816867 -34.27832236]\n",
      "[125.07964457  94.12199636  60.88045843  47.82996546  49.98833042\n",
      "  76.53777373 104.937276   129.29618054 145.11644786 158.68799298\n",
      " 164.51775757 186.06051983 216.62853184 241.7441774  262.85461351\n",
      " 263.32750883 231.93440292 225.3603452  245.70691146 285.91799148\n",
      " 305.93709435 303.82465795 340.21521028 353.66494735 330.52741168\n",
      " 320.32701152 333.49855083 346.72570353 358.67807215 364.21232606\n",
      " 342.74115114 330.62479993 344.32467694 359.61580407 363.66848464\n",
      " 343.06238251 327.61862732 307.56835561 279.86865501 266.62118695\n",
      " 259.10826016 245.66216333 228.22283972 227.24866605 237.31324356\n",
      " 251.96905034 266.50841253 283.89727084 296.74020891 305.70380997\n",
      " 316.70117737 329.77902008 344.27684658 357.87524546 373.2888753\n",
      " 355.69380728 333.62625642 312.34209279 294.13811767 287.50292029\n",
      " 290.25794988 296.13019068 320.37506934 323.24234084 305.25851153\n",
      " 287.73207325 286.88550366 289.50742125 287.81132267 291.03491271\n",
      " 292.5719871  304.53648497 320.01567733 322.30544884 315.59582464\n",
      " 295.79031945 284.07962163 277.74602869 267.28392689 250.3905387\n",
      " 229.79416284 207.01362877 202.36210003 178.41717977 163.22599609\n",
      " 181.66400725 226.70933839 231.08644328 217.41407712 210.30525239\n",
      " 190.47494523 167.65297892 155.02594442 152.86039322 144.20154295\n",
      " 115.41789519  87.1585423   91.83451034 118.37144281 118.39831282]\n"
     ]
    }
   ],
   "source": [
    "print(d1[770,1000:1100,30])\n",
    "print(d2[770,1000:1100,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.savemat('test.mat',{'tm':tm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
