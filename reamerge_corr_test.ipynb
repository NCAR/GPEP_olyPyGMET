{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var is  prcp\n",
      "weightmode is  BMA\n",
      "years are  2017 2018\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import auxiliary as au\n",
    "# from matplotlib import pyplot as plt\n",
    "from scipy import io\n",
    "import os\n",
    "# import sys\n",
    "# import h5py\n",
    "# import time\n",
    "# import random\n",
    "# import datetime\n",
    "from bma_merge import bma\n",
    "from auxiliary_merge import *\n",
    "\n",
    "\n",
    "def empirical_cdf(data, probtar):\n",
    "    # data: vector of data\n",
    "    data2 = data[~np.isnan(data)]\n",
    "    if len(data2) > 0:\n",
    "        ds = np.sort(data2)\n",
    "        probreal = np.arange(len(data2)) / (len(data2) + 1)\n",
    "        ecdf_out = np.interp(probtar, probreal, ds)\n",
    "    else:\n",
    "        ecdf_out = np.nan * np.zeros(len(probtar))\n",
    "    return ecdf_out\n",
    "\n",
    "\n",
    "def cdf_correction(cdf_ref, value_ref, cdf_raw, value_raw, value_tar):\n",
    "    prob_tar = np.interp(value_tar, value_raw, cdf_raw)\n",
    "    value_out = np.interp(prob_tar, cdf_ref, value_ref)\n",
    "    return value_out\n",
    "\n",
    "\n",
    "def calculate_anomaly(datatar, dataref, hwsize, amode, upbound=5, lowbound=0.2):\n",
    "    # datatar, dataref: 2D [nstn, ntime]\n",
    "    # amode: anomaly mode ('ratio' or 'diff')\n",
    "    # hwsize: define time window (2*hwsize+1) used to calculate ratio (as ratio for a specific day is too variable)\n",
    "    # upbound/lowbound: upper and lower limitation of ratio/difference\n",
    "    if np.ndim(datatar) == 1:  # only one time step\n",
    "        datatar = datatar[:, np.newaxis]\n",
    "        dataref = dataref[:, np.newaxis]\n",
    "\n",
    "    nstn, ntime = np.shape(datatar)\n",
    "    if ntime < hwsize * 2 + 1:\n",
    "        print('The window size is larger than time steps when calculating ratio between tar and ref datasets')\n",
    "        print('Please set a smaller hwsize')\n",
    "        sys.exit()\n",
    "\n",
    "    anom = np.ones([nstn, ntime])\n",
    "\n",
    "    for i in range(ntime):\n",
    "        if i < hwsize:\n",
    "            windex = np.arange(hwsize * 2 + 1)\n",
    "        elif i >= ntime - hwsize:\n",
    "            windex = np.arange(ntime - hwsize * 2 - 1, ntime)\n",
    "        else:\n",
    "            windex = np.arange(i - hwsize, i + hwsize + 1)\n",
    "        dtari = np.nanmean(datatar[:, windex], axis=1)\n",
    "        drefi = np.nanmean(dataref[:, windex], axis=1)\n",
    "\n",
    "        if amode == 'ratio':\n",
    "            temp = drefi / dtari\n",
    "            temp[(dtari == 0) & (drefi == 0)] = 1\n",
    "            anom[:, i] = temp\n",
    "        elif amode == 'diff':\n",
    "            anom[:, i] = drefi - dtari\n",
    "        else:\n",
    "            sys.exit('Unknow amode. Please use either ratio or diff')\n",
    "\n",
    "    anom[anom > upbound] = upbound\n",
    "    anom[anom < lowbound] = lowbound\n",
    "    return anom\n",
    "\n",
    "\n",
    "def findnearstn(stnlat, stnlon, tarlat, tarlon, nearnum, noself):\n",
    "    # only use lat/lon to find near stations without considering distance in km\n",
    "    # stnlat/stnlon: 1D\n",
    "    # tarlat/tarlon: 1D or 2D\n",
    "    # noself: 1--stnlat and tarlat have overlapped stations, which should be excluded from stnlat\n",
    "\n",
    "    stnll = np.zeros([len(stnlat), 2])\n",
    "    stnll[:, 0] = stnlat\n",
    "    stnll[:, 1] = stnlon\n",
    "\n",
    "    if len(np.shape(tarlat)) == 1:\n",
    "        num = len(tarlat)\n",
    "        nearstn_loc = -1 * np.ones([num, nearnum], dtype=int)\n",
    "        nearstn_dist = -1 * np.ones([num, nearnum], dtype=float)\n",
    "        for i in range(num):\n",
    "            if np.isnan(tarlat[i]) or np.isnan(tarlon[i]):\n",
    "                continue\n",
    "            tari = np.array([tarlat[i], tarlon[i]])\n",
    "            dist = au.distance(tari, stnll)\n",
    "            dist[np.isnan(dist)] = 1000000000\n",
    "            if noself == 1:\n",
    "                dist[dist == 0] = np.inf  # may not be perfect, but work for SCDNA\n",
    "            indi = np.argsort(dist)\n",
    "            nearstn_loc[i, :] = indi[0:nearnum]\n",
    "            nearstn_dist[i, :] = dist[nearstn_loc[i, :]]\n",
    "    elif len(np.shape(tarlat)) == 2:\n",
    "        nrows, ncols = np.shape(tarlat)\n",
    "        nearstn_loc = -1 * np.ones([nrows, ncols, nearnum], dtype=int)\n",
    "        nearstn_dist = -1 * np.ones([nrows, ncols, nearnum], dtype=float)\n",
    "        for r in range(nrows):\n",
    "            print('rows', r, nrows)\n",
    "            for c in range(ncols):\n",
    "                if np.isnan(tarlat[r, c]) or np.isnan(tarlon[r, c]):\n",
    "                    continue\n",
    "                tari = np.array([tarlat[r, c], tarlon[r, c]])\n",
    "                dist = au.distance(tari, stnll)\n",
    "                dist[np.isnan(dist)] = 1000000000\n",
    "                indi = np.argsort(dist)\n",
    "                nearstn_loc[r, c, :] = indi[0:nearnum]\n",
    "                nearstn_dist[r, c, :] = dist[nearstn_loc[r, c, :]]\n",
    "    else:\n",
    "        print('The dimensions of tarlat or tarlon are larger than 2')\n",
    "        sys.exit()\n",
    "\n",
    "    return nearstn_loc, nearstn_dist\n",
    "\n",
    "\n",
    "def error_correction_stn(corrmode, stndata_i2_near, nearstn_weighti2, readata_stn_i2, readata_i2_near, ecdf_prob):\n",
    "    # corrmode: QM, Mul_Climo, Mul_Daily, Add_Climo, Add_Climo\n",
    "    nearstn_numi2, ntimes = np.shape(stndata_i2_near)\n",
    "    corrdata_out = np.zeros(ntimes)\n",
    "    if corrmode == 'QM':\n",
    "        cdf_rea = empirical_cdf(readata_stn_i2, ecdf_prob)\n",
    "        for j in range(nearstn_numi2):\n",
    "            cdf_ref = empirical_cdf(stndata_i2_near[j, :], ecdf_prob)\n",
    "            qmdata_rj = cdf_correction(ecdf_prob, cdf_ref, ecdf_prob, cdf_rea, readata_stn_i2)\n",
    "            corrdata_out = corrdata_out + qmdata_rj * nearstn_weighti2[j]\n",
    "        corrdata_out = corrdata_out / np.sum(nearstn_weighti2)\n",
    "    elif corrmode[0:3] == 'Mul' or corrmode[0:3] == 'Add':\n",
    "        # multplicative correction or additive correction\n",
    "        if corrmode[4:] == 'Daily':\n",
    "            dtar = readata_i2_near\n",
    "            dref = stndata_i2_near\n",
    "        elif corrmode[4:] == 'Climo':\n",
    "            dtar = np.nanmean(readata_i2_near, axis=1)\n",
    "            dtar = dtar[:, np.newaxis]\n",
    "            dref = np.nanmean(stndata_i2_near, axis=1)\n",
    "            dref = dref[:, np.newaxis]\n",
    "        else:\n",
    "            sys.exit('Unknown corrmode')\n",
    "        if corrmode[0:3] == 'Mul':\n",
    "            corrfactor_i2_near = calculate_anomaly(dtar, dref, 0, 'ratio', 10, 0)  # 10 is default max limit\n",
    "        else:\n",
    "            corrfactor_i2_near = calculate_anomaly(dtar, dref, 0, 'diff', 9999, -9999)\n",
    "        weight_use = np.tile(nearstn_weighti2, (np.shape(corrfactor_i2_near)[1], 1)).T\n",
    "        weight_use[np.isnan(corrfactor_i2_near)] = np.nan\n",
    "        corrfactor_i2 = np.nansum(corrfactor_i2_near * weight_use, axis=0) / np.nansum(weight_use)\n",
    "        if corrmode[0:3] == 'Mul':\n",
    "            corrdata_out = readata_stn_i2 * corrfactor_i2\n",
    "        else:\n",
    "            corrdata_out = readata_stn_i2 + corrfactor_i2\n",
    "    else:\n",
    "        sys.exit('Unknown corrmode')\n",
    "\n",
    "    return corrdata_out\n",
    "\n",
    "\n",
    "def error_correction(dataori, anomaly, mode='ratio'):\n",
    "    # default: time is the last dimension\n",
    "    if mode == 'ratio':\n",
    "        datacorr = dataori * anomaly\n",
    "    elif mode == 'diff':\n",
    "        datacorr = dataori + anomaly\n",
    "    else:\n",
    "        sys.exit('Wrong error correction mode')\n",
    "    return datacorr\n",
    "\n",
    "\n",
    "def calweight(obs, rea, mode, preprocess=True):\n",
    "    ntimes, reanum = np.shape(rea)\n",
    "    if preprocess:\n",
    "        # delete the nan values\n",
    "        ind_nan = np.isnan(obs + np.sum(rea, axis=1))\n",
    "        obs = obs[~ind_nan]\n",
    "        rea = rea[~ind_nan, :]\n",
    "\n",
    "    if len(obs) > 2:\n",
    "        if mode == 'BMA':\n",
    "            weight, sigma, sigma_s = bma(rea, obs)\n",
    "        else:\n",
    "            met = np.zeros(reanum)\n",
    "            if mode == 'RMSE':\n",
    "                for i in range(reanum):\n",
    "                    met[i] = np.sqrt(np.sum(np.square(obs - rea[:, i])) / len(obs))  # RMSE\n",
    "                weight = 1 / (met ** 2)\n",
    "            elif mode == 'CC':\n",
    "                for i in range(reanum):\n",
    "                    met[i] = np.corrcoef(obs, rea[:, i])[0][1]\n",
    "                weight = (met ** 2)\n",
    "    else:\n",
    "        weight = np.ones(reanum) / reanum\n",
    "    if np.any(np.isnan(weight)):\n",
    "        weight = np.ones(reanum) / reanum\n",
    "    return weight\n",
    "\n",
    "\n",
    "def weightmerge(data, weight):\n",
    "    if np.ndim(data) == 2:\n",
    "        weight2 = weight.copy()\n",
    "        weight2[np.isnan(data)] = np.nan\n",
    "        dataout = np.nansum(data * weight2, axis=1) / np.nansum(weight2, axis=1)\n",
    "    elif np.ndim(data) == 3:\n",
    "        weight2 = weight.copy()\n",
    "        weight2[np.isnan(data)] = np.nan\n",
    "        dataout = np.nansum(data * weight2, axis=2) / np.nansum(weight2, axis=2)\n",
    "        dataout[np.isnan(data[:, :, 0])] = np.nan\n",
    "    return dataout\n",
    "\n",
    "\n",
    "def correction_merge_stn(stndata, ecdf_prob, readata_stn, nearstn_loc, nearstn_dist, var, corrmode, weightmode):\n",
    "    # corrmode = 'QM'  # QM, Mul_Climo, Mul_Daily, Add_Climo, Add_Climo\n",
    "    # use 2-layer cross-validation to estimate the weight and independent data of merge/correction data\n",
    "    reanum, nstn, ntimes = np.shape(readata_stn)\n",
    "\n",
    "    # initialization\n",
    "    reacorr_stn = np.nan * np.zeros([reanum, nstn, ntimes], dtype=np.float32)  # corrected reanalysis data\n",
    "    reamerge_weight_stn = np.nan * np.zeros([nstn, reanum], dtype=np.float32)  # weight used to obtain reamerge_stn\n",
    "    reamerge_stn = np.nan * np.zeros([nstn, ntimes], dtype=np.float32)  # merged reanalysis at station points\n",
    "\n",
    "    for i1 in range(nstn):  # layer-1\n",
    "        if np.mod(i1, 1000) == 0:\n",
    "            print(i1)\n",
    "        if np.isnan(stndata[i1, 0]):\n",
    "            continue\n",
    "        nearstn_loci1 = nearstn_loc[i1, :]\n",
    "        nearstn_disti1 = nearstn_dist[i1, :]\n",
    "        induse = nearstn_loci1 > -1\n",
    "        nearstn_loci1 = nearstn_loci1[induse]\n",
    "        nearstn_disti1 = nearstn_disti1[induse]\n",
    "        nearstn_numi1 = len(nearstn_loci1)\n",
    "        if nearstn_numi1 == 0:\n",
    "            sys.exit('No near station for the target station (layer-1)')\n",
    "\n",
    "        # start layer-2\n",
    "        reamerge_weight_i2 = np.zeros([nearstn_numi1, reanum])\n",
    "        for i2 in range(nearstn_numi1):  # layer-2\n",
    "            nearstn_loci2 = nearstn_loc[nearstn_loci1[i2], :]\n",
    "            nearstn_disti2 = nearstn_dist[nearstn_loci1[i2], :]\n",
    "            induse = (nearstn_loci2 > -1) & (nearstn_loci2 != i1)  # i1 should be independent\n",
    "            nearstn_loci2 = nearstn_loci2[induse]\n",
    "            nearstn_disti2 = nearstn_disti2[induse]\n",
    "            maxd = np.max([np.max(nearstn_disti2) + 1, 100])\n",
    "            nearstn_weighti2 = au.distanceweight(nearstn_disti2, maxd, 3)\n",
    "            nearstn_weighti2 = nearstn_weighti2 / np.sum(nearstn_weighti2)\n",
    "\n",
    "            nearstn_numi2 = len(nearstn_loci2)\n",
    "            if nearstn_numi2 == 0:\n",
    "                sys.exit('No near station for the target station (layer-1)')\n",
    "\n",
    "            # data at i2 station\n",
    "            stndata_i2 = stndata[nearstn_loci1[i2], :]\n",
    "            stndata_i2_near = stndata[nearstn_loci2, :]\n",
    "            readata_stn_i2 = readata_stn[:, nearstn_loci1[i2], :]\n",
    "            readata_i2_near = readata_stn[:, nearstn_loci2, :]\n",
    "\n",
    "            # error correction for each reanalysis dataset using different modes\n",
    "            corrdata_i2 = np.zeros([ntimes, reanum])\n",
    "            for r in range(reanum):\n",
    "                corrdata_i2[:, r] = error_correction_stn(corrmode, stndata_i2_near, nearstn_weighti2,\n",
    "                                                         readata_stn_i2[r, :], readata_i2_near[r, :, :], ecdf_prob)\n",
    "\n",
    "            # calculate merging weight for i2\n",
    "            if weightmode == 'BMA' and var == 'prcp':\n",
    "                # exclude zero precipitation and carry out box-cox transformation\n",
    "                datatemp = np.zeros([ntimes, reanum + 1])\n",
    "                datatemp[:, 0] = stndata_i2\n",
    "                datatemp[:, 1:] = corrdata_i2\n",
    "                ind0 = np.sum(datatemp >= 0.01, axis=1) == (reanum + 1)  # positive hit events\n",
    "                dobs = box_cox_transform(stndata_i2[ind0])\n",
    "                drea = box_cox_transform(corrdata_i2[ind0, :])\n",
    "            else:\n",
    "                dobs = stndata_i2\n",
    "                drea = corrdata_i2\n",
    "            reamerge_weight_i2[i2, :] = calweight(dobs, drea, weightmode)\n",
    "        # end layer-2\n",
    "\n",
    "        stndata_i1 = stndata[i1, :]\n",
    "        stndata_i1_near = stndata[nearstn_loci1, :]\n",
    "        readata_stn_i1 = readata_stn[:, i1, :]\n",
    "        readata_i1_near = readata_stn[:, nearstn_loci1, :]\n",
    "        maxd = np.max([np.max(nearstn_disti1) + 1, 100])\n",
    "        nearstn_weighti1 = au.distanceweight(nearstn_disti1, maxd, 3)\n",
    "        nearstn_weighti1 = nearstn_weighti1 / np.sum(nearstn_weighti1)\n",
    "\n",
    "        # get corrected data at i1\n",
    "        corrdata_i1 = np.zeros([ntimes, reanum])\n",
    "        for r in range(reanum):\n",
    "            corrdata_i1[:, r] = error_correction_stn(corrmode, stndata_i1_near, nearstn_weighti1,\n",
    "                                                     readata_stn_i1[r, :], readata_i1_near[r, :, :], ecdf_prob)\n",
    "        reacorr_stn[:, i1, :] = corrdata_i1.T\n",
    "\n",
    "        # get merging weight at i1 and merge reanalysis\n",
    "        # note: this weight is just for independent merging so we can estimate the error of merged reanalysis\n",
    "        # the real weight will be estimated using just one-layer cross-validation\n",
    "        weight_use = np.tile(nearstn_weighti1, (reanum, 1)).T\n",
    "        weight_i1 = np.sum(weight_use * reamerge_weight_i2, axis=0)\n",
    "        weight_i1 = weight_i1 / np.sum(weight_i1)\n",
    "\n",
    "        weight_use = np.tile(weight_i1, (ntimes, 1))\n",
    "        weight_use[np.isnan(corrdata_i1)] = np.nan\n",
    "        # if weightmode == 'BMA' and var == 'prcp':\n",
    "        #     # the merging after box-cox transformation underestimates precipitation in southeast US\n",
    "        #     # and the rationality of box-cox should be revisited\n",
    "        #     reamerge_stni1 = np.nansum(weight_use * box_cox_transform(corrdata_i1), axis=1) / np.nansum(weight_use, axis=1)\n",
    "        #     reamerge_stni1 = box_cox_recover(reamerge_stni1)\n",
    "        # else:\n",
    "        #     reamerge_stni1 = np.nansum(weight_use * corrdata_i1, axis=1) / np.nansum(weight_use, axis=1)\n",
    "        reamerge_stni1 = np.nansum(weight_use * corrdata_i1, axis=1) / np.nansum(weight_use, axis=1)\n",
    "        reamerge_stn[i1, :] = reamerge_stni1\n",
    "\n",
    "        # get the final merging weight\n",
    "        if weightmode == 'BMA' and var == 'prcp':\n",
    "            # exclude zero precipitation and carry out box-cox transformation\n",
    "            datatemp = np.zeros([ntimes, reanum + 1])\n",
    "            datatemp[:, 0] = stndata_i1\n",
    "            datatemp[:, 1:] = corrdata_i1\n",
    "            ind0 = np.sum(datatemp >= 0.01, axis=1) == (reanum + 1)  # positive hit events\n",
    "            dobs = box_cox_transform(stndata_i1[ind0])\n",
    "            drea = box_cox_transform(corrdata_i1[ind0, :])\n",
    "        else:\n",
    "            dobs = stndata_i1\n",
    "            drea = corrdata_i1\n",
    "        reamerge_weight_stn[i1, :] = calweight(dobs, drea, weightmode)\n",
    "\n",
    "    # note: reamerge_weight_stn is the final merging weight, and reacorr_stn is the final corrected data\n",
    "    # but reamerge_stn is just independent merging estimates which is calculated from 2-layer cross validation\n",
    "    return reamerge_stn, reamerge_weight_stn, reacorr_stn\n",
    "\n",
    "\n",
    "def correction_rea(stndata, ecdf_prob, readata_stn, nearstn_loc, nearstn_dist, corrmode):\n",
    "    # compare the performance of daily-scale multiplicative correction and QM\n",
    "    reanum, nstn, ntimes = np.shape(readata_stn)\n",
    "    nprob = len(ecdf_prob)\n",
    "\n",
    "    # initialization\n",
    "    reacorr = np.nan * np.zeros([reanum, nstn, ntimes], dtype=np.float32)  # corrected reanalysis data\n",
    "\n",
    "    for i1 in range(nstn):  # layer-1\n",
    "        # if np.mod(i1, 1000) == 0:\n",
    "        #     print(i1)\n",
    "        if np.isnan(stndata[i1, 0]):\n",
    "            continue\n",
    "\n",
    "        nearstn_loci1 = nearstn_loc[i1, :]\n",
    "        nearstn_disti1 = nearstn_dist[i1, :]\n",
    "        induse = nearstn_loci1 > -1\n",
    "        nearstn_loci1 = nearstn_loci1[induse]\n",
    "        nearstn_disti1 = nearstn_disti1[induse]\n",
    "        nearstn_numi1 = len(nearstn_loci1)\n",
    "        if nearstn_numi1 == 0:\n",
    "            sys.exit('No near station for the target station (layer-1)')\n",
    "\n",
    "        stndata_i1_near = stndata[nearstn_loci1, :]\n",
    "        readata_stn_i1 = readata_stn[:, i1, :]\n",
    "        readata_i1_near = readata_stn[:, nearstn_loci1, :]\n",
    "        maxd = np.max([np.max(nearstn_disti1) + 1, 100])\n",
    "        nearstn_weighti1 = au.distanceweight(nearstn_disti1, maxd, 3)\n",
    "        nearstn_weighti1 = nearstn_weighti1 / np.sum(nearstn_weighti1)\n",
    "\n",
    "        # get corrected data at i1\n",
    "        corrdata_i1 = np.zeros([ntimes, reanum])\n",
    "        for r in range(reanum):\n",
    "            corrdata_i1[:, r] = error_correction_stn(corrmode, stndata_i1_near, nearstn_weighti1,\n",
    "                                                     readata_stn_i1[r, :], readata_i1_near[r, :, :], ecdf_prob)\n",
    "        reacorr[:, i1, :] = corrdata_i1.T\n",
    "\n",
    "    return reacorr\n",
    "\n",
    "\n",
    "def correction_merge_grid(stndata, readata_raw, readata_stn, reacorr_stn, reamerge_stn, reamerge_weight_stn,\n",
    "                          neargrid_loc,\n",
    "                          neargrid_dist, merge_choice, mask, hwsize, corrmode, anombound, var, weightmode):\n",
    "    nrows, ncols, nearnum = np.shape(neargrid_loc)\n",
    "    reanum, nstn, nday = np.shape(readata_stn)\n",
    "\n",
    "    neargrid_loc = neargrid_loc.copy()\n",
    "    neargrid_dist = neargrid_dist.copy()\n",
    "    mask2 = np.tile(mask[:, :, np.newaxis], (1, 1, nearnum))\n",
    "    neargrid_loc[mask2 != 1] = -1\n",
    "    neargrid_dist[mask2 != 1] = np.nan\n",
    "    del mask2\n",
    "\n",
    "    # correct raw gridded reanalysis data using all stations\n",
    "    corr_data = np.nan * np.zeros([reanum, nrows, ncols, nday], dtype=np.float32)\n",
    "    for rr in range(reanum):\n",
    "        # calculate correction ratio at all station point\n",
    "        anom_ori = calculate_anomaly(readata_stn[rr, :, :], stndata[:, :],\n",
    "                                     hwsize, corrmode, upbound=anombound[1], lowbound=anombound[0])\n",
    "        anom_ext = extrapolation(anom_ori, neargrid_loc, neargrid_dist)\n",
    "        corr_data[rr, :, :, :] = error_correction(readata_raw[rr, :, :, :], anom_ext, mode=corrmode)\n",
    "\n",
    "    # first error estimation\n",
    "    corr_error = np.nan * np.zeros([reanum, nrows, ncols, nday], dtype=np.float32)\n",
    "    for rr in range(reanum):\n",
    "        corr_error[rr, :, :, :] = extrapolation(reacorr_stn[rr, :, :] - stndata, neargrid_loc, neargrid_dist)\n",
    "    merge_error0 = extrapolation(reamerge_stn - stndata, neargrid_loc, neargrid_dist)\n",
    "\n",
    "    if var == 'prcp' and weightmode == 'BMA':\n",
    "        corr_data = box_cox_transform(corr_data)\n",
    "        stndata = box_cox_transform(stndata)\n",
    "        reamerge_stn = box_cox_transform(reamerge_stn)\n",
    "        reacorr_stn = box_cox_transform(reacorr_stn)\n",
    "\n",
    "        # correction error in normal space (box-cox)\n",
    "        corr_error_bc = np.nan * np.zeros([reanum, nrows, ncols, nday], dtype=np.float32)\n",
    "        for rr in range(reanum):\n",
    "            corr_error_bc[rr, :, :, :] = extrapolation(reacorr_stn[rr, :, :] - stndata, neargrid_loc, neargrid_dist)\n",
    "        merge_error0_bc = extrapolation(reamerge_stn - stndata, neargrid_loc, neargrid_dist)\n",
    "\n",
    "    # merge reanalysis data\n",
    "    merge_data = np.nan * np.zeros([nrows, ncols, nday], dtype=np.float32)\n",
    "    weight_grid = extrapolation(reamerge_weight_stn, neargrid_loc, neargrid_dist)\n",
    "    for i in range(nday):\n",
    "        datain = np.zeros([nrows, ncols, reanum])\n",
    "        for rr in range(reanum):\n",
    "            datain[:, :, rr] = corr_data[rr, :, :, i]\n",
    "        merge_data[:, :, i] = weightmerge(datain, weight_grid)\n",
    "\n",
    "    # calculate the error of merged data (this is actually independent with merged data estimation)\n",
    "    merge_error = np.nan * np.zeros([nrows, ncols, nday], dtype=np.float32)\n",
    "    for r in range(nrows):\n",
    "        for c in range(ncols):\n",
    "            if not np.isnan(mask[r, c]):\n",
    "                chi = merge_choice[r, c]\n",
    "                if chi > 0:\n",
    "                    merge_error[r, c, :] = corr_error[chi - 1, r, c, :]\n",
    "                else:\n",
    "                    merge_error[r, c, :] = merge_error0[r, c, :]\n",
    "\n",
    "    if var == 'prcp' and weightmode == 'BMA':\n",
    "        merge_error_bc = np.nan * np.zeros([nrows, ncols, nday], dtype=np.float32)\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if not np.isnan(mask[r, c]):\n",
    "                    chi = merge_choice[r, c]\n",
    "                    if chi > 0:\n",
    "                        merge_error_bc[r, c, :] = corr_error_bc[chi - 1, r, c, :]\n",
    "                    else:\n",
    "                        merge_error_bc[r, c, :] = merge_error0_bc[r, c, :]\n",
    "\n",
    "    if var == 'prcp' and weightmode == 'BMA':\n",
    "        merge_error_out = [''] * 2\n",
    "        merge_error_out[0] = merge_error\n",
    "        merge_error_out[1] = merge_error_bc\n",
    "        merge_data = box_cox_recover(merge_data)\n",
    "        corr_data = box_cox_recover(corr_data)\n",
    "    else:\n",
    "        merge_error_out = ['']\n",
    "        merge_error_out[0] = merge_error\n",
    "    return corr_data, corr_error, merge_data, merge_error_out\n",
    "\n",
    "\n",
    "def mse_error(stndata, reacorr_stn, reamerge_stn, neargrid_loc, neargrid_dist, merge_choice, mask, var, pcptrans=False):\n",
    "    nrows, ncols, nearnum = np.shape(neargrid_loc)\n",
    "    reanum, nstn, nday = np.shape(reacorr_stn)\n",
    "\n",
    "    neargrid_loc = neargrid_loc.copy()\n",
    "    neargrid_dist = neargrid_dist.copy()\n",
    "    mask2 = np.tile(mask[:, :, np.newaxis], (1, 1, nearnum))\n",
    "    neargrid_loc[mask2 != 1] = -1\n",
    "    neargrid_dist[mask2 != 1] = np.nan\n",
    "    del mask2\n",
    "\n",
    "    if var == 'prcp' and pcptrans == True:\n",
    "        stndata = box_cox_transform(stndata)\n",
    "        reamerge_stn = box_cox_transform(reamerge_stn)\n",
    "        reacorr_stn = box_cox_transform(reacorr_stn)\n",
    "\n",
    "    corr_error = np.nan * np.zeros([reanum, nrows, ncols, nday], dtype=np.float32)\n",
    "    for rr in range(reanum):\n",
    "        corr_error[rr, :, :, :] = extrapolation((reacorr_stn[rr, :, :] - stndata) ** 2, neargrid_loc, neargrid_dist)\n",
    "    corr_error = corr_error ** 0.5\n",
    "    merge_error0 = extrapolation((reamerge_stn - stndata) ** 2, neargrid_loc, neargrid_dist)\n",
    "    merge_error0 = merge_error0 ** 0.5\n",
    "\n",
    "    mse_error = np.nan * np.zeros([nrows, ncols, nday], dtype=np.float32)\n",
    "    for r in range(nrows):\n",
    "        for c in range(ncols):\n",
    "            if not np.isnan(mask[r, c]):\n",
    "                chi = merge_choice[r, c]\n",
    "                if chi > 0:\n",
    "                    mse_error[r, c, :] = corr_error[chi - 1, r, c, :]\n",
    "                else:\n",
    "                    mse_error[r, c, :] = merge_error0[r, c, :]\n",
    "\n",
    "    return mse_error\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# time periods and methods\n",
    "# var = 'prcp'  # ['prcp', 'tmean', 'trange']: this should be input from sbtach script\n",
    "# weightmode for merging: (CC, RMSE, BMA): Weight = CC**2, or Weight = 1/RMSE**2, or Weight = BMA\n",
    "# corrmode: QM, Mul_Climo, Mul_Daily, Add_Climo, Add_Climo\n",
    "# year range for merging. note weight is calculated using all data not limited by year\n",
    "\n",
    "# read from inputs\n",
    "# var = sys.argv[1]\n",
    "# weightmode = sys.argv[2]\n",
    "# corrmode =  sys.argv[3]\n",
    "# y1 = int(sys.argv[4])\n",
    "# y2 = int(sys.argv[5])\n",
    "# year = [y1, y2]\n",
    "\n",
    "# embeded\n",
    "var = 'prcp'\n",
    "weightmode = 'BMA'\n",
    "corrmode = 'QM'\n",
    "y1 = 2017\n",
    "y2 = 2018\n",
    "year = [y1, y2]\n",
    "\n",
    "print('var is ', var)\n",
    "print('weightmode is ', weightmode)\n",
    "print('years are ', y1, y2)\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# basic settings\n",
    "lontar = np.arange(-180 + 0.05, -50, 0.1)\n",
    "lattar = np.arange(85 - 0.05, 5, -0.1)\n",
    "nearnum = 10  # the number of nearby stations used to extrapolate points to grids (for correction and merging)\n",
    "anombound = [0.2, 10]  # upper and lower bound when calculating the anomaly for correction\n",
    "prefix = ['ERA5_', 'MERRA2_', 'JRA55_']\n",
    "\n",
    "### Local Mac settings\n",
    "# input files/paths\n",
    "gmet_stnfile = '/Users/localuser/Research/EMDNA/basicinfo/stnlist_whole.txt'\n",
    "gmet_stndatafile = '/Users/localuser/Research/EMDNA/stndata_whole.npz'\n",
    "file_mask = './DEM/NA_DEM_010deg_trim.mat'\n",
    "path_readowngrid = ['/Users/localuser/Research/EMDNA/downscale/ERA5',  # downscaled gridded data\n",
    "                    '/Users/localuser/Research/EMDNA/downscale/MERRA2',\n",
    "                    '/Users/localuser/Research/EMDNA/downscale/JRA55']\n",
    "file_readownstn = ['/Users/localuser/Research/EMDNA/downscale/ERA5_downto_stn_nearest.npz', # downscaled to stn points\n",
    "                   '/Users/localuser/Research/EMDNA/downscale/MERRA2_downto_stn_nearest.npz',\n",
    "                   '/Users/localuser/Research/EMDNA/downscale/JRA55_downto_stn_nearest.npz']\n",
    "# file_readownstn = ['/Users/localuser/Research/EMDNA/downscale/JRA55_downto_stn_nearest.npz']\n",
    "\n",
    "# output files/paths (can also be used as inputs once generated)\n",
    "near_path = '/Users/localuser/Research/EMDNA/correction'  # path to save near station for each grid/cell\n",
    "path_reacorr = '/Users/localuser/Research/EMDNA/correction' # path to save corrected reanalysis data at station points\n",
    "path_merge = '/Users/localuser/Research/EMDNA/merge'\n",
    "path_ecdf = '/Users/localuser/Research/EMDNA/merge/ECDF'\n",
    "### Local Mac settings\n",
    "\n",
    "\n",
    "# ### Plato settings\n",
    "# # input files/paths\n",
    "# gmet_stnfile = '/datastore/GLOBALWATER/CommonData/EMDNA/StnGridInfo/stnlist_whole.txt'\n",
    "# gmet_stndatafile = '/datastore/GLOBALWATER/CommonData/EMDNA/stndata_whole.npz'\n",
    "# file_mask = '/datastore/GLOBALWATER/CommonData/EMDNA/DEM/NA_DEM_010deg_trim.mat'\n",
    "# path_readowngrid = ['/datastore/GLOBALWATER/CommonData/EMDNA/ERA5_day_ds',  # downscaled gridded data\n",
    "#                    '/datastore/GLOBALWATER/CommonData/EMDNA/MERRA2_day_ds',\n",
    "#                    '/datastore/GLOBALWATER/CommonData/EMDNA/JRA55_day_ds']\n",
    "# file_readownstn = ['/datastore/GLOBALWATER/CommonData/EMDNA/ERA5_day_ds/ERA5_downto_stn_GWR.npz', # downscaled to stn points\n",
    "#                    '/datastore/GLOBALWATER/CommonData/EMDNA/MERRA2_day_ds/MERRA2_downto_stn_GWR.npz',\n",
    "#                    '/datastore/GLOBALWATER/CommonData/EMDNA/JRA55_day_ds/JRA55_downto_stn_GWR.npz']\n",
    "#\n",
    "# # output files/paths (can also be used as inputs once generated)\n",
    "# near_path = '/home/gut428/ReanalysisCorrMerge'  # path to save near station for each grid/cell\n",
    "# path_reacorr = '/home/gut428/ReanalysisCorrMerge/Reanalysis_corr'  # path to save corrected reanalysis data at station points\n",
    "# path_merge = '/home/gut428/ReanalysisCorrMerge/Reanalysis_merge'\n",
    "# path_ecdf = '/home/gut428/ReanalysisCorrMerge/ECDF'\n",
    "# ### Plato settings\n",
    "\n",
    "\n",
    "near_stnfile = near_path + '/near_stn_' + var + '.npz'\n",
    "near_gridfile = near_path + '/near_grid_' + var + '.npz'\n",
    "file_corrmerge_stn = path_merge + '/mergecorr_stn_' + var + '_GWRQM_' + weightmode + '.npz'  # file of indepedent corrected/merging data and merging weights\n",
    "file_mergechoice = path_merge + '/mergechoice_' + var + '_' + weightmode + '.npz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start basic processing\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "\n",
    "# basic processing\n",
    "print('start basic processing')\n",
    "\n",
    "# mask\n",
    "mask = io.loadmat(file_mask)\n",
    "mask = mask['DEM']\n",
    "mask[~np.isnan(mask)] = 1  # 1: valid pixels\n",
    "\n",
    "# meshed lat/lon of the target region\n",
    "reanum = len(file_readownstn)\n",
    "nrows, ncols = np.shape(mask)\n",
    "lontarm, lattarm = np.meshgrid(lontar, lattar)\n",
    "lontarm[np.isnan(mask)] = np.nan\n",
    "lattarm[np.isnan(mask)] = np.nan\n",
    "\n",
    "# date list\n",
    "date_list, date_number = m_DateList(1979, 2018, 'ByYear')\n",
    "\n",
    "# load observations for all stations\n",
    "datatemp = np.load(gmet_stndatafile)\n",
    "stndata = datatemp[var + '_stn']\n",
    "stnlle = datatemp['stn_lle']\n",
    "nstn, ntimes = np.shape(stndata)\n",
    "del datatemp\n",
    "\n",
    "# probability bins for QM\n",
    "binprob = 500\n",
    "ecdf_prob = np.arange(0, 1 + 1 / binprob, 1 / binprob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load near station information for points\n",
      "load near station information for grids\n"
     ]
    }
   ],
   "source": [
    "########################################################################################################################\n",
    "\n",
    "# find near stations\n",
    "# find near stations for all stations\n",
    "if os.path.isfile(near_stnfile):\n",
    "    print('load near station information for points')\n",
    "    with np.load(near_stnfile) as datatemp:\n",
    "        nearstn_loc = datatemp['nearstn_loc']\n",
    "        nearstn_dist = datatemp['nearstn_dist']\n",
    "    del datatemp\n",
    "else:\n",
    "    print('find near stations for station points')\n",
    "    stnllein = stnlle.copy()\n",
    "    stnllein[np.isnan(stndata[:, 0]), :] = np.nan\n",
    "    nearstn_loc, nearstn_dist = findnearstn \\\n",
    "        (stnllein[:, 0], stnllein[:, 1], stnllein[:, 0], stnllein[:, 1], nearnum, 1)\n",
    "    np.savez_compressed(near_stnfile, nearstn_loc=nearstn_loc, nearstn_dist=nearstn_dist)\n",
    "\n",
    "# find near stations for all grids\n",
    "if os.path.isfile(near_gridfile):\n",
    "    print('load near station information for grids')\n",
    "    with np.load(near_gridfile) as datatemp:\n",
    "        neargrid_loc = datatemp['neargrid_loc']\n",
    "        neargrid_dist = datatemp['neargrid_dist']\n",
    "else:\n",
    "    print('find near stations for grids')\n",
    "    stnlle_in = stnlle.copy()\n",
    "    stnlle_in[np.isnan(stndata[:, 0]), 0:2] = np.nan\n",
    "    neargrid_loc, neargrid_dist = findnearstn(stnlle_in[:, 0], stnlle_in[:, 1], lattarm, lontarm, nearnum, 0)\n",
    "    np.savez_compressed(near_gridfile,neargrid_loc=neargrid_loc,neargrid_dist=neargrid_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################################################################################################################\n",
    "\n",
    "# # load downscaled reanalysis at station points\n",
    "# print('load downscaled reanalysis data at station points')\n",
    "# readata_stn = np.nan * np.zeros([reanum, nstn, ntimes], dtype=np.float32)\n",
    "# for rr in range(reanum):\n",
    "#     dr = np.load(file_readownstn[rr])\n",
    "#     temp = dr[var + '_readown']\n",
    "#     # if prefix[rr] == 'MERRA2_':  # unify the time length of all data as MERRA2 lacks 1979\n",
    "#     #     add = np.nan * np.zeros([nstn, 365])\n",
    "#     #     temp = np.concatenate((add, temp), axis=1)\n",
    "#     readata_stn[rr, :, :] = temp\n",
    "#     del dr, temp\n",
    "# if var == 'prcp':\n",
    "#     readata_stn[readata_stn < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load independent merged/corrected data at station points\n"
     ]
    }
   ],
   "source": [
    "# get merged and corrected reanalysis data at all station points using two-layer cross-validation\n",
    "if os.path.isfile(file_corrmerge_stn):\n",
    "    print('load independent merged/corrected data at station points')\n",
    "    datatemp = np.load(file_corrmerge_stn)\n",
    "    reamerge_stn = datatemp['reamerge_stn']\n",
    "    reamerge_weight_stn = datatemp['reamerge_weight_stn']\n",
    "    reacorr_stn = datatemp['reacorr_stn']\n",
    "    del datatemp\n",
    "else:\n",
    "    reamerge_stn = np.nan * np.zeros([nstn, ntimes], dtype=np.float32)\n",
    "    reamerge_weight_stn = np.nan * np.zeros([12, nstn, reanum], dtype=np.float32)\n",
    "    reacorr_stn = np.nan * np.zeros([reanum, nstn, ntimes], dtype=np.float32)\n",
    "    # for each month\n",
    "    for m in range(12):\n",
    "        print('month', m + 1)\n",
    "        indm = date_number['mm'] == (m + 1)\n",
    "        reamerge_stnm, reamerge_weight_stnm, reacorr_stnm = \\\n",
    "            correction_merge_stn(stndata[:, indm], ecdf_prob, readata_stn[:, :, indm], nearstn_loc, nearstn_dist,\n",
    "                                 var, corrmode, weightmode)\n",
    "        reamerge_stn[:, indm] = reamerge_stnm\n",
    "        reacorr_stn[:, :, indm] = reacorr_stnm\n",
    "        reamerge_weight_stn[m, :, :] = reamerge_weight_stnm\n",
    "\n",
    "    # the variables are independent with their concurrent stations. thus, station data can be used to evaluate them\n",
    "    np.savez_compressed(file_corrmerge_stn, reamerge_stn=reamerge_stn, reamerge_weight_stn=reamerge_weight_stn,\n",
    "                        reacorr_stn=reacorr_stn, date_list=date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "determine the best data choice for each grid cell\n",
      "month 1\n",
      "month 2\n",
      "month 3\n",
      "month 4\n",
      "month 5\n",
      "month 6\n",
      "month 7\n",
      "month 8\n",
      "month 9\n",
      "month 10\n",
      "month 11\n",
      "month 12\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(file_mergechoice):\n",
    "    print('load merge choice file')\n",
    "    datatemp = np.load(file_mergechoice)\n",
    "    merge_choice = datatemp['merge_choice']\n",
    "    del datatemp\n",
    "else:\n",
    "    print('determine the best data choice for each grid cell')\n",
    "    merge_choice = -1 * np.ones([12,nrows,ncols], dtype=int)\n",
    "    for m in range(12):\n",
    "        print('month', m+1)\n",
    "        indm = date_number['mm'] == (m + 1)\n",
    "        # evaluate merge and corrected reanalysis\n",
    "        # this evaluation is feasible as merge or corrected data are all obtained using independent stations\n",
    "        met_merge_stn = calmetric(reamerge_stn[:, indm], stndata[:, indm], metname='RMSE')\n",
    "        met_corr_stn = np.nan * np.zeros([nstn, reanum])\n",
    "        for rr in range(reanum):\n",
    "            met_corr_stn[:, rr] = calmetric(reacorr_stn[rr, :, indm].T, stndata[:, indm], metname='RMSE')\n",
    "\n",
    "        metric_all = np.zeros([nrows, ncols, reanum + 1])\n",
    "        met_merge_grid = extrapolation(met_merge_stn, neargrid_loc, neargrid_dist)\n",
    "        met_corr_grid = extrapolation(met_corr_stn, neargrid_loc, neargrid_dist)\n",
    "        metric_all[:, :, 0] = met_merge_grid\n",
    "        metric_all[:, :, 1:] = met_corr_grid\n",
    "        metric_all[np.isnan(metric_all)] = np.inf\n",
    "        merge_choicem = np.argmin(metric_all, axis=2)  # 0: merge, 1 to N: corresponding corrected reanalysis\n",
    "        merge_choicem[mask != 1] = -1\n",
    "        merge_choice[m, :, :] = merge_choicem\n",
    "        del metric_all, met_merge_grid, met_corr_grid\n",
    "    np.savez_compressed(file_mergechoice, merge_choice=merge_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month 2\n",
      "estimate ecdf of stations\n",
      "estimate ecdf of reanalysis\n",
      "reanalysis 0 / 3\n",
      "load reanalysis data for this month\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "calculate ecdf\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "reanalysis 1 / 3\n",
      "reanalysis 2 / 3\n"
     ]
    }
   ],
   "source": [
    "# strategy-2: read monthly files\n",
    "# if QM is used, we have to derive the CDF curve for all grids before correction\n",
    "for m in range(1,2):\n",
    "    print('month', m+1)\n",
    "    indm = date_number['mm'] == (m + 1)\n",
    "\n",
    "    # calculate the ecdf of station data\n",
    "    print('estimate ecdf of stations')\n",
    "    file_ecdf = path_ecdf + '/ecdf_stn_' + var + '_month_' + str(m+1) + '.npz'\n",
    "    if not os.path.isfile(file_ecdf):\n",
    "        ecdf_stn = np.nan * np.zeros([nstn, binprob + 1], dtype=np.float32)\n",
    "        for i in range(nstn):\n",
    "            if not np.isnan(stndata[i, 0]):\n",
    "                ecdf_stn[i, :] = empirical_cdf(stndata[i, indm], ecdf_prob)\n",
    "        np.savez_compressed(file_ecdf, ecdf=ecdf_stn, prob=ecdf_prob, stnlle=stnlle)\n",
    "        del ecdf_stn\n",
    "\n",
    "    print('estimate ecdf of reanalysis')\n",
    "    for rr in range(3):\n",
    "        print('reanalysis',rr,'/',reanum)\n",
    "        file_ecdf = path_ecdf + '/ecdf_' + prefix[rr] + var + '_month_' + str(m+1) + '.npz'\n",
    "        if os.path.isfile(file_ecdf):\n",
    "            continue\n",
    "\n",
    "        # read raw gridded reanalysis data\n",
    "        print('load reanalysis data for this month')\n",
    "        datam_rea = np.nan * np.zeros([nrows, ncols, np.sum(indm)], dtype=np.float32)\n",
    "        flag=0\n",
    "        for y in range(2010, 2019):\n",
    "            print(y)\n",
    "            mmy = date_number['mm'].copy()\n",
    "            mmy = mmy[date_number['yyyy'] == y]\n",
    "            indmmy = mmy == (m + 1)\n",
    "            mmdays = np.sum(indmmy)\n",
    "            if not (prefix[rr] == 'MERRA2_' and y == 1979):\n",
    "                filer = path_readowngrid[rr] + '/' + prefix[rr] + 'ds_' + var + '_' + str(y*100+m+1) + '.npz'\n",
    "                d = np.load(filer)\n",
    "                datam_rea[:, :, flag:flag + mmdays] = d['data']\n",
    "                del d\n",
    "            flag = flag + mmdays\n",
    "\n",
    "        # calculate ecdf\n",
    "        print('calculate ecdf')\n",
    "        ecdf_rea = np.nan * np.zeros([nrows, ncols, binprob + 1], dtype=np.float32)\n",
    "        for i in range(nrows):\n",
    "            if np.mod(i,100)==0:\n",
    "                print(i)\n",
    "            for j in range(ncols):\n",
    "                if not np.isnan(mask[i,j]):\n",
    "                    ecdf_rea[i, j, :] = empirical_cdf(datam_rea[i, j, :], ecdf_prob)\n",
    "        np.savez_compressed(file_ecdf, ecdf=ecdf_rea, prob=ecdf_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correction and Merge: month 2\n",
      "Correction and Merge: year 2018\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (28,27275) (27275,28) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-c1695efff0cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# (1) estimate the error of corrected data by interpolating stations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreanum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 corr_error[rr, :, :, :] = extrapolation(reacorr_stn[rr, :, indmmy2] - stndata[:, indmmy2],\n\u001b[0m\u001b[1;32m     58\u001b[0m                                                         neargrid_loc, neargrid_dist)\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (28,27275) (27275,28) "
     ]
    }
   ],
   "source": [
    "# process for each month\n",
    "for m in range(1,2):\n",
    "    print('Correction and Merge: month', m + 1)\n",
    "\n",
    "    # load the ecdf of stations and reanalysis for this month\n",
    "    file_ecdf = path_ecdf + '/ecdf_stn_' + var + '_month_' + str(m + 1) + '.npz'\n",
    "    datatemp = np.load(file_ecdf)\n",
    "    ecdf_stn = datatemp['ecdf']\n",
    "    del datatemp\n",
    "\n",
    "    ecdf_rea = np.nan * np.zeros([reanum, nrows, ncols, binprob+1])\n",
    "    for rr in range(reanum):\n",
    "        file_ecdf = path_ecdf + '/ecdf_' + prefix[rr] + var + '_month_' + str(m + 1) + '.npz'\n",
    "        datatemp = np.load(file_ecdf)\n",
    "        ecdf_rea[rr, :, :, :] = datatemp['ecdf']\n",
    "        del datatemp\n",
    "\n",
    "\n",
    "    for y in range(2018, 2019):\n",
    "        print('Correction and Merge: year',y)\n",
    "        filebma_merge = path_merge + '/bmamerge_' + var + '_' + str(y * 100 + m + 1) + weightmode + '.npz'\n",
    "        filecorr = path_reacorr + '/reacorrdata_' + var + '_' + str(y * 100 + m + 1) + '.npz'\n",
    "        if os.path.isfile(filebma_merge) and os.path.isfile(filecorr):\n",
    "            print('file exists ... continue')\n",
    "            continue\n",
    "\n",
    "        # date processing\n",
    "        mmy = date_number['mm'].copy()\n",
    "        mmy = mmy[date_number['yyyy'] == y]\n",
    "        indmmy = mmy == (m + 1)\n",
    "        indmmy2 = (date_number['yyyy'] == y) & (date_number['mm'] == m + 1)\n",
    "        mmdays = np.sum(indmmy)\n",
    "\n",
    "        # read raw gridded reanalysis data\n",
    "        readata_raw = np.nan * np.zeros([reanum, nrows, ncols, mmdays], dtype=np.float32)\n",
    "        for rr in range(reanum):\n",
    "            if not (prefix[rr] == 'MERRA2_' and y == 1979):\n",
    "                filer = path_readowngrid[rr] + '/' + prefix[rr] + 'ds_' + var + '_' + str(y*100 +m+1) + '.npz'\n",
    "                d = np.load(filer)\n",
    "                readata_raw[rr, :, :, :] = d['data']\n",
    "                del d\n",
    "\n",
    "        ################################################################################################################\n",
    "        # start QM-based error correction\n",
    "        if os.path.isfile(filecorr):\n",
    "            datatemp = np.load(filecorr)\n",
    "            corr_data = datatemp['corr_data']\n",
    "            corr_error = datatemp['corr_error']\n",
    "            del datatemp\n",
    "        else:\n",
    "            # initialization\n",
    "            corr_data = np.nan * np.zeros([reanum, nrows, ncols, mmdays], dtype=np.float32)\n",
    "            corr_error = np.nan * np.zeros([reanum, nrows, ncols, mmdays], dtype=np.float32)\n",
    "\n",
    "            # (1) estimate the error of corrected data by interpolating stations\n",
    "            for rr in range(reanum):\n",
    "                corr_error[rr, :, :, :] = extrapolation(reacorr_stn[rr, :, indmmy2].T - stndata[:, indmmy2],\n",
    "                                                        neargrid_loc, neargrid_dist)\n",
    "\n",
    "            # (2) estimate the value of corrected data\n",
    "            # error correction\n",
    "            for rr in range(reanum):\n",
    "                if (prefix[rr] == 'MERRA2_' and y == 1979):\n",
    "                    continue\n",
    "                for r in range(nrows):\n",
    "                    if np.mod(r,10)==0:\n",
    "                        print(rr,r,c)\n",
    "                    for c in range(ncols):\n",
    "                        if np.isnan(mask[r, c]):\n",
    "                            continue\n",
    "                        nearloc_rc = neargrid_loc[r, c, :]\n",
    "                        neardist_rc = neargrid_dist[r, c, :]\n",
    "                        maxdist = np.max([np.max(neardist_rc)+1, 100])\n",
    "                        nearweight_rc = au.distanceweight(neardist_rc, maxdist, 4)\n",
    "                        nearweight_rc = np.tile(nearweight_rc, [mmdays, 1]).T\n",
    "\n",
    "                        reacorr_rc = np.zeros([nearnum, mmdays])\n",
    "                        for i in range(nearnum):\n",
    "                            reacorr_rc[i, :] = cdf_correction(ecdf_prob, ecdf_stn[nearloc_rc[i], :],\n",
    "                                                              ecdf_prob, ecdf_rea[rr, r, c, :], readata_raw[rr, r, c, :])\n",
    "                        nearweight_rc[np.isnan(reacorr_rc)] = np.nan\n",
    "                        corr_data[rr, r, c, indmmy] = \\\n",
    "                            np.nansum(reacorr_rc * nearweight_rc, axis=0) / np.nansum(nearweight_rc, axis=0)\n",
    "\n",
    "            np.savez_compressed(filecorr, corr_data=corr_data, corr_error=corr_error,\n",
    "                                reaname=prefix, latitude=lattar, longitude=lontar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reamerge_weight_grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-d2293ced9035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# (2) estimate the value of merged data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mreamerge_weight_gridm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreamerge_weight_grid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmmdays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mdatai\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorr_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reamerge_weight_grid' is not defined"
     ]
    }
   ],
   "source": [
    "        # start BMA-based merging\n",
    "        if not os.path.isfile(filebma_merge):\n",
    "            # initialization\n",
    "            bma_data = np.nan * np.zeros([nrows, ncols, mmdays], dtype=np.float32)\n",
    "            # bma_error = np.nan * np.zeros([nrows, ncols, mmdays], dtype=np.float32)\n",
    "\n",
    "            # (1) estimate the error of corrected data by interpolating stations\n",
    "            bma_error = extrapolation(reamerge_stn[:, indmmy2] - stndata[:, indmmy2], neargrid_loc, neargrid_dist)\n",
    "\n",
    "            # (2) estimate the value of merged data\n",
    "            reamerge_weight_gridm = reamerge_weight_grid[m, :, :, :].copy()\n",
    "            for i in range(mmdays):\n",
    "                datai = corr_data[:, :, :, i]\n",
    "                weighti = reamerge_weight_gridm.copy()\n",
    "                weighti[np.isnan(datai)] = np.nan\n",
    "                bma_data[:, :, i] = np.nansum(weighti * datai, axis=0) / np.nansum(weighti, axis=0)\n",
    "            np.savez_compressed(filebma_merge, bma_data=bma_data, bma_error=bma_error,\n",
    "                                reaname=prefix, latitude=lattar, longitude=lontar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/localuser/Research/EMDNA/merge/bmamerge_prcp_201802BMA.npz'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filebma_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
