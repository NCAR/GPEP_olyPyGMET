{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var is  prcp\n",
      "weightmode is  BMA\n",
      "years are  2000 2000\n",
      "start basic processing\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import auxiliary as au\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import io\n",
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "from bma_merge import bma\n",
    "from auxiliary_merge import *\n",
    "\n",
    "\n",
    "def empirical_cdf(data, probtar):\n",
    "    # data: vector of data\n",
    "    data2 = data[~np.isnan(data)]\n",
    "    if len(data2) > 0:\n",
    "        ds = np.sort(data2)\n",
    "        probreal = np.arange(len(data2)) / (len(data2) + 1)\n",
    "        ecdf_out = np.interp(probtar, probreal, ds)\n",
    "    else:\n",
    "        ecdf_out = np.nan * np.zeros(len(probtar))\n",
    "    return ecdf_out\n",
    "\n",
    "\n",
    "def cdf_correction(cdf_ref, value_ref, cdf_raw, value_raw, value_tar):\n",
    "    prob_tar = np.interp(value_tar, value_raw, cdf_raw)\n",
    "    value_out = np.interp(prob_tar, cdf_ref, value_ref)\n",
    "    return value_out\n",
    "\n",
    "\n",
    "def calculate_anomaly(datatar, dataref, hwsize, amode, upbound=5, lowbound=0.2):\n",
    "    # datatar, dataref: 2D [nstn, ntime]\n",
    "    # amode: anomaly mode ('ratio' or 'diff')\n",
    "    # hwsize: define time window (2*hwsize+1) used to calculate ratio (as ratio for a specific day is too variable)\n",
    "    # upbound/lowbound: upper and lower limitation of ratio/difference\n",
    "    if np.ndim(datatar) == 1:  # only one time step\n",
    "        datatar = datatar[:, np.newaxis]\n",
    "        dataref = dataref[:, np.newaxis]\n",
    "\n",
    "    nstn, ntime = np.shape(datatar)\n",
    "    if ntime < hwsize * 2 + 1:\n",
    "        print('The window size is larger than time steps when calculating ratio between tar and ref datasets')\n",
    "        print('Please set a smaller hwsize')\n",
    "        sys.exit()\n",
    "\n",
    "    anom = np.ones([nstn, ntime])\n",
    "\n",
    "    for i in range(ntime):\n",
    "        if i < hwsize:\n",
    "            windex = np.arange(hwsize * 2 + 1)\n",
    "        elif i >= ntime - hwsize:\n",
    "            windex = np.arange(ntime - hwsize * 2 - 1, ntime)\n",
    "        else:\n",
    "            windex = np.arange(i - hwsize, i + hwsize + 1)\n",
    "        dtari = np.nanmean(datatar[:, windex], axis=1)\n",
    "        drefi = np.nanmean(dataref[:, windex], axis=1)\n",
    "\n",
    "        if amode == 'ratio':\n",
    "            temp = drefi / dtari\n",
    "            temp[(dtari == 0) & (drefi == 0)] = 1\n",
    "            anom[:, i] = temp\n",
    "        elif amode == 'diff':\n",
    "            anom[:, i] = drefi - dtari\n",
    "        else:\n",
    "            sys.exit('Unknow amode. Please use either ratio or diff')\n",
    "\n",
    "    anom[anom > upbound] = upbound\n",
    "    anom[anom < lowbound] = lowbound\n",
    "    return anom\n",
    "\n",
    "\n",
    "def findnearstn(stnlat, stnlon, tarlat, tarlon, nearnum, noself):\n",
    "    # only use lat/lon to find near stations without considering distance in km\n",
    "    # stnlat/stnlon: 1D\n",
    "    # tarlat/tarlon: 1D or 2D\n",
    "    # noself: 1--stnlat and tarlat have overlapped stations, which should be excluded from stnlat\n",
    "\n",
    "    stnll = np.zeros([len(stnlat), 2])\n",
    "    stnll[:, 0] = stnlat\n",
    "    stnll[:, 1] = stnlon\n",
    "\n",
    "    if len(np.shape(tarlat)) == 1:\n",
    "        num = len(tarlat)\n",
    "        nearstn_loc = -1 * np.ones([num, nearnum], dtype=int)\n",
    "        nearstn_dist = -1 * np.ones([num, nearnum], dtype=float)\n",
    "        for i in range(num):\n",
    "            if np.isnan(tarlat[i]) or np.isnan(tarlon[i]):\n",
    "                continue\n",
    "            tari = np.array([tarlat[i], tarlon[i]])\n",
    "            dist = au.distance(tari, stnll)\n",
    "            dist[np.isnan(dist)] = 1000000000\n",
    "            if noself == 1:\n",
    "                dist[dist == 0] = np.inf  # may not be perfect, but work for SCDNA\n",
    "            indi = np.argsort(dist)\n",
    "            nearstn_loc[i, :] = indi[0:nearnum]\n",
    "            nearstn_dist[i, :] = dist[nearstn_loc[i, :]]\n",
    "    elif len(np.shape(tarlat)) == 2:\n",
    "        nrows, ncols = np.shape(tarlat)\n",
    "        nearstn_loc = -1 * np.ones([nrows, ncols, nearnum], dtype=int)\n",
    "        nearstn_dist = -1 * np.ones([nrows, ncols, nearnum], dtype=float)\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if np.isnan(tarlat[r, c]) or np.isnan(tarlon[r, c]):\n",
    "                    continue\n",
    "                tari = np.array([tarlat[r, c], tarlon[r, c]])\n",
    "                dist = au.distance(tari, stnll)\n",
    "                dist[np.isnan(dist)] = 1000000000\n",
    "                indi = np.argsort(dist)\n",
    "                nearstn_loc[r, c, :] = indi[0:nearnum]\n",
    "                nearstn_dist[r, c, :] = dist[nearstn_loc[r, c, :]]\n",
    "    else:\n",
    "        print('The dimensions of tarlat or tarlon are larger than 2')\n",
    "        sys.exit()\n",
    "\n",
    "    return nearstn_loc, nearstn_dist\n",
    "\n",
    "\n",
    "def error_correction_new(corrmode, stndata_i2_near, nearstn_weighti2, readata_stn_i2, readata_i2_near, ecdf_prob):\n",
    "    # corrmode: QM, Mul_Climo, Mul_Daily, Add_Climo, Add_Climo\n",
    "    nearstn_numi2, ntimes = np.shape(stndata_i2_near)\n",
    "    corrdata_out = np.zeros(ntimes)\n",
    "    if corrmode == 'QM':\n",
    "        cdf_rea = empirical_cdf(readata_stn_i2, ecdf_prob)\n",
    "        for j in range(nearstn_numi2):\n",
    "            cdf_ref = empirical_cdf(stndata_i2_near[j, :], ecdf_prob)\n",
    "            qmdata_rj = cdf_correction(ecdf_prob, cdf_ref, ecdf_prob, cdf_rea, readata_stn_i2)\n",
    "            corrdata_out = corrdata_out + qmdata_rj * nearstn_weighti2[j]\n",
    "        corrdata_out = corrdata_out / np.sum(nearstn_weighti2)\n",
    "    elif corrmode[0:3] == 'Mul' or corrmode[0:3] == 'Add':\n",
    "        # multplicative correction or additive correction\n",
    "        if corrmode[4:] == 'Daily':\n",
    "            dtar = readata_i2_near\n",
    "            dref = stndata_i2_near\n",
    "        elif corrmode[4:] == 'Climo':\n",
    "            dtar = np.nanmean(readata_i2_near, axis=1)\n",
    "            dtar = dtar[:, np.newaxis]\n",
    "            dref = np.nanmean(stndata_i2_near, axis=1)\n",
    "            dref = dref[:, np.newaxis]\n",
    "        else:\n",
    "            sys.exit('Unknown corrmode')\n",
    "        if corrmode[0:3] == 'Mul':\n",
    "            corrfactor_i2_near = calculate_anomaly(dtar, dref, 0, 'ratio', 10, 0)  # 10 is default max limit\n",
    "        else:\n",
    "            corrfactor_i2_near = calculate_anomaly(dtar, dref, 0, 'diff', 9999, -9999)\n",
    "        weight_use = np.tile(nearstn_weighti2, (np.shape(corrfactor_i2_near)[1], 1)).T\n",
    "        weight_use[np.isnan(corrfactor_i2_near)] = np.nan\n",
    "        corrfactor_i2 = np.nansum(corrfactor_i2_near * weight_use, axis=0) / np.nansum(weight_use)\n",
    "        if corrmode[0:3] == 'Mul':\n",
    "            corrdata_out = readata_stn_i2 * corrfactor_i2\n",
    "        else:\n",
    "            corrdata_out = readata_stn_i2 + corrfactor_i2\n",
    "    else:\n",
    "        sys.exit('Unknown corrmode')\n",
    "\n",
    "    return corrdata_out\n",
    "\n",
    "\n",
    "def error_correction(dataori, anomaly, mode='ratio'):\n",
    "    # default: time is the last dimension\n",
    "    if mode == 'ratio':\n",
    "        datacorr = dataori * anomaly\n",
    "    elif mode == 'diff':\n",
    "        datacorr = dataori + anomaly\n",
    "    else:\n",
    "        sys.exit('Wrong error correction mode')\n",
    "    return datacorr\n",
    "\n",
    "\n",
    "def calweight(obs, rea, mode, preprocess=True):\n",
    "    ntimes, reanum = np.shape(rea)\n",
    "    if preprocess:\n",
    "        # delete the nan values\n",
    "        ind_nan = np.isnan(obs + np.sum(rea, axis=1))\n",
    "        obs = obs[~ind_nan]\n",
    "        rea = rea[~ind_nan, :]\n",
    "\n",
    "    if len(obs) > 2:\n",
    "        if mode == 'BMA':\n",
    "            weight, sigma, sigma_s = bma(rea, obs)\n",
    "        else:\n",
    "            met = np.zeros(reanum)\n",
    "            if mode == 'RMSE':\n",
    "                for i in range(reanum):\n",
    "                    met[i] = np.sqrt(np.sum(np.square(obs - rea[:, i])) / len(obs))  # RMSE\n",
    "                weight = 1 / (met ** 2)\n",
    "            elif mode == 'CC':\n",
    "                for i in range(reanum):\n",
    "                    met[i] = np.corrcoef(obs, rea[:, i])[0][1]\n",
    "                weight = (met ** 2)\n",
    "    else:\n",
    "        weight = np.ones(reanum) / reanum\n",
    "    if np.any(np.isnan(weight)):\n",
    "        weight = np.ones(reanum) / reanum\n",
    "    return weight\n",
    "\n",
    "\n",
    "def weightmerge(data, weight):\n",
    "    if np.ndim(data) == 2:\n",
    "        weight2 = weight.copy()\n",
    "        weight2[np.isnan(data)] = np.nan\n",
    "        dataout = np.nansum(data * weight2, axis=1) / np.nansum(weight2, axis=1)\n",
    "    elif np.ndim(data) == 3:\n",
    "        weight2 = weight.copy()\n",
    "        weight2[np.isnan(data)] = np.nan\n",
    "        dataout = np.nansum(data * weight2, axis=2) / np.nansum(weight2, axis=2)\n",
    "        dataout[np.isnan(data[:, :, 0])] = np.nan\n",
    "    return dataout\n",
    "\n",
    "\n",
    "def correction_merge_stn(stndata, ecdf_prob, readata_stn, nearstn_loc, nearstn_dist, var, corrmode, weightmode):\n",
    "    # corrmode = 'QM'  # QM, Mul_Climo, Mul_Daily, Add_Climo, Add_Climo\n",
    "    # use 2-layer cross-validation to estimate the weight and independent data of merge/correction data\n",
    "    reanum, nstn, ntimes = np.shape(readata_stn)\n",
    "\n",
    "    # initialization\n",
    "    reacorr_stn = np.nan * np.zeros([reanum, nstn, ntimes], dtype=np.float32)  # corrected reanalysis data\n",
    "    reamerge_weight_stn = np.nan * np.zeros([nstn, reanum], dtype=np.float32)  # weight used to obtain reamerge_stn\n",
    "    reamerge_stn = np.nan * np.zeros([nstn, ntimes], dtype=np.float32)  # merged reanalysis at station points\n",
    "\n",
    "    for i1 in range(nstn):  # layer-1\n",
    "        if np.mod(i1, 1000) == 0:\n",
    "            print(i1)\n",
    "        \n",
    "        if not np.mod(i1, 5) == 0:\n",
    "            continue\n",
    "        \n",
    "        if np.isnan(stndata[i1, 0]):\n",
    "            continue\n",
    "        nearstn_loci1 = nearstn_loc[i1, :]\n",
    "        nearstn_disti1 = nearstn_dist[i1, :]\n",
    "        induse = nearstn_loci1 > -1\n",
    "        nearstn_loci1 = nearstn_loci1[induse]\n",
    "        nearstn_disti1 = nearstn_disti1[induse]\n",
    "        nearstn_numi1 = len(nearstn_loci1)\n",
    "        if nearstn_numi1 == 0:\n",
    "            sys.exit('No near station for the target station (layer-1)')\n",
    "\n",
    "        # start layer-2\n",
    "        reamerge_weight_i2 = np.zeros([nearstn_numi1, reanum])\n",
    "        for i2 in range(nearstn_numi1):  # layer-2\n",
    "            nearstn_loci2 = nearstn_loc[nearstn_loci1[i2], :]\n",
    "            nearstn_disti2 = nearstn_dist[nearstn_loci1[i2], :]\n",
    "            induse = (nearstn_loci2 > -1) & (nearstn_loci2 != i1)  # i1 should be independent\n",
    "            nearstn_loci2 = nearstn_loci2[induse]\n",
    "            nearstn_disti2 = nearstn_disti2[induse]\n",
    "            maxd = np.max([np.max(nearstn_disti2) + 1, 100])\n",
    "            nearstn_weighti2 = au.distanceweight(nearstn_disti2, maxd, 3)\n",
    "            nearstn_weighti2 = nearstn_weighti2 / np.sum(nearstn_weighti2)\n",
    "\n",
    "            nearstn_numi2 = len(nearstn_loci2)\n",
    "            if nearstn_numi2 == 0:\n",
    "                sys.exit('No near station for the target station (layer-1)')\n",
    "\n",
    "            # data at i2 station\n",
    "            stndata_i2 = stndata[nearstn_loci1[i2], :]\n",
    "            stndata_i2_near = stndata[nearstn_loci2, :]\n",
    "            readata_stn_i2 = readata_stn[:, nearstn_loci1[i2], :]\n",
    "            readata_i2_near = readata_stn[:, nearstn_loci2, :]\n",
    "\n",
    "            # error correction for each reanalysis dataset using different modes\n",
    "            corrdata_i2 = np.zeros([ntimes, reanum])\n",
    "            for r in range(reanum):\n",
    "                corrdata_i2[:, r] = error_correction_new(corrmode, stndata_i2_near, nearstn_weighti2,\n",
    "                                                         readata_stn_i2[r, :], readata_i2_near[r, :, :], ecdf_prob)\n",
    "\n",
    "            # calculate merging weight for i2\n",
    "            if weightmode == 'BMA' and var == 'prcp':\n",
    "                # exclude zero precipitation and carry out box-cox transformation\n",
    "                datatemp = np.zeros([ntimes, reanum + 1])\n",
    "                datatemp[:, 0] = stndata_i2\n",
    "                datatemp[:, 1:] = corrdata_i2\n",
    "                ind0 = np.sum(datatemp >= 0.01, axis=1) == (reanum + 1)  # positive hit events\n",
    "                dobs = box_cox_transform(stndata_i2[ind0])\n",
    "                drea = box_cox_transform(corrdata_i2[ind0, :])\n",
    "            else:\n",
    "                dobs = stndata_i2\n",
    "                drea = corrdata_i2\n",
    "            reamerge_weight_i2[i2, :] = calweight(dobs, drea, weightmode)\n",
    "        # end layer-2\n",
    "\n",
    "        stndata_i1 = stndata[i1, :]\n",
    "        stndata_i1_near = stndata[nearstn_loci1, :]\n",
    "        readata_stn_i1 = readata_stn[:, i1, :]\n",
    "        readata_i1_near = readata_stn[:, nearstn_loci1, :]\n",
    "        maxd = np.max([np.max(nearstn_disti1) + 1, 100])\n",
    "        nearstn_weighti1 = au.distanceweight(nearstn_disti1, maxd, 3)\n",
    "        nearstn_weighti1 = nearstn_weighti1 / np.sum(nearstn_weighti1)\n",
    "\n",
    "        # get corrected data at i1\n",
    "        corrdata_i1 = np.zeros([ntimes, reanum])\n",
    "        for r in range(reanum):\n",
    "            corrdata_i1[:, r] = error_correction_new(corrmode, stndata_i1_near, nearstn_weighti1,\n",
    "                                                     readata_stn_i1[r, :], readata_i1_near[r, :, :], ecdf_prob)\n",
    "        reacorr_stn[:, i1, :] = corrdata_i1.T\n",
    "\n",
    "        # get merging weight at i1 and merge reanalysis\n",
    "        # note: this weight is just for independent merging so we can estimate the error of merged reanalysis\n",
    "        # the real weight will be estimated using just one-layer cross-validation\n",
    "        weight_use = np.tile(nearstn_weighti1, (reanum, 1)).T\n",
    "        weight_i1 = np.sum(weight_use * reamerge_weight_i2, axis=0)\n",
    "        weight_i1 = weight_i1 / np.sum(weight_i1)\n",
    "\n",
    "        weight_use = np.tile(weight_i1, (ntimes, 1))\n",
    "        if weightmode == 'BMA' and var == 'prcp':\n",
    "            reamerge_stni1 = np.sum(weight_use * box_cox_transform(corrdata_i1), axis=1)\n",
    "            reamerge_stni1 = box_cox_recover(reamerge_stni1)\n",
    "        else:\n",
    "            reamerge_stni1 = np.sum(weight_use * corrdata_i1, axis=1)\n",
    "        reamerge_stn[i1, :] = reamerge_stni1\n",
    "\n",
    "        # get the final merging weight\n",
    "        if weightmode == 'BMA' and var == 'prcp':\n",
    "            # exclude zero precipitation and carry out box-cox transformation\n",
    "            datatemp = np.zeros([ntimes, reanum + 1])\n",
    "            datatemp[:, 0] = stndata_i1\n",
    "            datatemp[:, 1:] = corrdata_i1\n",
    "            ind0 = np.sum(datatemp >= 0.01, axis=1) == (reanum + 1)  # positive hit events\n",
    "            dobs = box_cox_transform(stndata_i1[ind0])\n",
    "            drea = box_cox_transform(corrdata_i1[ind0, :])\n",
    "        else:\n",
    "            dobs = stndata_i1\n",
    "            drea = corrdata_i1\n",
    "        reamerge_weight_stn[i1, :] = calweight(dobs, drea, weightmode)\n",
    "\n",
    "    # note: reamerge_weight_stn is the final merging weight, and reacorr_stn is the final corrected data\n",
    "    # but reamerge_stn is just independent merging estimates which is calculated from 2-layer cross validation\n",
    "    return reamerge_stn, reamerge_weight_stn, reacorr_stn\n",
    "\n",
    "\n",
    "def correction_rea(stndata, ecdf_prob, readata_stn, nearstn_loc, nearstn_dist, corrmode):\n",
    "    # compare the performance of daily-scale multiplicative correction and QM\n",
    "    reanum, nstn, ntimes = np.shape(readata_stn)\n",
    "    nprob = len(ecdf_prob)\n",
    "\n",
    "    # initialization\n",
    "    reacorr = np.nan * np.zeros([reanum, nstn, ntimes], dtype=np.float32)  # corrected reanalysis data\n",
    "\n",
    "    for i1 in range(nstn):  # layer-1\n",
    "        # if np.mod(i1, 1000) == 0:\n",
    "        #     print(i1)\n",
    "        if np.isnan(stndata[i1, 0]):\n",
    "            continue\n",
    "\n",
    "        nearstn_loci1 = nearstn_loc[i1, :]\n",
    "        nearstn_disti1 = nearstn_dist[i1, :]\n",
    "        induse = nearstn_loci1 > -1\n",
    "        nearstn_loci1 = nearstn_loci1[induse]\n",
    "        nearstn_disti1 = nearstn_disti1[induse]\n",
    "        nearstn_numi1 = len(nearstn_loci1)\n",
    "        if nearstn_numi1 == 0:\n",
    "            sys.exit('No near station for the target station (layer-1)')\n",
    "\n",
    "        stndata_i1_near = stndata[nearstn_loci1, :]\n",
    "        readata_stn_i1 = readata_stn[:, i1, :]\n",
    "        readata_i1_near = readata_stn[:, nearstn_loci1, :]\n",
    "        maxd = np.max([np.max(nearstn_disti1) + 1, 100])\n",
    "        nearstn_weighti1 = au.distanceweight(nearstn_disti1, maxd, 3)\n",
    "        nearstn_weighti1 = nearstn_weighti1 / np.sum(nearstn_weighti1)\n",
    "\n",
    "        # get corrected data at i1\n",
    "        corrdata_i1 = np.zeros([ntimes, reanum])\n",
    "        for r in range(reanum):\n",
    "            corrdata_i1[:, r] = error_correction_new(corrmode, stndata_i1_near, nearstn_weighti1,\n",
    "                                                     readata_stn_i1[r, :], readata_i1_near[r, :, :], ecdf_prob)\n",
    "        reacorr[:, i1, :] = corrdata_i1.T\n",
    "\n",
    "    return reacorr\n",
    "\n",
    "\n",
    "def correction_merge_grid(stndata, readata_raw, readata_stn, reacorr_stn, reamerge_stn, reamerge_weight_stn,\n",
    "                          neargrid_loc,\n",
    "                          neargrid_dist, merge_choice, mask, hwsize, corrmode, anombound, var, weightmode):\n",
    "    nrows, ncols, nearnum = np.shape(neargrid_loc)\n",
    "    reanum, nstn, nday = np.shape(readata_stn)\n",
    "\n",
    "    neargrid_loc = neargrid_loc.copy()\n",
    "    neargrid_dist = neargrid_dist.copy()\n",
    "    mask2 = np.tile(mask[:, :, np.newaxis], (1, 1, nearnum))\n",
    "    neargrid_loc[mask2 != 1] = -1\n",
    "    neargrid_dist[mask2 != 1] = np.nan\n",
    "    del mask2\n",
    "\n",
    "    # correct raw gridded reanalysis data using all stations\n",
    "    corr_data = np.nan * np.zeros([reanum, nrows, ncols, nday], dtype=np.float32)\n",
    "    for rr in range(reanum):\n",
    "        # calculate correction ratio at all station point\n",
    "        anom_ori = calculate_anomaly(readata_stn[rr, :, :], stndata[:, :],\n",
    "                                     hwsize, corrmode, upbound=anombound[1], lowbound=anombound[0])\n",
    "        anom_ext = extrapolation(anom_ori, neargrid_loc, neargrid_dist)\n",
    "        corr_data[rr, :, :, :] = error_correction(readata_raw[rr, :, :, :], anom_ext, mode=corrmode)\n",
    "\n",
    "    # first error estimation\n",
    "    corr_error = np.nan * np.zeros([reanum, nrows, ncols, nday], dtype=np.float32)\n",
    "    for rr in range(reanum):\n",
    "        corr_error[rr, :, :, :] = extrapolation(reacorr_stn[rr, :, :] - stndata, neargrid_loc, neargrid_dist)\n",
    "    merge_error0 = extrapolation(reamerge_stn - stndata, neargrid_loc, neargrid_dist)\n",
    "\n",
    "    if var == 'prcp' and weightmode == 'BMA':\n",
    "        corr_data = box_cox_transform(corr_data)\n",
    "        stndata = box_cox_transform(stndata)\n",
    "        reamerge_stn = box_cox_transform(reamerge_stn)\n",
    "        reacorr_stn = box_cox_transform(reacorr_stn)\n",
    "\n",
    "        # correction error in normal space (box-cox)\n",
    "        corr_error_bc = np.nan * np.zeros([reanum, nrows, ncols, nday], dtype=np.float32)\n",
    "        for rr in range(reanum):\n",
    "            corr_error_bc[rr, :, :, :] = extrapolation(reacorr_stn[rr, :, :] - stndata, neargrid_loc, neargrid_dist)\n",
    "        merge_error0_bc = extrapolation(reamerge_stn - stndata, neargrid_loc, neargrid_dist)\n",
    "\n",
    "    # merge reanalysis data\n",
    "    merge_data = np.nan * np.zeros([nrows, ncols, nday], dtype=np.float32)\n",
    "    weight_grid = extrapolation(reamerge_weight_stn, neargrid_loc, neargrid_dist)\n",
    "    for i in range(nday):\n",
    "        datain = np.zeros([nrows, ncols, reanum])\n",
    "        for rr in range(reanum):\n",
    "            datain[:, :, rr] = corr_data[rr, :, :, i]\n",
    "        merge_data[:, :, i] = weightmerge(datain, weight_grid)\n",
    "\n",
    "    # calculate the error of merged data (this is actually independent with merged data estimation)\n",
    "    merge_error = np.nan * np.zeros([nrows, ncols, nday], dtype=np.float32)\n",
    "    for r in range(nrows):\n",
    "        for c in range(ncols):\n",
    "            if not np.isnan(mask[r, c]):\n",
    "                chi = merge_choice[r, c]\n",
    "                if chi > 0:\n",
    "                    merge_error[r, c, :] = corr_error[chi - 1, r, c, :]\n",
    "                else:\n",
    "                    merge_error[r, c, :] = merge_error0[r, c, :]\n",
    "\n",
    "    if var == 'prcp' and weightmode == 'BMA':\n",
    "        merge_error_bc = np.nan * np.zeros([nrows, ncols, nday], dtype=np.float32)\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if not np.isnan(mask[r, c]):\n",
    "                    chi = merge_choice[r, c]\n",
    "                    if chi > 0:\n",
    "                        merge_error_bc[r, c, :] = corr_error_bc[chi - 1, r, c, :]\n",
    "                    else:\n",
    "                        merge_error_bc[r, c, :] = merge_error0_bc[r, c, :]\n",
    "\n",
    "    if var == 'prcp' and weightmode == 'BMA':\n",
    "        merge_error_out = [''] * 2\n",
    "        merge_error_out[0] = merge_error\n",
    "        merge_error_out[1] = merge_error_bc\n",
    "        merge_data = box_cox_recover(merge_data)\n",
    "        corr_data = box_cox_recover(corr_data)\n",
    "    else:\n",
    "        merge_error_out = ['']\n",
    "        merge_error_out[0] = merge_error\n",
    "    return corr_data, corr_error, merge_data, merge_error_out\n",
    "\n",
    "\n",
    "def mse_error(stndata, reacorr_stn, reamerge_stn, neargrid_loc, neargrid_dist, merge_choice, mask, var, pcptrans=False):\n",
    "    nrows, ncols, nearnum = np.shape(neargrid_loc)\n",
    "    reanum, nstn, nday = np.shape(reacorr_stn)\n",
    "\n",
    "    neargrid_loc = neargrid_loc.copy()\n",
    "    neargrid_dist = neargrid_dist.copy()\n",
    "    mask2 = np.tile(mask[:, :, np.newaxis], (1, 1, nearnum))\n",
    "    neargrid_loc[mask2 != 1] = -1\n",
    "    neargrid_dist[mask2 != 1] = np.nan\n",
    "    del mask2\n",
    "\n",
    "    if var == 'prcp' and pcptrans == True:\n",
    "        stndata = box_cox_transform(stndata)\n",
    "        reamerge_stn = box_cox_transform(reamerge_stn)\n",
    "        reacorr_stn = box_cox_transform(reacorr_stn)\n",
    "\n",
    "    corr_error = np.nan * np.zeros([reanum, nrows, ncols, nday], dtype=np.float32)\n",
    "    for rr in range(reanum):\n",
    "        corr_error[rr, :, :, :] = extrapolation((reacorr_stn[rr, :, :] - stndata) ** 2, neargrid_loc, neargrid_dist)\n",
    "    corr_error = corr_error ** 0.5\n",
    "    merge_error0 = extrapolation((reamerge_stn - stndata) ** 2, neargrid_loc, neargrid_dist)\n",
    "    merge_error0 = merge_error0 ** 0.5\n",
    "\n",
    "    mse_error = np.nan * np.zeros([nrows, ncols, nday], dtype=np.float32)\n",
    "    for r in range(nrows):\n",
    "        for c in range(ncols):\n",
    "            if not np.isnan(mask[r, c]):\n",
    "                chi = merge_choice[r, c]\n",
    "                if chi > 0:\n",
    "                    mse_error[r, c, :] = corr_error[chi - 1, r, c, :]\n",
    "                else:\n",
    "                    mse_error[r, c, :] = merge_error0[r, c, :]\n",
    "\n",
    "    return mse_error\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# time periods and methods\n",
    "# var = 'prcp'  # ['prcp', 'tmean', 'trange']: this should be input from sbtach script\n",
    "# weightmode for merging: (CC, RMSE, BMA): Weight = CC**2, or Weight = 1/RMSE**2, or Weight = BMA\n",
    "# corrmode: QM, Mul_Climo, Mul_Daily, Add_Climo, Add_Climo\n",
    "# year range for merging. note weight is calculated using all data not limited by year\n",
    "\n",
    "# read from inputs\n",
    "# var = sys.argv[1]\n",
    "# weightmode = sys.argv[2]\n",
    "# y1 = int(sys.argv[3])\n",
    "# y2 = int(sys.argv[4])\n",
    "# year = [y1, y2]\n",
    "\n",
    "# embeded\n",
    "var = 'prcp'\n",
    "weightmode = 'BMA'\n",
    "corrmode = 'QM'\n",
    "y1 = 2000\n",
    "y2 = 2000\n",
    "year = [y1, y2]\n",
    "\n",
    "print('var is ', var)\n",
    "print('weightmode is ', weightmode)\n",
    "print('years are ', y1, y2)\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# basic settings\n",
    "lontar = np.arange(-180 + 0.05, -50, 0.1)\n",
    "lattar = np.arange(85 - 0.05, 5, -0.1)\n",
    "nearnum = 8  # the number of nearby stations used to extrapolate points to grids (for correction and merging)\n",
    "anombound = [0.2, 10]  # upper and lower bound when calculating the anomaly for correction\n",
    "prefix = ['ERA5_', 'MERRA2_', 'JRA55_']\n",
    "\n",
    "### Local Mac settings\n",
    "# input files/paths\n",
    "gmet_stnfile = '/Users/localuser/Research/EMDNA/basicinfo/stnlist_whole.txt'\n",
    "gmet_stndatafile = '/Users/localuser//Research/EMDNA/stndata_whole.npz'\n",
    "file_mask = './DEM/NA_DEM_010deg_trim.mat'\n",
    "# path_readowngrid = ['/Users/localuser/Research/Test',  # downscaled gridded data\n",
    "#                     '/Users/localuser/Research/Test',\n",
    "#                     '/Users/localuser/Research/Test']\n",
    "file_readownstn = ['/Users/localuser/Research/EMDNA/downscale/ERA5_downto_stn_nearest.npz', # downscaled to stn points\n",
    "                   '/Users/localuser/Research/EMDNA/downscale/MERRA2_downto_stn_nearest.npz',\n",
    "                   '/Users/localuser/Research/EMDNA/downscale/JRA55_downto_stn_nearest.npz']\n",
    "# file_readownstn = ['/Users/localuser/Research/EMDNA/downscale/JRA55_downto_stn_nearest.npz']\n",
    "\n",
    "# output files/paths (can also be used as inputs once generated)\n",
    "near_path = '/Users/localuser/Research/EMDNA/correction'  # path to save near station for each grid/cell\n",
    "path_reacorr = '/Users/localuser/Research/EMDNA/correction' # path to save corrected reanalysis data at station points\n",
    "path_merge = '/Users/localuser/Research/EMDNA/merge'\n",
    "\n",
    "near_stnfile = near_path + '/near_stn_' + var + '.npz'\n",
    "near_gridfile = near_path + '/near_grid_' + var + '.npz'\n",
    "file_corrmerge_stn = path_merge + '/mergecorr_stn_' + var + '_' + weightmode + '.npz'  # file of indepedent corrected/merging data and merging weights\n",
    "file_mergechoice = path_merge + '/mergechoice_' + var + '_' + weightmode + '.npz'\n",
    "### Local Mac settings\n",
    "\n",
    "\n",
    "# ### Plato settings\n",
    "# # input files/paths\n",
    "# gmet_stnfile = '/home/gut428/GMET/eCAI_EMDNA/StnGridInfo/stnlist_whole.txt'\n",
    "# gmet_stndatafile = '/Users/localuser/Research/EMDNA/stndata_whole.npz'\n",
    "# file_mask = '/datastore/GLOBALWATER/CommonData/EMDNA/DEM/NA_DEM_010deg_trim.mat'\n",
    "# # path_readowngrid = ['/datastore/GLOBALWATER/CommonData/EMDNA/ERA5_day_ds',  # downscaled gridded data\n",
    "# #                    '/datastore/GLOBALWATER/CommonData/EMDNA/MERRA2_day_ds',\n",
    "# #                    '/datastore/GLOBALWATER/CommonData/EMDNA/JRA55_day_ds']\n",
    "# # file_readownstn = ['/datastore/GLOBALWATER/CommonData/EMDNA/ERA5_day_ds/ERA5_downto_stn.npz', # downscaled to stn points\n",
    "# #                    '/datastore/GLOBALWATER/CommonData/EMDNA/MERRA2_day_ds/MERRA2_downto_stn.npz',\n",
    "# #                    '/datastore/GLOBALWATER/CommonData/EMDNA/JRA55_day_ds/JRA55_downto_stn.npz']\n",
    "# file_readownstn = ['/datastore/GLOBALWATER/CommonData/EMDNA/MERRA2_day_ds/MERRA2_downto_stn.npz']\n",
    "#\n",
    "# # output files/paths (can also be used as inputs once generated)\n",
    "# near_path = '/home/gut428/ReanalysisCorrMerge'  # path to save near station for each grid/cell\n",
    "# path_reacorr = '/home/gut428/ReanalysisCorrMerge/Reanalysis_corr'  # path to save corrected reanalysis data at station points\n",
    "# path_merge = '/home/gut428/ReanalysisCorrMerge/Reanalysis_merge'\n",
    "#\n",
    "# near_stnfile = near_path + '/near_stn_' + var + '.npz'\n",
    "# near_gridfile = near_path + '/near_grid_' + var + '.npz'\n",
    "# file_corrmerge_stn = path_merge + '/mergecorr_stn_' + var + '_' + weightmode + '.npz'  # file of indepedent corrected/merging data and merging weights\n",
    "# file_mergechoice = path_merge + '/mergechoice_' + var + '_' + weightmode + '.npz'\n",
    "# ### Plato settings\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# basic processing\n",
    "print('start basic processing')\n",
    "\n",
    "# mask\n",
    "mask = io.loadmat(file_mask)\n",
    "mask = mask['DEM']\n",
    "mask[~np.isnan(mask)] = 1  # 1: valid pixels\n",
    "\n",
    "# meshed lat/lon of the target region\n",
    "reanum = len(file_readownstn)\n",
    "nrows, ncols = np.shape(mask)\n",
    "lontarm, lattarm = np.meshgrid(lontar, lattar)\n",
    "lontarm[np.isnan(mask)] = np.nan\n",
    "lattarm[np.isnan(mask)] = np.nan\n",
    "\n",
    "# date list\n",
    "date_list, date_number = m_DateList(1979, 2018, 'ByYear')\n",
    "\n",
    "# load observations for all stations\n",
    "datatemp = np.load(gmet_stndatafile)\n",
    "stndata = datatemp[var + '_stn']\n",
    "stnlle = datatemp['stn_lle']\n",
    "nstn, ntimes = np.shape(stndata)\n",
    "del datatemp\n",
    "\n",
    "# probability bins for QM\n",
    "binprob = 500\n",
    "ecdf_prob = np.arange(0, 1 + 1 / binprob, 1 / binprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load near station information for points\n"
     ]
    }
   ],
   "source": [
    "# find near stations\n",
    "# find near stations for all stations\n",
    "if os.path.isfile(near_stnfile):\n",
    "    print('load near station information for points')\n",
    "    with np.load(near_stnfile) as datatemp:\n",
    "        nearstn_loc = datatemp['nearstn_loc']\n",
    "        nearstn_dist = datatemp['nearstn_dist']\n",
    "    del datatemp\n",
    "else:\n",
    "    print('find near stations for station points')\n",
    "    stnllein = stnlle.copy()\n",
    "    stnllein[np.isnan(stndata[:, 0]), :] = np.nan\n",
    "    nearstn_loc, nearstn_dist = findnearstn \\\n",
    "        (stnllein[:, 0], stnllein[:, 1], stnllein[:, 0], stnllein[:, 1], nearnum, 1)\n",
    "    np.savez_compressed(near_stnfile, nearstn_loc=nearstn_loc, nearstn_dist=nearstn_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load downscaled reanalysis data at station points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in less\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "# load downscaled reanalysis at station points\n",
    "print('load downscaled reanalysis data at station points')\n",
    "readata_stn = np.nan * np.zeros([reanum, nstn, ntimes], dtype=np.float32)\n",
    "for rr in range(reanum):\n",
    "    dr = np.load(file_readownstn[rr])\n",
    "    temp = dr[var + '_readown']\n",
    "    # if prefix[rr] == 'MERRA2_':  # unify the time length of all data as MERRA2 lacks 1979\n",
    "    #     add = np.nan * np.zeros([nstn, 365])\n",
    "    #     temp = np.concatenate((add, temp), axis=1)\n",
    "    readata_stn[rr, :, :] = temp\n",
    "    del dr, temp\n",
    "if var == 'prcp':\n",
    "    readata_stn[readata_stn < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # correction reanalysis at station points\n",
    "# # this step is to support comparison between different methods. In practice, this step is not very necessary.\n",
    "# filecorrstn = path_reacorr + '/JRA55_corrstn_nearest_' + var + '_' + corrmode + '.npz'\n",
    "# if not os.path.isfile(filecorrstn):\n",
    "#     reacorr_stn = np.nan * np.zeros([reanum, nstn, ntimes], dtype=np.float32)\n",
    "#     for m in range(12):\n",
    "#         print('month', m + 1)\n",
    "#         indm = date_number['mm'] == (m + 1)\n",
    "#         corrm = correction_rea(stndata[:, indm], ecdf_prob, readata_stn[:, :, indm],\n",
    "#                                nearstn_loc, nearstn_dist, corrmode)\n",
    "#         reacorr_stn[:, :, indm] = corrm\n",
    "#         np.savez_compressed(filecorrstn, reacorr_stn=reacorr_stn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month 1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:274: RuntimeWarning: invalid value encountered in greater_equal\n",
      "/Users/localuser/Github/PyGMET/auxiliary_merge.py:47: RuntimeWarning: invalid value encountered in less\n",
      "  dataout[data < -1/exp] = 0\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:319: RuntimeWarning: invalid value encountered in greater_equal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "month 2\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "month 3\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "month 4\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "month 5\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "month 6\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "month 7\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "month 8\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "month 9\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "month 10\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "month 11\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "month 12\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n"
     ]
    }
   ],
   "source": [
    "# get merged and corrected reanalysis data at all station points using two-layer cross-validation\n",
    "if os.path.isfile(file_corrmerge_stn):\n",
    "    print('load independent merged/corrected data at station points')\n",
    "    datatemp = np.load(file_corrmerge_stn)\n",
    "    reamerge_stn = datatemp['reamerge_stn']\n",
    "    reamerge_weight_stn = datatemp['reamerge_weight_stn']\n",
    "    reacorr_stn = datatemp['reacorr_stn']\n",
    "    del datatemp\n",
    "else:\n",
    "    reamerge_stn = np.nan * np.zeros([nstn, ntimes], dtype=np.float32)\n",
    "    reamerge_weight_stn = np.nan * np.zeros([12, nstn, reanum], dtype=np.float32)\n",
    "    reacorr_stn = np.nan * np.zeros([reanum, nstn, ntimes], dtype=np.float32)\n",
    "    # for each month\n",
    "    for m in range(12):\n",
    "        print('month', m + 1)\n",
    "        indm = date_number['mm'] == (m + 1)\n",
    "        reamerge_stnm, reamerge_weight_stnm, reacorr_stnm = \\\n",
    "            correction_merge_stn(stndata[:, indm], ecdf_prob, readata_stn[:, :, indm], nearstn_loc, nearstn_dist,\n",
    "                                 var, corrmode, weightmode)\n",
    "        reamerge_stn[:, indm] = reamerge_stnm\n",
    "        reacorr_stn[:, :, indm] = reacorr_stnm\n",
    "        reamerge_weight_stn[m, :, :] = reamerge_weight_stnm\n",
    "\n",
    "    # the variables are independent with their concurrent stations. thus, station data can be used to evaluate them\n",
    "    np.savez_compressed(file_corrmerge_stn, reamerge_stn=reamerge_stn, reamerge_weight_stn=reamerge_weight_stn,\n",
    "                        reacorr_stn=reacorr_stn, date_list=date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
