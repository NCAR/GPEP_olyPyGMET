{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0cc596a-969f-489a-9885-ecfc037a0a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from tqdm.contrib import itertools\n",
    "import sys, os, math\n",
    "\n",
    "# sys.path.insert(0, '../src')\n",
    "# from regression import *\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# ludcmp, lubksb, and linearsolver\n",
    "\n",
    "# Source: https://phyweb.physics.nus.edu.sg/~phywjs/CZ5101/ludcmp.py\n",
    "# Similar to Fortran GMET solution\n",
    "\n",
    "# \"Numerical Recipes\" ludcmp() C code on page 46 translated into Python\n",
    "# I make it as simple as possible, disregard efficiency.\n",
    "# Here a is a list of list, n is integer (size of the matrix)\n",
    "# index is a list, and d is also a list of size 1\n",
    "# Python list index starts from 0.  So matrix index is from 0 to n-1.\n",
    "#\n",
    "import math\n",
    "import time\n",
    "\n",
    "def ludcmp(a, n, indx, d):\n",
    "    d[0] = 1.0\n",
    "    # looking for the largest a in each row and store it in vv as inverse\n",
    "    # We need a new list same size as indx, for this we use .copy()\n",
    "    vv = indx.copy()\n",
    "    for i in range(0, n):\n",
    "        big = 0.0\n",
    "        for j in range(0, n):\n",
    "            temp = math.fabs(a[i][j])\n",
    "            if (temp > big):\n",
    "                big = temp\n",
    "        vv[i] = 1.0 / big\n",
    "    #\n",
    "    # run Crout's algorithm\n",
    "    for j in range(0, n):\n",
    "        # top half & bottom part are combined\n",
    "        # but the upper limit l for k sum is different\n",
    "        big = 0.0\n",
    "        for i in range(0, n):\n",
    "            if (i < j):\n",
    "                l = i\n",
    "            else:\n",
    "                l = j\n",
    "            sum = a[i][j]\n",
    "            for k in range(0, l):\n",
    "                sum -= a[i][k] * a[k][j]\n",
    "            # end for k\n",
    "            a[i][j] = sum\n",
    "            # for bottom half, we keep track which row is larger\n",
    "            if (i >= j):\n",
    "                dum = vv[i] * math.fabs(sum)\n",
    "                if (dum >= big):\n",
    "                    big = dum\n",
    "                    imax = i\n",
    "            # end if (i>= ...)\n",
    "        # end for i\n",
    "        # pivoting part, swap row j with row imax, a[j] is a whole row\n",
    "        if (j != imax):\n",
    "            dum = a[imax]\n",
    "            a[imax] = a[j]\n",
    "            a[j] = dum\n",
    "            d[0] = - d[0]\n",
    "            vv[imax] = vv[j]\n",
    "        # end if (j != ...)\n",
    "        # divide by the beta diagonal value\n",
    "        indx[j] = imax\n",
    "        dum = 1.0 / a[j][j]\n",
    "        for i in range(j + 1, n):\n",
    "            a[i][j] *= dum\n",
    "        # end for i\n",
    "    # end for j\n",
    "\n",
    "\n",
    "# end of def ludcmp\n",
    "\n",
    "# We do backward substitution in lubksb() take the row swapped LU decomposed\n",
    "# a, size n, and swapping indx, and b vector as input.  The output is\n",
    "# in b after calling.\n",
    "def lubksb(a, n, indx, b):\n",
    "    ii = -1\n",
    "    # forward\n",
    "    for i in range(0, n):\n",
    "        ip = indx[i]\n",
    "        sum = b[ip]\n",
    "        b[ip] = b[i]\n",
    "        if (ii != -1):\n",
    "            for j in range(ii, i):\n",
    "                sum -= a[i][j] * b[j]\n",
    "        elif (sum != 0):\n",
    "            ii = i\n",
    "        b[i] = sum\n",
    "    # bote alpha_{ii} is 1 above\n",
    "    #  backward\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        sum = b[i]\n",
    "        for j in range(i + 1, n):\n",
    "            sum -= a[i][j] * b[j]\n",
    "        b[i] = sum / a[i][i]\n",
    "\n",
    "\n",
    "# end lubksb()\n",
    "\n",
    "\n",
    "# unfortunately a is destroyed (become swapped LU)\n",
    "def linearsolver(a, n, b):\n",
    "    indx = list(range(n))\n",
    "    d = [1]\n",
    "    ludcmp(a, n, indx, d)\n",
    "    x = b.copy()\n",
    "    lubksb(a, n, indx, x)\n",
    "    # print(\"x=\", x)\n",
    "    return x\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # using ludcmp and lubksb\n",
    "#     t1 = time.time()\n",
    "#     for i in range(10000):\n",
    "#         a = [[1, 3, 3, -5], [2, -4, 7, -1], [7, 0.5, 3, -6], [9, -2, 3, 8]]\n",
    "#         n = 4\n",
    "#         b = [0, 2, 3, -10]\n",
    "#         x2 = linearsolver(a, n, b)\n",
    "#     print(x2)\n",
    "#     t2 = time.time()\n",
    "#     print(t2-t1)\n",
    "#\n",
    "#     # just using numpy\n",
    "#     from numpy.linalg import inv\n",
    "#     import numpy as np\n",
    "#     t1 = time.time()\n",
    "#     for i in range(10000):\n",
    "#         a = np.array([[1, 3, 3, -5], [2, -4, 7, -1], [7, 0.5, 3, -6], [9, -2, 3, 8]])\n",
    "#         b = np.array([0, 2, 3, -10])\n",
    "#         ainv = inv(a)\n",
    "#         x2 = np.matmul(ainv, b)\n",
    "#     print(x2)\n",
    "#     t2 = time.time()\n",
    "#     print(t2-t1)\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# basic regression utility functions\n",
    "\n",
    "def least_squares_numpy(x, y, tx):\n",
    "    # In fortran version, ludcmp and lubksb are used to calcualte matrix inversion\n",
    "    # call ludcmp(a, indx, d)\n",
    "    # call lubksb(a, indx, b)\n",
    "\n",
    "    # In Python version, numpy is used to calculate matrix inversion\n",
    "    c = np.matmul(tx, y)\n",
    "    a = np.matmul(tx, x)\n",
    "\n",
    "    n = np.shape(a)[0]\n",
    "    b = np.zeros(n)\n",
    "\n",
    "    deta = np.linalg.det(a)  # Compute the determinant of an array\n",
    "    if deta == 0:\n",
    "        # print('Singular matrix')\n",
    "        b[:] = 0\n",
    "    else:\n",
    "        ainv = np.linalg.inv(a)\n",
    "        b = np.matmul(ainv, c)\n",
    "\n",
    "    return b\n",
    "\n",
    "\n",
    "def least_squares_ludcmp(x, y, tx):\n",
    "    # In fortran version, ludcmp and lubksb are used to calcualte matrix inversion\n",
    "    # call ludcmp(a, indx, d)\n",
    "    # call lubksb(a, indx, b)\n",
    "\n",
    "    # In Python version, numpy is used to calculate matrix inversion\n",
    "    c = np.matmul(tx, y)\n",
    "    a = np.matmul(tx, x)\n",
    "    n = np.shape(a)[0]\n",
    "    b = linearsolver(list(a), n, list(c))\n",
    "\n",
    "    return b\n",
    "\n",
    "def logistic_regression(x, tx, yp):\n",
    "    # nstn: station number\n",
    "    # nvars: station attributes (1, lat/lon/...), 1 is for regression\n",
    "    nstn, nvars = np.shape(x)\n",
    "\n",
    "    b = np.zeros(nvars)  # regression coefficients (beta)\n",
    "    p = np.zeros(nstn)  # estimated pop (probability of precipitation)\n",
    "    f = 0  # flag: continue or stop loops\n",
    "    it = 0  # iteration times\n",
    "\n",
    "    while f != 1:\n",
    "        # check for divergence\n",
    "        xb = -np.matmul(x, b)\n",
    "        if np.any(xb > 50):\n",
    "            f = 1\n",
    "        else:\n",
    "            p = 1 / (1 + np.exp(xb))\n",
    "\n",
    "        # check for divergence\n",
    "        if np.any(p > 0.9999):\n",
    "            # logistic regression diverging\n",
    "            f = 1\n",
    "        else:\n",
    "            v = np.zeros([nstn, nstn])  # diagonal variance matrix\n",
    "            for i in range(nstn):\n",
    "                v[i, i] = p[i] * (1 - p[i])\n",
    "            xv = np.matmul(v, x)\n",
    "            yn = yp - p  # difference between station occurrence (0/1) and estimated pop: Bnew -Bold in Martyn and Slater 2006\n",
    "            bn = least_squares_ludcmp(xv, yn, tx)\n",
    "\n",
    "            # check: converging\n",
    "            if np.any(np.abs(bn) > 1e-4):\n",
    "                f = 0\n",
    "            else:\n",
    "                f = 1\n",
    "\n",
    "            # check: iteration times\n",
    "            if it > 20:\n",
    "                f = 1\n",
    "            else:\n",
    "                f = 0\n",
    "\n",
    "            b = b + bn  # update coefficients\n",
    "\n",
    "        it = it + 1\n",
    "\n",
    "    return b\n",
    "\n",
    "\n",
    "def weight_linear_regression(nearinfo, weightnear, datanear, tarinfo):\n",
    "    # # nearinfo: predictors from neighboring stations\n",
    "    # # [station number, predictor number + 1] array with the first column being ones\n",
    "    # nearinfo = np.zeros([nnum, npred+1])\n",
    "    #\n",
    "    # # weightnear: weight of neighboring stations\n",
    "    # # [station number, station number] array with weights located in the diagonal\n",
    "    # weightnear = np.zeros([nnum, nnum])\n",
    "    # for i in range(nnum):\n",
    "    #     weightnear[i, i] = 123\n",
    "    #\n",
    "    # # tarinfo:  predictors from target stations\n",
    "    # # [predictor number + 1] vector with the first value being one\n",
    "    # tarinfo = np.zeros(npred+1)\n",
    "    #\n",
    "    # # datanear: data from neighboring stations. [station number] vector\n",
    "    # datanear = np.zeros(nnum)\n",
    "\n",
    "    # start regression\n",
    "    w_pcp_red = np.diag(np.squeeze(weightnear))\n",
    "    tx_red = np.transpose(nearinfo)\n",
    "    twx_red = np.matmul(tx_red, w_pcp_red)\n",
    "    b = least_squares_ludcmp(nearinfo, datanear, twx_red)\n",
    "    datatar = np.dot(tarinfo, b)\n",
    "\n",
    "    return datatar\n",
    "\n",
    "def weight_logistic_regression(nearinfo, weightnear, datanear, tarinfo):\n",
    "\n",
    "    w_pcp_red = np.diag(np.squeeze(weightnear))\n",
    "\n",
    "    # if len(np.unique(datanear)) == 1:\n",
    "    #     pop = datanear[0] # all 0 (no rain) or all 1 (rain everywhere)\n",
    "    # else:\n",
    "    tx_red = np.transpose(nearinfo)\n",
    "    twx_red = np.matmul(tx_red, w_pcp_red)\n",
    "    b = logistic_regression(nearinfo, twx_red, datanear)\n",
    "    if np.all(b == 0) or np.any(np.isnan(b)):\n",
    "        pop = np.dot(weightnear, datanear)\n",
    "    else:\n",
    "        zb = - np.dot(tarinfo, b)\n",
    "        pop = 1 / (1 + np.exp(zb))\n",
    "\n",
    "    return pop\n",
    "\n",
    "def check_predictor_matrix_behavior(nearinfo, weightnear):\n",
    "    # check to see if the station predictor matrix will be well behaved\n",
    "    w_pcp_red = np.diag(np.squeeze(weightnear))\n",
    "    tx_red = np.transpose(nearinfo)\n",
    "    twx_red = np.matmul(tx_red, w_pcp_red)\n",
    "    tmp = np.matmul(twx_red, nearinfo)\n",
    "    vv =  np.max(np.abs(tmp), axis=1)\n",
    "    if np.any(vv==0):\n",
    "        flag = False # bad performance\n",
    "    else:\n",
    "        flag = True # good performance\n",
    "    return flag\n",
    "\n",
    "\n",
    "###########################\n",
    "# regression using Python sklearn package: easier to use, but slower and a bit different from the above functions\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def sklearn_weight_linear_regression(nearinfo, weightnear, datanear, tarinfo):\n",
    "    model = LinearRegression()\n",
    "    model = model.fit(nearinfo, datanear, sample_weight=weightnear)\n",
    "    datatar = model.predict(tarinfo[np.newaxis, :])\n",
    "    return datatar\n",
    "def sklearn_weight_logistic_regression(nearinfo, weightnear, datanear, tarinfo):\n",
    "\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model = model.fit(nearinfo, datanear, sample_weight=weightnear)\n",
    "    pop = model.predict_proba(tarinfo[np.newaxis,:])[0, 1]\n",
    "    return pop\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# dynmaic predictor-related functions\n",
    "def initial_check_dynamic_predictor(dynamic_predictor_name, dynamic_predictor_filelist, target_vars):\n",
    "\n",
    "    if not isinstance(dynamic_predictor_name, list):\n",
    "        sys.exit(f'Error! dynamic_predictor_name must be a list.')\n",
    "\n",
    "    if (len(target_vars) != len(dynamic_predictor_name)) and (len(dynamic_predictor_name) > 0):\n",
    "        sys.exit(f'Error! len(dynamic_predictor_name)>0 but is different from len(target_vars)!')\n",
    "\n",
    "    flag = False\n",
    "    if not os.path.isfile(dynamic_predictor_filelist):\n",
    "        print('Do not find dynamic_predictor_filelist:', dynamic_predictor_filelist)\n",
    "    elif len(dynamic_predictor_name) == 0:\n",
    "        print('dynamic_predictor_name length is 0')\n",
    "    else:\n",
    "        with open(dynamic_predictor_filelist, 'r') as f:\n",
    "            file0 = f.readline().strip()\n",
    "        if not os.path.isfile(file0):\n",
    "            print(f'Do not find the first file: {file0} in dynamic_predictor_filelist: {dynamic_predictor_filelist}')\n",
    "        else:\n",
    "            # with nc.Dataset(file0) as ds:\n",
    "            with xr.open_dataset(file0) as ds:\n",
    "                for i in range(len(dynamic_predictor_name)):\n",
    "                    tmp = []\n",
    "                    for v in dynamic_predictor_name[i]:\n",
    "                        # if v in ds.variables.keys():\n",
    "                        if v in ds.data_vars:\n",
    "                            print(f'Include {v} as a dynamic predictor for {target_vars[i]}')\n",
    "                            tmp.append(v)\n",
    "                        else:\n",
    "                            print(f'Cannot find {v} in {file0}. Do not include it as a dynamic predictor for {target_vars[i]}')\n",
    "                    if len(tmp) > 0:\n",
    "                        flag = True\n",
    "                    dynamic_predictor_name[i] = tmp\n",
    "\n",
    "    if flag == False:\n",
    "        print('Dynamic predictor is not activated')\n",
    "    else:\n",
    "        print(f'Dynamic predictor is activated. Dynamic predicotrs are {dynamic_predictor_name}')\n",
    "    return flag\n",
    "\n",
    "def map_filelist_timestep(dynamic_predictor_filelist, timeseries):\n",
    "    # for every time step, find the corresponding input file\n",
    "\n",
    "    # get file list\n",
    "    filelist = []\n",
    "    with open(dynamic_predictor_filelist, 'r') as f:\n",
    "        for line in f:\n",
    "            filelist.append(line.strip())\n",
    "\n",
    "    # make a dateframe containing all time steps\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(len(filelist)):\n",
    "        with xr.open_dataset(filelist[i]) as ds:\n",
    "            timei = ds.time.values\n",
    "            files = [filelist[i]] * len(timei)\n",
    "            dfi = pd.DataFrame({'intime': timei, 'file': files, 'ind': np.arange(len(timei))})\n",
    "            df = pd.concat((df, dfi))\n",
    "\n",
    "    # for each time step, find the closest\n",
    "    df_mapping = pd.DataFrame()\n",
    "    for t in timeseries:\n",
    "        if t >= df['intime'].iloc[0] and t <= df['intime'].iloc[-1]: # don't extrapolate \n",
    "            indi = np.argmin(np.abs(df['intime'] - t)) # nearest neighbor\n",
    "            df_mapping = pd.concat((df_mapping, df.iloc[[indi]]))\n",
    "        else:\n",
    "            df_mapping = pd.concat((df_mapping, pd.DataFrame({'intime': [np.nan], 'file': [''], 'ind': [np.nan]})))\n",
    "\n",
    "    df_mapping['tartime'] = timeseries\n",
    "    df_mapping.index = np.arange(len(df_mapping))\n",
    "\n",
    "    return df_mapping\n",
    "\n",
    "# def read_timestep_input_data(df_mapping, tarindex, varnames):\n",
    "#     # not finished\n",
    "#     df = df_mapping.iloc[tarindex]\n",
    "#     with xr.open_dataset(df['file']) as ds:\n",
    "#         ds = ds.isel(time=tarindex)\n",
    "#         ds = ds[varnames]\n",
    "#         ds = ds.load()\n",
    "#     return ds\n",
    "\n",
    "\n",
    "def read_period_input_data(df_mapping, varnames):\n",
    "    # read and\n",
    "    # select period\n",
    "    # select variables\n",
    "\n",
    "    # tarlon/tarlat: target lat/lon vector\n",
    "    # default: dynmaic input files must have lat/lon dimensions\n",
    "\n",
    "    files = np.unique(df_mapping['file'].values)\n",
    "    files = [f for f in files if len(f)>0]\n",
    "\n",
    "    intime = df_mapping['intime'].values\n",
    "    intime = intime[~np.isnan(intime)]\n",
    "\n",
    "    tartime = df_mapping['tartime'].values\n",
    "\n",
    "    if len(files) == 0:\n",
    "        print('Warning! Cannot find any valid dynamic input files')\n",
    "        ds = xr.Dataset()\n",
    "    else:\n",
    "        ds = xr.open_mfdataset(files)\n",
    "        if np.any([not d in ds.dims for d in ['time', 'lat','lon']]):\n",
    "            sys.exit(f'Dynamic input files must have lat, lon, and time dimensions!')\n",
    "        # select ...\n",
    "        ds = ds[varnames]\n",
    "        ds = ds.sel(time=slice(tartime[0], tartime[-1]))\n",
    "        ds = ds.load()\n",
    "        ds = ds.interp(time=tartime, method='nearest')\n",
    "    return ds\n",
    "\n",
    "\n",
    "def regrid_xarray(ds, tarlon, tarlat, target):\n",
    "    # if target='1D', tarlon and tarlat are vector of station points\n",
    "    # if target='2D', tarlon and tarlat are vector defining grids\n",
    "\n",
    "    # regridding: space and time\n",
    "    if target == '2D':\n",
    "\n",
    "        ds = ds.transpose('time', 'lat', 'lon')\n",
    "\n",
    "        if ds.lat.values[0] > ds.lat.values[1]:\n",
    "            ds = ds.sel(lat=slice(tarlat[-1], tarlat[0]))\n",
    "        else:\n",
    "            ds = ds.sel(lat=slice(tarlat[0], tarlat[-1]))\n",
    "        if ds.lon.values[0] > ds.lon.values[1]:\n",
    "            ds = ds.sel(lon=slice(tarlon[-1], tarlon[0]))\n",
    "        else:\n",
    "            ds = ds.sel(lon=slice(tarlon[0], tarlon[-1]))\n",
    "\n",
    "        ds_out = ds.interp(lat=tarlat, lon=tarlon, method='linear')\n",
    "\n",
    "    elif target == '1D':\n",
    "        tarlat = xr.DataArray(tarlat, dims=('z'))\n",
    "        tarlon = xr.DataArray(tarlon, dims=('z'))\n",
    "        ds_out = ds.interp(lat=tarlat, lon=tarlon, method='linear')\n",
    "        ds_out = ds_out.transpose('time', 'z')\n",
    "\n",
    "    else:\n",
    "        sys.exit('Unknown target')\n",
    "\n",
    "    return ds_out\n",
    "\n",
    "\n",
    "\n",
    "def flatten_list(lst):\n",
    "    flat_list = []\n",
    "    for item in lst:\n",
    "        if isinstance(item, list):\n",
    "            flat_list.extend(flatten_list(item))\n",
    "        else:\n",
    "            flat_list.append(item)\n",
    "    return flat_list\n",
    "\n",
    "########################################################################################################################\n",
    "# wrapped up regression functions\n",
    "\n",
    "def loop_regression_2Dor3D(stn_data, stn_predictor, tar_nearIndex, tar_nearWeight, tar_predictor, method, dynamic_predictors={}):\n",
    "    # regression for 2D (vector) or 3D (array) inputs\n",
    "    # 2D:\n",
    "    # stn_data: [input station, time steps]\n",
    "    # stn_predictor: [input station, number of predictors]\n",
    "    # nearIndex/nearWeight: [target point, number of nearby stations]\n",
    "    # tar_predictor: [target point, number of predictors]\n",
    "    # 3D:\n",
    "    # stn_data: [input station, time steps]\n",
    "    # stn_predictor: [input station, number of predictors]\n",
    "    # nearIndex/nearWeight: [row, col, number of nearby stations]\n",
    "    # tar_predictor: [row, col, number of predictors]\n",
    "\n",
    "    np.savez_compressed('../docs/data_for_parallel_test.npz', stn_data=stn_data, stn_predictor=stn_predictor,\n",
    "                        tar_nearIndex=tar_nearIndex, tar_nearWeight=tar_nearWeight, tar_predictor=tar_predictor, method=method, dynamic_predictors=dynamic_predictors)\n",
    "\n",
    "    if len(dynamic_predictors) == 0:\n",
    "        dynamic_predictors['flag'] = False\n",
    "\n",
    "    # if tar_nearIndex.ndim == 2:\n",
    "    #     # make it a 3D array to be consistent\n",
    "    #     tar_nearIndex = tar_nearIndex[np.newaxis, :, :]\n",
    "    #     tar_nearWeight = tar_nearWeight[np.newaxis, :, :]\n",
    "    #     tar_predictor = tar_predictor[np.newaxis, :, :]\n",
    "    #\n",
    "    #     if dynamic_predictors['flag'] == True:\n",
    "    #         # change raw dim: [n_feature, n_time, n_station] to [n_feature, n_time, 1, n_station]\n",
    "    #         dynamic_predictors['tar_predictor_dynamic'] = dynamic_predictors['tar_predictor_dynamic'][:, :, np.newaxis, :]\n",
    "\n",
    "\n",
    "    nstn, ntime = np.shape(stn_data)\n",
    "    nrow, ncol, nearmax = np.shape(tar_nearIndex)\n",
    "    estimates = np.nan * np.zeros([nrow, ncol, ntime], dtype=np.float32)\n",
    "\n",
    "    for r, c in itertools.product(range(nrow), range(ncol)):\n",
    "\n",
    "        # prepare xdata and sample weight for training and weights of neighboring stations\n",
    "        sample_nearIndex = tar_nearIndex[r, c, :]\n",
    "        index_valid = sample_nearIndex >= 0\n",
    "\n",
    "        if np.sum(index_valid) > 0:\n",
    "            sample_nearIndex = sample_nearIndex[index_valid]\n",
    "\n",
    "            sample_weight = tar_nearWeight[r, c, :][index_valid]\n",
    "            sample_weight = sample_weight / np.sum(sample_weight)\n",
    "\n",
    "            xdata_near0 = stn_predictor[sample_nearIndex, :]\n",
    "            xdata_g0 = tar_predictor[r, c, :]\n",
    "\n",
    "            # interpolation for every time step\n",
    "            for d in range(ntime):\n",
    "\n",
    "                ydata_near = np.squeeze(stn_data[sample_nearIndex, d])\n",
    "                if len(np.unique(ydata_near)) == 1:  # e.g., for prcp, all zero\n",
    "                    ydata_tar = ydata_near[0]\n",
    "                else:\n",
    "\n",
    "                    # add dynamic predictors if flag is true and predictors are good\n",
    "                    xdata_near = xdata_near0\n",
    "                    xdata_g = xdata_g0\n",
    "                    if dynamic_predictors['flag'] == True:\n",
    "                        xdata_near_add = dynamic_predictors['stn_predictor_dynamic'][:, d, sample_nearIndex].T\n",
    "                        xdata_g_add = dynamic_predictors['tar_predictor_dynamic'][:, d, r, c]\n",
    "                        if np.all(~np.isnan(xdata_near_add)) and np.all(~np.isnan(xdata_g_add)):\n",
    "                            xdata_near_try = np.hstack((xdata_near, xdata_near_add))\n",
    "                            xdata_g_try = np.hstack((xdata_g, xdata_g_add))\n",
    "                            # check if dynamic predictors are good for regression\n",
    "                            if check_predictor_matrix_behavior(xdata_near_try, sample_weight) == True:\n",
    "                                xdata_near = xdata_near_try\n",
    "                                xdata_g = xdata_g_try\n",
    "                            else:\n",
    "                                xdata_near_try = np.hstack((xdata_near, xdata_near_add[:, ~dynamic_predictors['predictor_checkflag']]))\n",
    "                                xdata_g_try = np.hstack((xdata_g, xdata_g_add[~dynamic_predictors['predictor_checkflag']]))\n",
    "                                if check_predictor_matrix_behavior(xdata_near_try, sample_weight) == True:\n",
    "                                    xdata_near = xdata_near_try\n",
    "                                    xdata_g = xdata_g_try\n",
    "\n",
    "                    # regression\n",
    "                    if method == 'linear':\n",
    "                        ydata_tar = weight_linear_regression(xdata_near, sample_weight, ydata_near, xdata_g)\n",
    "                    elif method == 'logistic':\n",
    "                        ydata_tar = weight_logistic_regression(xdata_near, sample_weight, ydata_near, xdata_g)\n",
    "                    else:\n",
    "                        sys.exit(f'Unknonwn regression method: {method}')\n",
    "                estimates[r, c, d] = ydata_tar\n",
    "    return np.squeeze(estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b74cb3-8cbb-4352-ae20-193c8779e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data0 = np.load('data_for_parallel_test.npz', allow_pickle=True)\n",
    "stn_data=data0['stn_data']\n",
    "stn_predictor=data0['stn_predictor']\n",
    "tar_nearIndex=data0['tar_nearIndex']\n",
    "tar_nearWeight=data0['tar_nearWeight']\n",
    "tar_predictor=data0['tar_predictor']\n",
    "method=data0['method']\n",
    "dynamic_predictors=data0['dynamic_predictors'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a87ef945-a844-4f4d-b4cd-3980489c5895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34752c96a44844d2819776d294bb5c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.3 s, sys: 337 ms, total: 27.6 s\n",
      "Wall time: 27.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def weight_linear_regression(nearinfo, weightnear, datanear, tarinfo):\n",
    "    # # nearinfo: predictors from neighboring stations\n",
    "    # # [station number, predictor number + 1] array with the first column being ones\n",
    "    # nearinfo = np.zeros([nnum, npred+1])\n",
    "    #\n",
    "    # # weightnear: weight of neighboring stations\n",
    "    # # [station number, station number] array with weights located in the diagonal\n",
    "    # weightnear = np.zeros([nnum, nnum])\n",
    "    # for i in range(nnum):\n",
    "    #     weightnear[i, i] = 123\n",
    "    #\n",
    "    # # tarinfo:  predictors from target stations\n",
    "    # # [predictor number + 1] vector with the first value being one\n",
    "    # tarinfo = np.zeros(npred+1)\n",
    "    #\n",
    "    # # datanear: data from neighboring stations. [station number] vector\n",
    "    # datanear = np.zeros(nnum)\n",
    "\n",
    "    # start regression\n",
    "    w_pcp_red = np.diag(np.squeeze(weightnear))\n",
    "    tx_red = np.transpose(nearinfo)\n",
    "    twx_red = np.matmul(tx_red, w_pcp_red)\n",
    "    b = least_squares_ludcmp(nearinfo, datanear, twx_red)\n",
    "    datatar = np.dot(tarinfo, b)\n",
    "\n",
    "    return datatar\n",
    "\n",
    "# loop_regression_2Dor3D function\n",
    "\n",
    "\n",
    "if len(dynamic_predictors) == 0:\n",
    "    dynamic_predictors['flag'] = False\n",
    "\n",
    "# if tar_nearIndex.ndim == 2:\n",
    "#     # make it a 3D array to be consistent\n",
    "#     tar_nearIndex = tar_nearIndex[np.newaxis, :, :]\n",
    "#     tar_nearWeight = tar_nearWeight[np.newaxis, :, :]\n",
    "#     tar_predictor = tar_predictor[np.newaxis, :, :]\n",
    "#\n",
    "#     if dynamic_predictors['flag'] == True:\n",
    "#         # change raw dim: [n_feature, n_time, n_station] to [n_feature, n_time, 1, n_station]\n",
    "#         dynamic_predictors['tar_predictor_dynamic'] = dynamic_predictors['tar_predictor_dynamic'][:, :, np.newaxis, :]\n",
    "\n",
    "\n",
    "nstn, ntime = np.shape(stn_data)\n",
    "nrow, ncol, nearmax = np.shape(tar_nearIndex)\n",
    "estimates = np.nan * np.zeros([nrow, ncol, ntime], dtype=np.float32)\n",
    "\n",
    "for r, c in itertools.product(range(nrow), range(ncol)):\n",
    "\n",
    "    # prepare xdata and sample weight for training and weights of neighboring stations\n",
    "    sample_nearIndex = tar_nearIndex[r, c, :]\n",
    "    index_valid = sample_nearIndex >= 0\n",
    "\n",
    "    if np.sum(index_valid) > 0:\n",
    "        sample_nearIndex = sample_nearIndex[index_valid]\n",
    "\n",
    "        sample_weight = tar_nearWeight[r, c, :][index_valid]\n",
    "        sample_weight = sample_weight / np.sum(sample_weight)\n",
    "\n",
    "        xdata_near0 = stn_predictor[sample_nearIndex, :]\n",
    "        xdata_g0 = tar_predictor[r, c, :]\n",
    "\n",
    "        # interpolation for every time step\n",
    "        for d in range(ntime):\n",
    "\n",
    "            ydata_near = np.squeeze(stn_data[sample_nearIndex, d])\n",
    "            if len(np.unique(ydata_near)) == 1:  # e.g., for prcp, all zero\n",
    "                ydata_tar = ydata_near[0]\n",
    "            else:\n",
    "\n",
    "                # add dynamic predictors if flag is true and predictors are good\n",
    "                xdata_near = xdata_near0\n",
    "                xdata_g = xdata_g0\n",
    "                if dynamic_predictors['flag'] == True:\n",
    "                    xdata_near_add = dynamic_predictors['stn_predictor_dynamic'][:, d, sample_nearIndex].T\n",
    "                    xdata_g_add = dynamic_predictors['tar_predictor_dynamic'][:, d, r, c]\n",
    "                    if np.all(~np.isnan(xdata_near_add)) and np.all(~np.isnan(xdata_g_add)):\n",
    "                        xdata_near_try = np.hstack((xdata_near, xdata_near_add))\n",
    "                        xdata_g_try = np.hstack((xdata_g, xdata_g_add))\n",
    "                        # check if dynamic predictors are good for regression\n",
    "                        if check_predictor_matrix_behavior(xdata_near_try, sample_weight) == True:\n",
    "                            xdata_near = xdata_near_try\n",
    "                            xdata_g = xdata_g_try\n",
    "                        else:\n",
    "                            xdata_near_try = np.hstack((xdata_near, xdata_near_add[:, ~dynamic_predictors['predictor_checkflag']]))\n",
    "                            xdata_g_try = np.hstack((xdata_g, xdata_g_add[~dynamic_predictors['predictor_checkflag']]))\n",
    "                            if check_predictor_matrix_behavior(xdata_near_try, sample_weight) == True:\n",
    "                                xdata_near = xdata_near_try\n",
    "                                xdata_g = xdata_g_try\n",
    "\n",
    "                # regression\n",
    "                if method == 'linear':\n",
    "                    ydata_tar = weight_linear_regression(xdata_near, sample_weight, ydata_near, xdata_g)\n",
    "                elif method == 'logistic':\n",
    "                    ydata_tar = weight_logistic_regression(xdata_near, sample_weight, ydata_near, xdata_g)\n",
    "                else:\n",
    "                    sys.exit(f'Unknonwn regression method: {method}')\n",
    "            estimates[r, c, d] = ydata_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df6cd6-dac1-4abe-8248-4f4af784e7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67fc5663-5d0d-4ce3-85ab-b279ae905cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_worker(stn_data, stn_predictor, tar_nearIndex, tar_nearWeight, tar_predictor, method, dynamic_predictors):\n",
    "    # Using a dictionary is not strictly necessary. You can also\n",
    "    # use global variables.\n",
    "    var_dict['stn_data'] = stn_data\n",
    "    var_dict['stn_predictor'] = stn_predictor\n",
    "    var_dict['tar_nearIndex'] = tar_nearIndex\n",
    "    var_dict['tar_nearWeight'] = tar_nearWeight\n",
    "    var_dict['tar_predictor'] = tar_predictor\n",
    "    var_dict['method'] = method\n",
    "    var_dict['dynamic_predictors'] = dynamic_predictors\n",
    "\n",
    "\n",
    "\n",
    "def worker_func(r, c):\n",
    "    stn_data = var_dict['stn_data']\n",
    "    stn_predictor = var_dict['stn_predictor']\n",
    "    tar_nearIndex = var_dict['tar_nearIndex']\n",
    "    tar_nearWeight = var_dict['tar_nearWeight']\n",
    "    tar_predictor = var_dict['tar_predictor']\n",
    "    method = var_dict['method']\n",
    "    dynamic_predictors = var_dict['dynamic_predictors']\n",
    "\n",
    "    nstn, ntime = np.shape(stn_data)\n",
    "    nrow, ncol, nearmax = np.shape(tar_nearIndex)\n",
    "\n",
    "    # prepare xdata and sample weight for training and weights of neighboring stations\n",
    "    sample_nearIndex = tar_nearIndex[r, c, :]\n",
    "    index_valid = sample_nearIndex >= 0\n",
    "\n",
    "    if np.sum(index_valid) > 0:\n",
    "        sample_nearIndex = sample_nearIndex[index_valid]\n",
    "\n",
    "        sample_weight = tar_nearWeight[r, c, :][index_valid]\n",
    "        sample_weight = sample_weight / np.sum(sample_weight)\n",
    "\n",
    "        xdata_near0 = stn_predictor[sample_nearIndex, :]\n",
    "        xdata_g0 = tar_predictor[r, c, :]\n",
    "\n",
    "        # interpolation for every time step\n",
    "        for d in range(ntime):\n",
    "\n",
    "            ydata_near = np.squeeze(stn_data[sample_nearIndex, d])\n",
    "            if len(np.unique(ydata_near)) == 1:  # e.g., for prcp, all zero\n",
    "                ydata_tar = ydata_near[0]\n",
    "            else:\n",
    "\n",
    "                # add dynamic predictors if flag is true and predictors are good\n",
    "                xdata_near = xdata_near0\n",
    "                xdata_g = xdata_g0\n",
    "                if dynamic_predictors['flag'] == True:\n",
    "                    xdata_near_add = dynamic_predictors['stn_predictor_dynamic'][:, d, sample_nearIndex].T\n",
    "                    xdata_g_add = dynamic_predictors['tar_predictor_dynamic'][:, d, r, c]\n",
    "                    if np.all(~np.isnan(xdata_near_add)) and np.all(~np.isnan(xdata_g_add)):\n",
    "                        xdata_near_try = np.hstack((xdata_near, xdata_near_add))\n",
    "                        xdata_g_try = np.hstack((xdata_g, xdata_g_add))\n",
    "                        # check if dynamic predictors are good for regression\n",
    "                        if check_predictor_matrix_behavior(xdata_near_try, sample_weight) == True:\n",
    "                            xdata_near = xdata_near_try\n",
    "                            xdata_g = xdata_g_try\n",
    "                        else:\n",
    "                            xdata_near_try = np.hstack(\n",
    "                                (xdata_near, xdata_near_add[:, ~dynamic_predictors['predictor_checkflag']]))\n",
    "                            xdata_g_try = np.hstack((xdata_g, xdata_g_add[~dynamic_predictors['predictor_checkflag']]))\n",
    "                            if check_predictor_matrix_behavior(xdata_near_try, sample_weight) == True:\n",
    "                                xdata_near = xdata_near_try\n",
    "                                xdata_g = xdata_g_try\n",
    "\n",
    "                # regression\n",
    "                if method == 'linear':\n",
    "                    ydata_tar = weight_linear_regression(xdata_near, sample_weight, ydata_near, xdata_g)\n",
    "                elif method == 'logistic':\n",
    "                    ydata_tar = weight_logistic_regression(xdata_near, sample_weight, ydata_near, xdata_g)\n",
    "                else:\n",
    "                    sys.exit(f'Unknonwn regression method: {method}')\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed970321-9dc7-4a29-92e4-7d3c1a32b2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618bf95dd23346a786fdc2e7bd36a185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'init_worker' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/context.py\u001b[0m in \u001b[0;36mPool\u001b[0;34m(self, processes, initializer, initargs, maxtasksperchild)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;34m'''Returns a process pool object'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         return Pool(processes, initializer, initargs, maxtasksperchild,\n\u001b[0m\u001b[1;32m    120\u001b[0m                     context=self.get_context())\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, processes, initializer, initargs, maxtasksperchild, context)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_processes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_repopulate_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_repopulate_pool\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repopulate_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         return self._repopulate_pool_static(self._ctx, self.Process,\n\u001b[0m\u001b[1;32m    304\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_processes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inqueue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_repopulate_pool_static\u001b[0;34m(ctx, Process, processes, pool, inqueue, outqueue, initializer, initargs, maxtasksperchild, wrap_exception)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Process'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PoolWorker'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m             \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'added worker'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_spawn_posix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mForkServerProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mfds_to_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "\n",
    "    import multiprocessing\n",
    "    from multiprocessing import Pool\n",
    "\n",
    "    # np.savez_compressed('../docs/data_for_parallel_test.npz', stn_data=stn_data, stn_predictor=stn_predictor,\n",
    "    #                     tar_nearIndex=tar_nearIndex, tar_nearWeight=tar_nearWeight, tar_predictor=tar_predictor, method=method, dynamic_predictors=dynamic_predictors)\n",
    "\n",
    "    if len(dynamic_predictors) == 0:\n",
    "        dynamic_predictors['flag'] = False\n",
    "\n",
    "    # if tar_nearIndex.ndim == 2:\n",
    "    #     # make it a 3D array to be consistent\n",
    "    #     tar_nearIndex = tar_nearIndex[np.newaxis, :, :]\n",
    "    #     tar_nearWeight = tar_nearWeight[np.newaxis, :, :]\n",
    "    #     tar_predictor = tar_predictor[np.newaxis, :, :]\n",
    "    #\n",
    "    #     if dynamic_predictors['flag'] == True:\n",
    "    #         # change raw dim: [n_feature, n_time, n_station] to [n_feature, n_time, 1, n_station]\n",
    "    #         dynamic_predictors['tar_predictor_dynamic'] = dynamic_predictors['tar_predictor_dynamic'][:, :, np.newaxis, :]\n",
    "\n",
    "\n",
    "    nstn, ntime = np.shape(stn_data)\n",
    "    nrow, ncol, nearmax = np.shape(tar_nearIndex)\n",
    "    estimates = np.nan * np.zeros([nrow, ncol, ntime], dtype=np.float32)\n",
    "\n",
    "    items = [(r,c) for r, c in itertools.product(range(nrow), range(ncol))]\n",
    "    with Pool(processes=5, initializer=init_worker, initargs=(stn_data, stn_predictor, tar_nearIndex, tar_nearWeight, tar_predictor, method, dynamic_predictors)) as pool:\n",
    "        result = pool.starmap(worker_func, items)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "442bdccd-d8e9-4b3f-acf0-517488be2d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "# without specifying unique thread, the function is executed\n",
    "# on all threads\n",
    "client = Client(n_workers=6, threads_per_worker=1)\n",
    "client.close()\n",
    "# loop_regression_2Dor3D function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59ce87f0-fcd5-41a1-b778-3f19373fd34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def6ffaaf4224efdbf60ec9b1aa3348c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 6s, sys: 20.4 s, total: 1min 26s\n",
      "Wall time: 52.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# loop_regression_2Dor3D function\n",
    "@dask.delayed\n",
    "def reg_fun(stn_data, stn_predictor, tar_nearIndex, tar_nearWeight, tar_predictor, method, dynamic_predictors, r, c):\n",
    "    # prepare xdata and sample weight for training and weights of neighboring stations\n",
    "    nstn, ntime = np.shape(stn_data)\n",
    "    nrow, ncol, nearmax = np.shape(tar_nearIndex)\n",
    "    sample_nearIndex = tar_nearIndex[r, c, :]\n",
    "    index_valid = sample_nearIndex >= 0\n",
    "    ydata_tar_all = np.nan * np.zeros(ntime)\n",
    "    if np.sum(index_valid) > 0:\n",
    "        sample_nearIndex = sample_nearIndex[index_valid]\n",
    "\n",
    "        sample_weight = tar_nearWeight[r, c, :][index_valid]\n",
    "        sample_weight = sample_weight / np.sum(sample_weight)\n",
    "\n",
    "        xdata_near0 = stn_predictor[sample_nearIndex, :]\n",
    "        xdata_g0 = tar_predictor[r, c, :]\n",
    "\n",
    "        # interpolation for every time step\n",
    "        for d in range(ntime):\n",
    "\n",
    "            ydata_near = np.squeeze(stn_data[sample_nearIndex, d])\n",
    "            if len(np.unique(ydata_near)) == 1:  # e.g., for prcp, all zero\n",
    "                ydata_tar = ydata_near[0]\n",
    "            else:\n",
    "\n",
    "                # add dynamic predictors if flag is true and predictors are good\n",
    "                xdata_near = xdata_near0\n",
    "                xdata_g = xdata_g0\n",
    "                if dynamic_predictors['flag'] == True:\n",
    "                    xdata_near_add = dynamic_predictors['stn_predictor_dynamic'][:, d, sample_nearIndex].T\n",
    "                    xdata_g_add = dynamic_predictors['tar_predictor_dynamic'][:, d, r, c]\n",
    "                    if np.all(~np.isnan(xdata_near_add)) and np.all(~np.isnan(xdata_g_add)):\n",
    "                        xdata_near_try = np.hstack((xdata_near, xdata_near_add))\n",
    "                        xdata_g_try = np.hstack((xdata_g, xdata_g_add))\n",
    "                        # check if dynamic predictors are good for regression\n",
    "                        if check_predictor_matrix_behavior(xdata_near_try, sample_weight) == True:\n",
    "                            xdata_near = xdata_near_try\n",
    "                            xdata_g = xdata_g_try\n",
    "                        else:\n",
    "                            xdata_near_try = np.hstack((xdata_near, xdata_near_add[:, ~dynamic_predictors['predictor_checkflag']]))\n",
    "                            xdata_g_try = np.hstack((xdata_g, xdata_g_add[~dynamic_predictors['predictor_checkflag']]))\n",
    "                            if check_predictor_matrix_behavior(xdata_near_try, sample_weight) == True:\n",
    "                                xdata_near = xdata_near_try\n",
    "                                xdata_g = xdata_g_try\n",
    "\n",
    "                # regression\n",
    "                if method == 'linear':\n",
    "                    ydata_tar_all[d] = weight_linear_regression(xdata_near, sample_weight, ydata_near, xdata_g)\n",
    "                elif method == 'logistic':\n",
    "                    ydata_tar_all[d] = weight_logistic_regression(xdata_near, sample_weight, ydata_near, xdata_g)\n",
    "                else:\n",
    "                    sys.exit(f'Unknonwn regression method: {method}')\n",
    "    return ydata_tar_all\n",
    "\n",
    "\n",
    "nstn, ntime = np.shape(stn_data)\n",
    "nrow, ncol, nearmax = np.shape(tar_nearIndex)\n",
    "estimates = np.nan * np.zeros([nrow, ncol, ntime], dtype=np.float32)\n",
    "\n",
    "outputs = []\n",
    "for r, c in itertools.product(range(nrow), range(ncol)):\n",
    "    ydata_tar_all = reg_fun(stn_data, stn_predictor, tar_nearIndex, tar_nearWeight, tar_predictor, method, dynamic_predictors, r, c)\n",
    "    outputs.append(ydata_tar_all)\n",
    "    \n",
    "outputs = dask.compute(outputs, n_workers=4)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "16072ade-7c5d-44c9-84a5-b88b5baca4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/general/lib/python3.9/site-packages/distributed/node.py:179: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 51901 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "# without specifying unique thread, the function is executed\n",
    "# on all threads\n",
    "client = Client(n_workers=4, threads_per_worker=1)\n",
    "\n",
    "# loop_regression_2Dor3D function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbfe4509-be18-4343-83c4-a71fdc39be31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1fc23bc81c64413a046bf2f29143d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 19s, sys: 7.19 s, total: 1min 26s\n",
      "Wall time: 59.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "exception calling callback for <Future at 0x7fe4da4f8eb0 state=finished returned list>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/concurrent/futures/_base.py\", line 330, in _invoke_callbacks\n",
      "    callback(self)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/asyncio/futures.py\", line 398, in _call_set_state\n",
      "    dest_loop.call_soon_threadsafe(_set_state, destination, source)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/asyncio/base_events.py\", line 796, in call_soon_threadsafe\n",
      "    self._check_closed()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/asyncio/base_events.py\", line 515, in _check_closed\n",
      "    raise RuntimeError('Event loop is closed')\n",
      "RuntimeError: Event loop is closed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def weight_linear_regression(nearinfo, weightnear, datanear, tarinfo):\n",
    "    # # nearinfo: predictors from neighboring stations\n",
    "    # # [station number, predictor number + 1] array with the first column being ones\n",
    "    # nearinfo = np.zeros([nnum, npred+1])\n",
    "    #\n",
    "    # # weightnear: weight of neighboring stations\n",
    "    # # [station number, station number] array with weights located in the diagonal\n",
    "    # weightnear = np.zeros([nnum, nnum])\n",
    "    # for i in range(nnum):\n",
    "    #     weightnear[i, i] = 123\n",
    "    #\n",
    "    # # tarinfo:  predictors from target stations\n",
    "    # # [predictor number + 1] vector with the first value being one\n",
    "    # tarinfo = np.zeros(npred+1)\n",
    "    #\n",
    "    # # datanear: data from neighboring stations. [station number] vector\n",
    "    # datanear = np.zeros(nnum)\n",
    "    # start regression\n",
    "    w_pcp_red = np.diag(np.squeeze(weightnear))\n",
    "    tx_red = np.transpose(nearinfo)\n",
    "    twx_red = np.matmul(tx_red, w_pcp_red)\n",
    "    b = least_squares_ludcmp(nearinfo, datanear, twx_red)\n",
    "    datatar = np.dot(tarinfo, b)\n",
    "    return datatar\n",
    "\n",
    "\n",
    "nstn, ntime = np.shape(stn_data)\n",
    "nrow, ncol, nearmax = np.shape(tar_nearIndex)\n",
    "estimates = []\n",
    "\n",
    "for r, c in itertools.product(range(nrow), range(ncol)):\n",
    "\n",
    "    # prepare xdata and sample weight for training and weights of neighboring stations\n",
    "    sample_nearIndex = tar_nearIndex[r, c, :]\n",
    "    index_valid = sample_nearIndex >= 0\n",
    "\n",
    "    if np.sum(index_valid) > 0:\n",
    "        sample_nearIndex = sample_nearIndex[index_valid]\n",
    "\n",
    "        sample_weight = tar_nearWeight[r, c, :][index_valid]\n",
    "        sample_weight = sample_weight / np.sum(sample_weight)\n",
    "\n",
    "        xdata_near0 = stn_predictor[sample_nearIndex, :]\n",
    "        xdata_g0 = tar_predictor[r, c, :]\n",
    "\n",
    "        # interpolation for every time step\n",
    "        for d in range(ntime):\n",
    "\n",
    "            ydata_near = np.squeeze(stn_data[sample_nearIndex, d])\n",
    "            if len(np.unique(ydata_near)) == 1:  # e.g., for prcp, all zero\n",
    "                ydata_tar = ydata_near[0]\n",
    "            else:\n",
    "\n",
    "                # add dynamic predictors if flag is true and predictors are good\n",
    "                xdata_near = xdata_near0\n",
    "                xdata_g = xdata_g0\n",
    "                if dynamic_predictors['flag'] == True:\n",
    "                    xdata_near_add = dynamic_predictors['stn_predictor_dynamic'][:, d, sample_nearIndex].T\n",
    "                    xdata_g_add = dynamic_predictors['tar_predictor_dynamic'][:, d, r, c]\n",
    "                    if np.all(~np.isnan(xdata_near_add)) and np.all(~np.isnan(xdata_g_add)):\n",
    "                        xdata_near_try = np.hstack((xdata_near, xdata_near_add))\n",
    "                        xdata_g_try = np.hstack((xdata_g, xdata_g_add))\n",
    "                        # check if dynamic predictors are good for regression\n",
    "                        if check_predictor_matrix_behavior(xdata_near_try, sample_weight) == True:\n",
    "                            xdata_near = xdata_near_try\n",
    "                            xdata_g = xdata_g_try\n",
    "                        else:\n",
    "                            xdata_near_try = np.hstack((xdata_near, xdata_near_add[:, ~dynamic_predictors['predictor_checkflag']]))\n",
    "                            xdata_g_try = np.hstack((xdata_g, xdata_g_add[~dynamic_predictors['predictor_checkflag']]))\n",
    "                            if check_predictor_matrix_behavior(xdata_near_try, sample_weight) == True:\n",
    "                                xdata_near = xdata_near_try\n",
    "                                xdata_g = xdata_g_try\n",
    "\n",
    "                # regression\n",
    "                if method == 'linear':\n",
    "                    ydata_tar = weight_linear_regression(xdata_near, sample_weight, ydata_near, xdata_g)\n",
    "                elif method == 'logistic':\n",
    "                    ydata_tar = weight_logistic_regression(xdata_near, sample_weight, ydata_near, xdata_g)\n",
    "                else:\n",
    "                    sys.exit(f'Unknonwn regression method: {method}')\n",
    "            estimates.append(ydata_tar)\n",
    "            \n",
    "outputs = dask.compute(estimates, n_workers=6)\n",
    "\n",
    "# client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eb9743d-3da1-4c49-bfba-ff17ca3dee5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'slow_function' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'slow_function' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'slow_function' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'slow_function' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'slow_function' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'slow_function' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'slow_function' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'slow_function' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'slow_function' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'slow_function' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-19:\n",
      "Process SpawnPoolWorker-13:\n",
      "Process SpawnPoolWorker-18:\n",
      "Process SpawnPoolWorker-15:\n",
      "Process SpawnPoolWorker-20:\n",
      "Process SpawnPoolWorker-11:\n",
      "Process SpawnPoolWorker-14:\n",
      "Process SpawnPoolWorker-17:\n",
      "Traceback (most recent call last):\n",
      "Process SpawnPoolWorker-16:\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process SpawnPoolWorker-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/connection.py\", line 221, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pf/n30tfn0j0n93dr81jm369hyc0000gp/T/ipykernel_4299/2057035114.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# list of inputs to process in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslow_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# parallelize the loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         '''\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/general/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/general/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-25:\n",
      "Process SpawnPoolWorker-26:\n",
      "Process SpawnPoolWorker-30:\n",
      "Process SpawnPoolWorker-24:\n",
      "Process SpawnPoolWorker-27:\n",
      "Process SpawnPoolWorker-21:\n",
      "Process SpawnPoolWorker-23:\n",
      "Process SpawnPoolWorker-29:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process SpawnPoolWorker-28:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/connection.py\", line 221, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Process SpawnPoolWorker-22:\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def slow_function(arg):\n",
    "    # some slow computation\n",
    "    return arg * 2\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pool = multiprocessing.Pool()\n",
    "    inputs = range(10)  # list of inputs to process in parallel\n",
    "    results = pool.map(slow_function, inputs)  # parallelize the loop\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e077983d-24ef-49ed-8172-2096bb6add9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7102002709574844\n",
      "0.7753833457865317\n",
      "0.869570256803165\n",
      "0.5802933092048493\n",
      "0.8325173082271703\n",
      "0.6394695673917928\n",
      "0.364350936563697\n",
      "0.744907480397409\n",
      "0.36804718491988764\n",
      "0.0033366109138929234\n"
     ]
    }
   ],
   "source": [
    "# SuperFastPython.com\n",
    "# example of a sequential for loop\n",
    "from time import sleep\n",
    "from random import random\n",
    " \n",
    "# a task to execute in another process\n",
    "def task(arg):\n",
    "    # generate a value between 0 and 1\n",
    "    value = random()\n",
    "    # block for a fraction of a second to simulate work\n",
    "    sleep(value)\n",
    "    # return the generated value\n",
    "    return value\n",
    " \n",
    "# entry point for the program\n",
    "if __name__ == '__main__':\n",
    "    # call the same function with different data sequentially\n",
    "    for result in map(task, range(10)):\n",
    "        # report the value to show progress\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8443a7-08fc-45da-b072-a72d85542e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-43:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'task' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-41:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'task' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-44:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'task' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-42:\n",
      "Process SpawnPoolWorker-46:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "AttributeError: Can't get attribute 'task' on <module '__main__' (built-in)>\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'task' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-45:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'task' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-47:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'task' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-48:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'task' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-50:\n",
      "Process SpawnPoolWorker-49:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'task' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/general/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'task' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "# SuperFastPython.com\n",
    "# example of a parallel for loop with no return values\n",
    "from time import sleep\n",
    "from random import random\n",
    "from multiprocessing import Pool\n",
    " \n",
    "# task to execute in another process\n",
    "def task(arg):\n",
    "    # generate a value between 0 and 1\n",
    "    value = random()\n",
    "    # block for a fraction of a second to simulate work\n",
    "    sleep(value)\n",
    "    # # report the value to show progress\n",
    "    print(f'{arg} got {value}', flush=True)\n",
    " \n",
    "# entry point for the program\n",
    "if __name__ == '__main__':\n",
    "    # create the process pool\n",
    "    with Pool() as pool:\n",
    "        # call the same function with different data in parallel\n",
    "        pool.map(task, range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88783f10-795e-4a00-9939-866b63203b16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:general]",
   "language": "python",
   "name": "conda-env-general-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
