# perform locally weighted regression for all stations using leave-one-out


import numpy as np
import os
import sys, time

sys.path.append('..')
import regression as reg


# # define random data
# x_red_use = np.array([ [1, 1.2, 1.4],
#                        [1, 2.2, 1.42],
#                        [1, 4.2, 3.33],
#                        [1, 12, 3.5],
#                        [1, 0.4, 2],
#                        [1, 0.523, 1.2],
#                        [1, 5.5, 2.3]])
#
# w_pcp_red = np.zeros([7, 7])
# w_vector = [0.1, 0.5, 0.23, 0.6, 0.23, 0.66, 0.9]
# for i in range(7):
#     w_pcp_red[i, i] = w_vector[i]  # eye matrix: stn weight in one-one line
#
# y_prcp_red = np.array([0.2, 1.2, 0.4, 0.3, 0.55, 0.9, 0.44])




# define random data
x_red_use = np.array([[ 1.000000e+00,  3.458940e+01, -1.184547e+02,  6.367000e+02, -7.564000e+00,  2.711000e+00],
       [ 1.000000e+00,  3.438690e+01, -1.185342e+02,  3.758000e+02, -1.046100e+01,  4.945000e+00],
       [ 1.000000e+00,  3.449390e+01, -1.182714e+02,  8.635000e+02, -6.687000e+00,  8.084000e+00],
       [ 1.000000e+00,  3.470500e+01, -1.184242e+02,  9.327000e+02, -4.846000e+00, -2.170000e-01],
       [ 1.000000e+00,  3.432940e+01, -1.184006e+02,  4.724000e+02, -1.102800e+01,  7.943000e+00],
       [ 1.000000e+00,  3.431670e+01, -1.185500e+02,  7.215000e+02, -1.107900e+01,  5.416000e+00],
       [ 1.000000e+00,  3.440610e+01, -1.187569e+02,  2.170000e+02, -1.372900e+01, -5.280000e-01],
       [ 1.000000e+00,  3.448830e+01, -1.181419e+02,  9.623000e+02, -4.959000e+00,  8.682000e+00],
       [ 1.000000e+00,  3.424470e+01, -1.185250e+02,  2.890000e+02, -1.076500e+01,  4.833000e+00],
       [ 1.000000e+00,  3.474110e+01, -1.182117e+02,  7.126000e+02, 3.260000e-01, -1.710000e-01],
       [ 1.000000e+00,  3.474360e+01, -1.187242e+02,  1.374600e+03, -6.755000e+00, -3.813000e+00],
       [ 1.000000e+00,  3.475000e+01, -1.187167e+02,  1.226800e+03, -3.942000e+00, -2.677000e+00],
       [ 1.000000e+00,  3.458830e+01, -1.180939e+02,  7.943000e+02, 1.093000e+00,  5.567000e+00],
       [ 1.000000e+00,  3.429470e+01, -1.181883e+02,  7.090000e+02, -1.306000e+01,  9.611000e+00],
       [ 1.000000e+00,  3.462940e+01, -1.180836e+02,  7.693000e+02, 2.977000e+00,  3.458000e+00],
       [ 1.000000e+00,  3.420060e+01, -1.183575e+02,  2.259000e+02, -1.275200e+01,  7.601000e+00],
       [ 1.000000e+00,  3.418190e+01, -1.185744e+02,  2.408000e+02, -9.294000e+00,  3.120000e+00],
       [ 1.000000e+00,  3.422220e+01, -1.182378e+02,  4.709000e+02, -1.413100e+01,  8.969000e+00],
       [ 1.000000e+00,  3.418220e+01, -1.181383e+02,  3.435000e+02, -1.502000e+01,  8.760000e+00],
       [ 1.000000e+00,  3.483280e+01, -1.188650e+02,  1.092700e+03,  2.736000e+00, -1.163000e+00],
       [ 1.000000e+00,  3.423080e+01, -1.180711e+02,  1.740400e+03, -1.510600e+01,  1.001800e+01],
       [ 1.000000e+00,  3.408420e+01, -1.185989e+02,  2.271000e+02, -8.123000e+00,  2.397000e+00],
       [ 1.000000e+00,  3.406970e+01, -1.184428e+02,  1.311000e+02, -9.495000e+00,  3.607000e+00],
       [ 1.000000e+00,  3.414810e+01, -1.181444e+02,  2.633000e+02, -1.502000e+01,  8.760000e+00],
       [ 1.000000e+00,  3.450280e+01, -1.178944e+02,  9.296000e+02, 1.073000e+00,  3.717000e+00],
       [ 1.000000e+00,  3.401580e+01, -1.184514e+02,  5.300000e+01, -8.343000e+00,  2.458000e+00],
       [ 1.000000e+00,  3.405110e+01, -1.182353e+02,  7.010000e+01, -1.213700e+01,  5.152000e+00],
       [ 1.000000e+00,  3.410610e+01, -1.181000e+02,  1.372000e+02, -1.514300e+01,  8.431000e+00],
       [ 1.000000e+00,  3.400750e+01, -1.184997e+02,  4.300000e+00, -8.343000e+00,  2.458000e+00],
       [ 1.000000e+00,  3.400530e+01, -1.184136e+02,  2.070000e+01, -9.192000e+00,  2.974000e+00],
       [ 1.000000e+00,  3.502330e+01, -1.187497e+02,  4.343000e+02, 4.263000e+00,  9.616000e+00],
       [ 1.000000e+00,  3.431750e+01, -1.178419e+02,  1.629200e+03, -9.666000e+00,  3.394000e+00],
       [ 1.000000e+00,  3.407640e+01, -1.188803e+02,  4.877000e+02, -7.230000e+00,  3.000000e+00],
       [ 1.000000e+00,  3.510110e+01, -1.184222e+02,  1.286300e+03, -5.493000e+00, -1.356000e+00],
       [ 1.000000e+00,  3.504920e+01, -1.181619e+02,  8.336000e+02, -4.850000e+00, -8.311000e+00]])

w_pcp_red = np.zeros([35, 35])
w_vector = [0.0388809 , 0.03832985, 0.03817355, 0.03802249, 0.03745646,
       0.03711991, 0.03588912, 0.03574807, 0.0351188 , 0.03482195,
       0.03478868, 0.03477532, 0.03415687, 0.03338446, 0.03333384,
       0.03278177, 0.03189196, 0.0317251 , 0.02656133, 0.02649087,
       0.02641929, 0.02530463, 0.02513205, 0.02471289, 0.0241944 ,
       0.02079555, 0.02064448, 0.02017512, 0.02005636, 0.01975397,
       0.01841752, 0.01702782, 0.01630212, 0.01601774, 0.01559474]
for i in range(35):
    w_pcp_red[i, i] = w_vector[i]  # eye matrix: stn weight in one-one line

y_prcp_red = np.array([ 4.416522  ,  1.9206624 ,  2.5261984 ,  3.1489668 ,  3.6673174 ,
        3.7148647 ,  2.908574  ,  1.9249749 ,  1.885819  ,  0.7568283 ,
        2.6417985 ,  1.5446663 ,  1.5836291 ,  4.1812468 ,  1.0297337 ,
        0.63115644,  2.529779  ,  2.1395035 ,  3.5515766 ,  3.2349849 ,
        4.497094  ,  0.1865406 ,  2.4116774 ,  3.3983598 ,  3.8984742 ,
        0.7568283 ,  2.0682702 ,  1.264296  ,  0.34635973,  2.6417985 ,
        5.8850327 ,  4.673559  ,  2.4834137 ,  4.097801  , -4.        ])



tx_red = np.transpose(x_red_use)
twx_red = np.matmul(tx_red, w_pcp_red)

# least square
t1 = time.time()
for i in range(100000):
    b = reg.least_squares(x_red_use, y_prcp_red, twx_red)
t2 = time.time()
print(b)
print('least square time cost:', t2-t1)

tarinfo = [   1.        ,   34.53125   , -118.46875   ,  805.        ,
         -8.2303107 ,    4.21243016]
datatar = np.dot(tarinfo, b)
print('estimate is ', datatar)
# logistic regression







